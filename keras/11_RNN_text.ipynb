{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for texts\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4}\n"
     ]
    }
   ],
   "source": [
    "# Standardization and tokenizing\n",
    "dataset = ['The cat sat on the mat.']\n",
    "\n",
    "def standardize(text):\n",
    "    text = text.lower()\n",
    "    punctuation = ['.', ',', ':', ';', '-', '!', '?']\n",
    "    for symbol in punctuation:\n",
    "        text = text.replace(symbol, '')\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "            \n",
    "print(vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(token):\n",
    "    vector = np.zeros(shape=(len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
    "    \n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)\n",
    "\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(output_mode=\"int\",\n",
    "                                       standardize=custom_standardization_fn,\n",
    "                                       split=custom_split_fn,\n",
    ")\n",
    "\n",
    "dataset = [\n",
    "\"I write, erase, rewrite\",\n",
    "\"Erase again, and then\",\n",
    "\"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)\n",
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB dataset\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path('aclimdb')\n",
    "val_dir = base_dir /'val'\n",
    "train_dir = base_dir / 'train'\n",
    "\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category/ fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'The first film had little ambition so nothing sticks to the screen. It was a bad version of \\'Back to the Future\\' with zero charm. Once accepted that Bill & Ted are nitwits, the joke can only get hammered at the audience for so long before it breaks.<br /><br />This is a surprise. This is your only spoiler warning...<br /><br />By today\\'s standards, this is more fun. This was shunned upon release, sad considering that more talent is involved than the first time. We get the photographer of \\'Face/Off\\', the editor of \\'Fugitive\\', a production designer from Burton\\'s early work, and the sound designer of \\'Matrix\\'.<br /><br />The writers made up for their shallow first outing with something deep. Since this was shunned by the fanbase and public, the director probably decided the style was too extreme. It\\'s not, it fits the material. Like \\'Death Becomes Her\\' and \\'Catch-22\\', this dares to be smart, but we like our movies \"simple\" so we don\\'t buy it. Probably since this dared to be different is why it took 12 producers to pull it off. What\\'s so good?<br /><br />--Nice self-reference towards Keanu; from airhead to Messiah. See also Arnold Schwarzenegger.<br /><br />--Joss hates his creations as much as he hates their counterparts, he makes his own hatred. The Evil B&T and the \"good robot usses\" have the same vocabulary as their originals: lesser copies and depreciation of language.<br /><br />--The \"duality\" motif. Nowhere else is this evident than in the photography styles, lots of high and low angles. They even use Roy Brocksmith from \\'Total Recall\\' to emphasize the point.<br /><br />--The \"choices\" motif. I don\\'t know where this started in the genre (maybe \\'Ghostbusters\\'), but it\\'s used pretty well here. It even boils down to the 7 games against death--Battleship and Club.<br /><br />--Film self-reference, even present in the game against Death (Clue). This is smarter than Tarantino or Brooks. Notice the Premier magazine cover at the end: \"Bill and Ted: The Movie.\" Ironic also how Death and Nomolos were villains in the \\'Die Hard\\' and \\'Lethal Weapon\\' sequels.<br /><br />I still have some minor nits, but nothing compared to the original. Music and film are different mediums so it makes no sense why many scripts revolve around the former--particularly in the teen market. Carlin is a great comedian, but in these movies he\\'s wasted. Also, for all the daring this effort shows, even cracking gay jokes, they can\\'t kill a cat?<br /><br />So, despite looking like a Nickelodeon production, this is incredibly interesting. From this movie we got Beavis and Butt-Head. \"We\\'re in Heaven and we just mugged three people.\" <br /><br />Final Analysis = = Midrange Material', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One word (1gram) for bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 10ms/step - loss: 0.3939 - accuracy: 0.8392 - val_loss: 0.2961 - val_accuracy: 0.8840\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2650 - accuracy: 0.9014 - val_loss: 0.2974 - val_accuracy: 0.8886\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2354 - accuracy: 0.9187 - val_loss: 0.3165 - val_accuracy: 0.8896\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2198 - accuracy: 0.9251 - val_loss: 0.3348 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2098 - accuracy: 0.9316 - val_loss: 0.3491 - val_accuracy: 0.8876\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2177 - accuracy: 0.9333 - val_loss: 0.3563 - val_accuracy: 0.8870\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2003 - accuracy: 0.9368 - val_loss: 0.3802 - val_accuracy: 0.8894\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2042 - accuracy: 0.9373 - val_loss: 0.3767 - val_accuracy: 0.8862\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2044 - accuracy: 0.9378 - val_loss: 0.3818 - val_accuracy: 0.8908\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1993 - accuracy: 0.9407 - val_loss: 0.3947 - val_accuracy: 0.8878\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2890 - accuracy: 0.8867\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2890 - accuracy: 0.8867TA: 1s - ETA: 0s - loss: 0.2888 - accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28895309567451477, 0.886680006980896]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(binary_1gram_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode=\"multi_hot\")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.3755 - accuracy: 0.8456 - val_loss: 0.2780 - val_accuracy: 0.8934\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2413 - accuracy: 0.9166 - val_loss: 0.2894 - val_accuracy: 0.8946\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2088 - accuracy: 0.9337 - val_loss: 0.3090 - val_accuracy: 0.8928\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1969 - accuracy: 0.9401 - val_loss: 0.3174 - val_accuracy: 0.8992\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1742 - accuracy: 0.9459 - val_loss: 0.3336 - val_accuracy: 0.8954\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1833 - accuracy: 0.9475 - val_loss: 0.3478 - val_accuracy: 0.8934\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1695 - accuracy: 0.9503 - val_loss: 0.3573 - val_accuracy: 0.8968\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1721 - accuracy: 0.9524 - val_loss: 0.3638 - val_accuracy: 0.8966\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1680 - accuracy: 0.9535 - val_loss: 0.3689 - val_accuracy: 0.8908\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1797 - accuracy: 0.9549 - val_loss: 0.3781 - val_accuracy: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5bc362a08>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3651 - accuracy: 0.8916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3650892972946167, 0.8915600180625916]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(binary_2gram_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIGRAMS WITH TF-IDF ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2,\n",
    "                                       max_tokens=20000,\n",
    "                                       output_mode=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 8s 11ms/step - loss: 0.4847 - accuracy: 0.7831 - val_loss: 0.3178 - val_accuracy: 0.8800\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3294 - accuracy: 0.8602 - val_loss: 0.2978 - val_accuracy: 0.8908\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3017 - accuracy: 0.8727 - val_loss: 0.3177 - val_accuracy: 0.8738\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2870 - accuracy: 0.8809 - val_loss: 0.3681 - val_accuracy: 0.8758\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2677 - accuracy: 0.8960 - val_loss: 0.3497 - val_accuracy: 0.8854\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2466 - accuracy: 0.9049 - val_loss: 0.3264 - val_accuracy: 0.8832\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2426 - accuracy: 0.9068 - val_loss: 0.3569 - val_accuracy: 0.8808\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2366 - accuracy: 0.9081 - val_loss: 0.3698 - val_accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2289 - accuracy: 0.9141 - val_loss: 0.3548 - val_accuracy: 0.8822\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2249 - accuracy: 0.9128 - val_loss: 0.3784 - val_accuracy: 0.8784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5b8e32d88>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True) ]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 7ms/step - loss: 0.3775 - accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3775244653224945, 0.8719599843025208]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfidf_2gram_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = layers.TextVectorization(max_tokens=max_tokens,\n",
    "                                              output_mode=\"int\",\n",
    "                                              output_sequence_length=max_length)\n",
    "\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "tf.one_hot (TFOpLambda)      (None, None, 20000)       0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                5128448   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens) # Encode the integers into binary 20,000 dimensional vectors.\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 61/625 [=>............................] - ETA: 1:10:46 - loss: 0.6916 - accuracy: 0.5282"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CD86~1\\AppData\\Local\\Temp/ipykernel_2144/2500901405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                73984     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 223s 352ms/step - loss: 0.4901 - accuracy: 0.7744 - val_loss: 0.3742 - val_accuracy: 0.8418\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 199s 319ms/step - loss: 0.3103 - accuracy: 0.8848 - val_loss: 0.3139 - val_accuracy: 0.8750\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 199s 318ms/step - loss: 0.2489 - accuracy: 0.9116 - val_loss: 0.3462 - val_accuracy: 0.8566\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 197s 315ms/step - loss: 0.2046 - accuracy: 0.9308 - val_loss: 0.3418 - val_accuracy: 0.8702\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 204s 327ms/step - loss: 0.1714 - accuracy: 0.9415 - val_loss: 0.3744 - val_accuracy: 0.8694\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 197s 315ms/step - loss: 0.1408 - accuracy: 0.9523 - val_loss: 0.3691 - val_accuracy: 0.8808\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 202s 324ms/step - loss: 0.1120 - accuracy: 0.9628 - val_loss: 0.4034 - val_accuracy: 0.8736\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 202s 323ms/step - loss: 0.0943 - accuracy: 0.9695 - val_loss: 0.4526 - val_accuracy: 0.8758\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 204s 326ms/step - loss: 0.0787 - accuracy: 0.9753 - val_loss: 0.4214 - val_accuracy: 0.8702\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 199s 319ms/step - loss: 0.0647 - accuracy: 0.9796 - val_loss: 0.5553 - val_accuracy: 0.8606\n",
      "782/782 [==============================] - 84s 107ms/step - loss: 0.6568 - accuracy: 0.8385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6567695140838623, 0.8385199904441833]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)\n",
    "model.evaluate(int_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add masking to ignore padding of zeros for short sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "some_input = [\n",
    "    [4, 3, 2, 1, 0, 0, 0],\n",
    "    [5, 4, 3, 2, 1, 0, 0],\n",
    "    [2, 1, 0, 0, 0, 0, 0]]\n",
    "mask = embedding_layer.compute_mask(some_input)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                73984     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, \n",
    "                            output_dim=256, \n",
    "                            mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 263s 414ms/step - loss: 0.3930 - accuracy: 0.8246 - val_loss: 0.2897 - val_accuracy: 0.8790\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 317s 508ms/step - loss: 0.2261 - accuracy: 0.9126 - val_loss: 0.3065 - val_accuracy: 0.8752\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 317s 508ms/step - loss: 0.1600 - accuracy: 0.9402 - val_loss: 0.3062 - val_accuracy: 0.8860\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 316s 505ms/step - loss: 0.1178 - accuracy: 0.9592 - val_loss: 0.3406 - val_accuracy: 0.8744\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 291s 466ms/step - loss: 0.0855 - accuracy: 0.9707 - val_loss: 0.3853 - val_accuracy: 0.8746\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 332s 532ms/step - loss: 0.0630 - accuracy: 0.9785 - val_loss: 0.4020 - val_accuracy: 0.8660\n",
      "Epoch 7/10\n",
      "229/625 [=========>....................] - ETA: 2:52 - loss: 0.0489 - accuracy: 0.9829"
     ]
    }
   ],
   "source": [
    "model.fit(int_train_ds, \n",
    "          validation_data=int_val_ds, \n",
    "          epochs=10)\n",
    "model.evaluate(int_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
