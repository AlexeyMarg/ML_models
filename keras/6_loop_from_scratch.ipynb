{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GradientTape: a first end-to-end example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_train[:10000], y_train[:10000]\n",
    "x_train, y_train = x_train[10000:], y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x1 = layers.Dense(64, activation='relu')(inputs)\n",
    "x2 = layers.Dense(64, activation='relu')(inputs)\n",
    "outputs =layers.Dense(10, name='predictions')(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_f = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 0\n",
      "Training loss (for one batch) at step 1: 232.9317\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 2: 155.1023\n",
      "Seen so far: 192 samples\n",
      "Training loss (for one batch) at step 3: 95.5584\n",
      "Seen so far: 256 samples\n",
      "Training loss (for one batch) at step 4: 39.2264\n",
      "Seen so far: 320 samples\n",
      "Training loss (for one batch) at step 5: 18.6083\n",
      "Seen so far: 384 samples\n",
      "Training loss (for one batch) at step 6: 10.5849\n",
      "Seen so far: 448 samples\n",
      "Training loss (for one batch) at step 7: 13.4401\n",
      "Seen so far: 512 samples\n",
      "Training loss (for one batch) at step 8: 7.9432\n",
      "Seen so far: 576 samples\n",
      "Training loss (for one batch) at step 9: 7.5258\n",
      "Seen so far: 640 samples\n",
      "Training loss (for one batch) at step 10: 6.6949\n",
      "Seen so far: 704 samples\n",
      "Training loss (for one batch) at step 11: 6.2490\n",
      "Seen so far: 768 samples\n",
      "Training loss (for one batch) at step 12: 3.3532\n",
      "Seen so far: 832 samples\n",
      "Training loss (for one batch) at step 13: 3.1668\n",
      "Seen so far: 896 samples\n",
      "Training loss (for one batch) at step 14: 4.1691\n",
      "Seen so far: 960 samples\n",
      "Training loss (for one batch) at step 15: 4.7405\n",
      "Seen so far: 1024 samples\n",
      "Training loss (for one batch) at step 16: 6.9372\n",
      "Seen so far: 1088 samples\n",
      "Training loss (for one batch) at step 17: 4.1109\n",
      "Seen so far: 1152 samples\n",
      "Training loss (for one batch) at step 18: 3.9573\n",
      "Seen so far: 1216 samples\n",
      "Training loss (for one batch) at step 19: 4.2150\n",
      "Seen so far: 1280 samples\n",
      "Training loss (for one batch) at step 20: 3.0379\n",
      "Seen so far: 1344 samples\n",
      "Training loss (for one batch) at step 21: 3.8719\n",
      "Seen so far: 1408 samples\n",
      "Training loss (for one batch) at step 22: 4.2665\n",
      "Seen so far: 1472 samples\n",
      "Training loss (for one batch) at step 23: 3.5947\n",
      "Seen so far: 1536 samples\n",
      "Training loss (for one batch) at step 24: 5.0947\n",
      "Seen so far: 1600 samples\n",
      "Training loss (for one batch) at step 25: 2.9976\n",
      "Seen so far: 1664 samples\n",
      "Training loss (for one batch) at step 26: 3.5248\n",
      "Seen so far: 1728 samples\n",
      "Training loss (for one batch) at step 27: 3.2258\n",
      "Seen so far: 1792 samples\n",
      "Training loss (for one batch) at step 28: 2.8143\n",
      "Seen so far: 1856 samples\n",
      "Training loss (for one batch) at step 29: 2.7767\n",
      "Seen so far: 1920 samples\n",
      "Training loss (for one batch) at step 30: 3.1110\n",
      "Seen so far: 1984 samples\n",
      "Training loss (for one batch) at step 31: 2.0662\n",
      "Seen so far: 2048 samples\n",
      "Training loss (for one batch) at step 32: 3.6063\n",
      "Seen so far: 2112 samples\n",
      "Training loss (for one batch) at step 33: 3.0479\n",
      "Seen so far: 2176 samples\n",
      "Training loss (for one batch) at step 34: 2.7205\n",
      "Seen so far: 2240 samples\n",
      "Training loss (for one batch) at step 35: 3.3282\n",
      "Seen so far: 2304 samples\n",
      "Training loss (for one batch) at step 36: 2.6505\n",
      "Seen so far: 2368 samples\n",
      "Training loss (for one batch) at step 37: 2.8113\n",
      "Seen so far: 2432 samples\n",
      "Training loss (for one batch) at step 38: 2.3635\n",
      "Seen so far: 2496 samples\n",
      "Training loss (for one batch) at step 39: 2.2532\n",
      "Seen so far: 2560 samples\n",
      "Training loss (for one batch) at step 40: 3.1553\n",
      "Seen so far: 2624 samples\n",
      "Training loss (for one batch) at step 41: 2.8860\n",
      "Seen so far: 2688 samples\n",
      "Training loss (for one batch) at step 42: 2.7682\n",
      "Seen so far: 2752 samples\n",
      "Training loss (for one batch) at step 43: 3.0738\n",
      "Seen so far: 2816 samples\n",
      "Training loss (for one batch) at step 44: 1.8361\n",
      "Seen so far: 2880 samples\n",
      "Training loss (for one batch) at step 45: 2.3379\n",
      "Seen so far: 2944 samples\n",
      "Training loss (for one batch) at step 46: 3.4727\n",
      "Seen so far: 3008 samples\n",
      "Training loss (for one batch) at step 47: 3.7214\n",
      "Seen so far: 3072 samples\n",
      "Training loss (for one batch) at step 48: 2.2967\n",
      "Seen so far: 3136 samples\n",
      "Training loss (for one batch) at step 49: 2.5890\n",
      "Seen so far: 3200 samples\n",
      "Training loss (for one batch) at step 50: 2.1571\n",
      "Seen so far: 3264 samples\n",
      "Training loss (for one batch) at step 51: 3.1622\n",
      "Seen so far: 3328 samples\n",
      "Training loss (for one batch) at step 52: 2.2315\n",
      "Seen so far: 3392 samples\n",
      "Training loss (for one batch) at step 53: 2.4599\n",
      "Seen so far: 3456 samples\n",
      "Training loss (for one batch) at step 54: 2.1601\n",
      "Seen so far: 3520 samples\n",
      "Training loss (for one batch) at step 55: 2.0389\n",
      "Seen so far: 3584 samples\n",
      "Training loss (for one batch) at step 56: 2.1988\n",
      "Seen so far: 3648 samples\n",
      "Training loss (for one batch) at step 57: 2.8581\n",
      "Seen so far: 3712 samples\n",
      "Training loss (for one batch) at step 58: 3.1281\n",
      "Seen so far: 3776 samples\n",
      "Training loss (for one batch) at step 59: 2.7954\n",
      "Seen so far: 3840 samples\n",
      "Training loss (for one batch) at step 60: 3.1041\n",
      "Seen so far: 3904 samples\n",
      "Training loss (for one batch) at step 61: 2.7194\n",
      "Seen so far: 3968 samples\n",
      "Training loss (for one batch) at step 62: 2.1808\n",
      "Seen so far: 4032 samples\n",
      "Training loss (for one batch) at step 63: 2.1447\n",
      "Seen so far: 4096 samples\n",
      "Training loss (for one batch) at step 64: 2.6576\n",
      "Seen so far: 4160 samples\n",
      "Training loss (for one batch) at step 65: 3.0849\n",
      "Seen so far: 4224 samples\n",
      "Training loss (for one batch) at step 66: 1.9463\n",
      "Seen so far: 4288 samples\n",
      "Training loss (for one batch) at step 67: 2.2850\n",
      "Seen so far: 4352 samples\n",
      "Training loss (for one batch) at step 68: 2.5928\n",
      "Seen so far: 4416 samples\n",
      "Training loss (for one batch) at step 69: 2.3848\n",
      "Seen so far: 4480 samples\n",
      "Training loss (for one batch) at step 70: 2.1022\n",
      "Seen so far: 4544 samples\n",
      "Training loss (for one batch) at step 71: 2.7338\n",
      "Seen so far: 4608 samples\n",
      "Training loss (for one batch) at step 72: 2.1457\n",
      "Seen so far: 4672 samples\n",
      "Training loss (for one batch) at step 73: 3.4072\n",
      "Seen so far: 4736 samples\n",
      "Training loss (for one batch) at step 74: 1.9045\n",
      "Seen so far: 4800 samples\n",
      "Training loss (for one batch) at step 75: 2.0657\n",
      "Seen so far: 4864 samples\n",
      "Training loss (for one batch) at step 76: 2.7695\n",
      "Seen so far: 4928 samples\n",
      "Training loss (for one batch) at step 77: 2.2238\n",
      "Seen so far: 4992 samples\n",
      "Training loss (for one batch) at step 78: 3.4810\n",
      "Seen so far: 5056 samples\n",
      "Training loss (for one batch) at step 79: 1.9660\n",
      "Seen so far: 5120 samples\n",
      "Training loss (for one batch) at step 80: 2.3605\n",
      "Seen so far: 5184 samples\n",
      "Training loss (for one batch) at step 81: 3.5907\n",
      "Seen so far: 5248 samples\n",
      "Training loss (for one batch) at step 82: 2.2755\n",
      "Seen so far: 5312 samples\n",
      "Training loss (for one batch) at step 83: 1.9624\n",
      "Seen so far: 5376 samples\n",
      "Training loss (for one batch) at step 84: 1.8506\n",
      "Seen so far: 5440 samples\n",
      "Training loss (for one batch) at step 85: 2.2443\n",
      "Seen so far: 5504 samples\n",
      "Training loss (for one batch) at step 86: 2.0684\n",
      "Seen so far: 5568 samples\n",
      "Training loss (for one batch) at step 87: 1.5421\n",
      "Seen so far: 5632 samples\n",
      "Training loss (for one batch) at step 88: 2.1270\n",
      "Seen so far: 5696 samples\n",
      "Training loss (for one batch) at step 89: 2.0061\n",
      "Seen so far: 5760 samples\n",
      "Training loss (for one batch) at step 90: 1.8763\n",
      "Seen so far: 5824 samples\n",
      "Training loss (for one batch) at step 91: 3.4670\n",
      "Seen so far: 5888 samples\n",
      "Training loss (for one batch) at step 92: 1.7262\n",
      "Seen so far: 5952 samples\n",
      "Training loss (for one batch) at step 93: 2.2801\n",
      "Seen so far: 6016 samples\n",
      "Training loss (for one batch) at step 94: 2.9122\n",
      "Seen so far: 6080 samples\n",
      "Training loss (for one batch) at step 95: 2.0691\n",
      "Seen so far: 6144 samples\n",
      "Training loss (for one batch) at step 96: 2.0571\n",
      "Seen so far: 6208 samples\n",
      "Training loss (for one batch) at step 97: 3.1124\n",
      "Seen so far: 6272 samples\n",
      "Training loss (for one batch) at step 98: 1.7110\n",
      "Seen so far: 6336 samples\n",
      "Training loss (for one batch) at step 99: 2.2492\n",
      "Seen so far: 6400 samples\n",
      "Training loss (for one batch) at step 100: 2.8067\n",
      "Seen so far: 6464 samples\n",
      "Training loss (for one batch) at step 101: 1.9166\n",
      "Seen so far: 6528 samples\n",
      "Training loss (for one batch) at step 102: 3.3099\n",
      "Seen so far: 6592 samples\n",
      "Training loss (for one batch) at step 103: 1.9452\n",
      "Seen so far: 6656 samples\n",
      "Training loss (for one batch) at step 104: 1.9867\n",
      "Seen so far: 6720 samples\n",
      "Training loss (for one batch) at step 105: 1.8393\n",
      "Seen so far: 6784 samples\n",
      "Training loss (for one batch) at step 106: 1.5937\n",
      "Seen so far: 6848 samples\n",
      "Training loss (for one batch) at step 107: 1.6227\n",
      "Seen so far: 6912 samples\n",
      "Training loss (for one batch) at step 108: 2.0090\n",
      "Seen so far: 6976 samples\n",
      "Training loss (for one batch) at step 109: 2.1495\n",
      "Seen so far: 7040 samples\n",
      "Training loss (for one batch) at step 110: 2.2292\n",
      "Seen so far: 7104 samples\n",
      "Training loss (for one batch) at step 111: 2.0877\n",
      "Seen so far: 7168 samples\n",
      "Training loss (for one batch) at step 112: 1.6966\n",
      "Seen so far: 7232 samples\n",
      "Training loss (for one batch) at step 113: 2.2319\n",
      "Seen so far: 7296 samples\n",
      "Training loss (for one batch) at step 114: 1.6920\n",
      "Seen so far: 7360 samples\n",
      "Training loss (for one batch) at step 115: 2.2360\n",
      "Seen so far: 7424 samples\n",
      "Training loss (for one batch) at step 116: 1.3273\n",
      "Seen so far: 7488 samples\n",
      "Training loss (for one batch) at step 117: 1.9344\n",
      "Seen so far: 7552 samples\n",
      "Training loss (for one batch) at step 118: 2.1036\n",
      "Seen so far: 7616 samples\n",
      "Training loss (for one batch) at step 119: 1.7491\n",
      "Seen so far: 7680 samples\n",
      "Training loss (for one batch) at step 120: 1.7448\n",
      "Seen so far: 7744 samples\n",
      "Training loss (for one batch) at step 121: 1.5855\n",
      "Seen so far: 7808 samples\n",
      "Training loss (for one batch) at step 122: 1.7679\n",
      "Seen so far: 7872 samples\n",
      "Training loss (for one batch) at step 123: 1.4494\n",
      "Seen so far: 7936 samples\n",
      "Training loss (for one batch) at step 124: 1.5845\n",
      "Seen so far: 8000 samples\n",
      "Training loss (for one batch) at step 125: 2.1358\n",
      "Seen so far: 8064 samples\n",
      "Training loss (for one batch) at step 126: 1.7194\n",
      "Seen so far: 8128 samples\n",
      "Training loss (for one batch) at step 127: 1.8153\n",
      "Seen so far: 8192 samples\n",
      "Training loss (for one batch) at step 128: 1.5674\n",
      "Seen so far: 8256 samples\n",
      "Training loss (for one batch) at step 129: 1.7749\n",
      "Seen so far: 8320 samples\n",
      "Training loss (for one batch) at step 130: 1.8010\n",
      "Seen so far: 8384 samples\n",
      "Training loss (for one batch) at step 131: 1.5688\n",
      "Seen so far: 8448 samples\n",
      "Training loss (for one batch) at step 132: 1.4875\n",
      "Seen so far: 8512 samples\n",
      "Training loss (for one batch) at step 133: 1.7817\n",
      "Seen so far: 8576 samples\n",
      "Training loss (for one batch) at step 134: 1.7011\n",
      "Seen so far: 8640 samples\n",
      "Training loss (for one batch) at step 135: 1.6799\n",
      "Seen so far: 8704 samples\n",
      "Training loss (for one batch) at step 136: 2.3357\n",
      "Seen so far: 8768 samples\n",
      "Training loss (for one batch) at step 137: 1.3973\n",
      "Seen so far: 8832 samples\n",
      "Training loss (for one batch) at step 138: 1.6561\n",
      "Seen so far: 8896 samples\n",
      "Training loss (for one batch) at step 139: 2.0119\n",
      "Seen so far: 8960 samples\n",
      "Training loss (for one batch) at step 140: 1.9728\n",
      "Seen so far: 9024 samples\n",
      "Training loss (for one batch) at step 141: 1.9984\n",
      "Seen so far: 9088 samples\n",
      "Training loss (for one batch) at step 142: 2.1043\n",
      "Seen so far: 9152 samples\n",
      "Training loss (for one batch) at step 143: 1.5194\n",
      "Seen so far: 9216 samples\n",
      "Training loss (for one batch) at step 144: 1.9401\n",
      "Seen so far: 9280 samples\n",
      "Training loss (for one batch) at step 145: 1.5667\n",
      "Seen so far: 9344 samples\n",
      "Training loss (for one batch) at step 146: 2.0565\n",
      "Seen so far: 9408 samples\n",
      "Training loss (for one batch) at step 147: 1.5768\n",
      "Seen so far: 9472 samples\n",
      "Training loss (for one batch) at step 148: 1.6943\n",
      "Seen so far: 9536 samples\n",
      "Training loss (for one batch) at step 149: 1.7964\n",
      "Seen so far: 9600 samples\n",
      "Training loss (for one batch) at step 150: 1.8417\n",
      "Seen so far: 9664 samples\n",
      "Training loss (for one batch) at step 151: 1.8843\n",
      "Seen so far: 9728 samples\n",
      "Training loss (for one batch) at step 152: 1.9413\n",
      "Seen so far: 9792 samples\n",
      "Training loss (for one batch) at step 153: 2.3845\n",
      "Seen so far: 9856 samples\n",
      "Training loss (for one batch) at step 154: 2.1379\n",
      "Seen so far: 9920 samples\n",
      "Training loss (for one batch) at step 155: 2.2905\n",
      "Seen so far: 9984 samples\n",
      "Training loss (for one batch) at step 156: 2.5780\n",
      "Seen so far: 10048 samples\n",
      "Training loss (for one batch) at step 157: 1.6184\n",
      "Seen so far: 10112 samples\n",
      "Training loss (for one batch) at step 158: 1.8037\n",
      "Seen so far: 10176 samples\n",
      "Training loss (for one batch) at step 159: 2.1585\n",
      "Seen so far: 10240 samples\n",
      "Training loss (for one batch) at step 160: 1.5200\n",
      "Seen so far: 10304 samples\n",
      "Training loss (for one batch) at step 161: 2.5464\n",
      "Seen so far: 10368 samples\n",
      "Training loss (for one batch) at step 162: 1.9102\n",
      "Seen so far: 10432 samples\n",
      "Training loss (for one batch) at step 163: 1.6428\n",
      "Seen so far: 10496 samples\n",
      "Training loss (for one batch) at step 164: 1.5066\n",
      "Seen so far: 10560 samples\n",
      "Training loss (for one batch) at step 165: 1.6484\n",
      "Seen so far: 10624 samples\n",
      "Training loss (for one batch) at step 166: 2.5208\n",
      "Seen so far: 10688 samples\n",
      "Training loss (for one batch) at step 167: 1.1813\n",
      "Seen so far: 10752 samples\n",
      "Training loss (for one batch) at step 168: 2.2040\n",
      "Seen so far: 10816 samples\n",
      "Training loss (for one batch) at step 169: 1.5229\n",
      "Seen so far: 10880 samples\n",
      "Training loss (for one batch) at step 170: 1.6950\n",
      "Seen so far: 10944 samples\n",
      "Training loss (for one batch) at step 171: 2.2447\n",
      "Seen so far: 11008 samples\n",
      "Training loss (for one batch) at step 172: 2.0238\n",
      "Seen so far: 11072 samples\n",
      "Training loss (for one batch) at step 173: 1.9505\n",
      "Seen so far: 11136 samples\n",
      "Training loss (for one batch) at step 174: 2.5762\n",
      "Seen so far: 11200 samples\n",
      "Training loss (for one batch) at step 175: 1.9379\n",
      "Seen so far: 11264 samples\n",
      "Training loss (for one batch) at step 176: 1.5225\n",
      "Seen so far: 11328 samples\n",
      "Training loss (for one batch) at step 177: 1.5827\n",
      "Seen so far: 11392 samples\n",
      "Training loss (for one batch) at step 178: 1.8360\n",
      "Seen so far: 11456 samples\n",
      "Training loss (for one batch) at step 179: 1.9567\n",
      "Seen so far: 11520 samples\n",
      "Training loss (for one batch) at step 180: 1.5354\n",
      "Seen so far: 11584 samples\n",
      "Training loss (for one batch) at step 181: 1.9590\n",
      "Seen so far: 11648 samples\n",
      "Training loss (for one batch) at step 182: 1.5169\n",
      "Seen so far: 11712 samples\n",
      "Training loss (for one batch) at step 183: 1.8425\n",
      "Seen so far: 11776 samples\n",
      "Training loss (for one batch) at step 184: 1.7420\n",
      "Seen so far: 11840 samples\n",
      "Training loss (for one batch) at step 185: 1.8281\n",
      "Seen so far: 11904 samples\n",
      "Training loss (for one batch) at step 186: 1.6145\n",
      "Seen so far: 11968 samples\n",
      "Training loss (for one batch) at step 187: 1.9020\n",
      "Seen so far: 12032 samples\n",
      "Training loss (for one batch) at step 188: 2.0397\n",
      "Seen so far: 12096 samples\n",
      "Training loss (for one batch) at step 189: 1.7676\n",
      "Seen so far: 12160 samples\n",
      "Training loss (for one batch) at step 190: 1.5363\n",
      "Seen so far: 12224 samples\n",
      "Training loss (for one batch) at step 191: 1.8368\n",
      "Seen so far: 12288 samples\n",
      "Training loss (for one batch) at step 192: 1.8059\n",
      "Seen so far: 12352 samples\n",
      "Training loss (for one batch) at step 193: 1.6599\n",
      "Seen so far: 12416 samples\n",
      "Training loss (for one batch) at step 194: 1.7156\n",
      "Seen so far: 12480 samples\n",
      "Training loss (for one batch) at step 195: 1.8295\n",
      "Seen so far: 12544 samples\n",
      "Training loss (for one batch) at step 196: 1.4787\n",
      "Seen so far: 12608 samples\n",
      "Training loss (for one batch) at step 197: 1.6850\n",
      "Seen so far: 12672 samples\n",
      "Training loss (for one batch) at step 198: 1.5890\n",
      "Seen so far: 12736 samples\n",
      "Training loss (for one batch) at step 199: 2.9022\n",
      "Seen so far: 12800 samples\n",
      "Training loss (for one batch) at step 201: 1.8230\n",
      "Seen so far: 12928 samples\n",
      "Training loss (for one batch) at step 202: 2.3326\n",
      "Seen so far: 12992 samples\n",
      "Training loss (for one batch) at step 203: 1.7858\n",
      "Seen so far: 13056 samples\n",
      "Training loss (for one batch) at step 204: 1.4669\n",
      "Seen so far: 13120 samples\n",
      "Training loss (for one batch) at step 205: 2.4936\n",
      "Seen so far: 13184 samples\n",
      "Training loss (for one batch) at step 206: 1.7076\n",
      "Seen so far: 13248 samples\n",
      "Training loss (for one batch) at step 207: 1.5110\n",
      "Seen so far: 13312 samples\n",
      "Training loss (for one batch) at step 208: 1.9521\n",
      "Seen so far: 13376 samples\n",
      "Training loss (for one batch) at step 209: 1.4903\n",
      "Seen so far: 13440 samples\n",
      "Training loss (for one batch) at step 210: 1.8070\n",
      "Seen so far: 13504 samples\n",
      "Training loss (for one batch) at step 211: 1.3783\n",
      "Seen so far: 13568 samples\n",
      "Training loss (for one batch) at step 212: 1.5613\n",
      "Seen so far: 13632 samples\n",
      "Training loss (for one batch) at step 213: 2.3967\n",
      "Seen so far: 13696 samples\n",
      "Training loss (for one batch) at step 214: 1.8399\n",
      "Seen so far: 13760 samples\n",
      "Training loss (for one batch) at step 215: 1.6360\n",
      "Seen so far: 13824 samples\n",
      "Training loss (for one batch) at step 216: 1.4603\n",
      "Seen so far: 13888 samples\n",
      "Training loss (for one batch) at step 217: 1.6415\n",
      "Seen so far: 13952 samples\n",
      "Training loss (for one batch) at step 218: 2.0635\n",
      "Seen so far: 14016 samples\n",
      "Training loss (for one batch) at step 219: 1.6598\n",
      "Seen so far: 14080 samples\n",
      "Training loss (for one batch) at step 220: 1.8343\n",
      "Seen so far: 14144 samples\n",
      "Training loss (for one batch) at step 221: 1.9094\n",
      "Seen so far: 14208 samples\n",
      "Training loss (for one batch) at step 222: 1.9785\n",
      "Seen so far: 14272 samples\n",
      "Training loss (for one batch) at step 223: 1.5073\n",
      "Seen so far: 14336 samples\n",
      "Training loss (for one batch) at step 224: 1.6611\n",
      "Seen so far: 14400 samples\n",
      "Training loss (for one batch) at step 225: 1.3858\n",
      "Seen so far: 14464 samples\n",
      "Training loss (for one batch) at step 226: 1.7154\n",
      "Seen so far: 14528 samples\n",
      "Training loss (for one batch) at step 227: 1.6676\n",
      "Seen so far: 14592 samples\n",
      "Training loss (for one batch) at step 228: 2.1600\n",
      "Seen so far: 14656 samples\n",
      "Training loss (for one batch) at step 229: 2.0413\n",
      "Seen so far: 14720 samples\n",
      "Training loss (for one batch) at step 230: 2.6048\n",
      "Seen so far: 14784 samples\n",
      "Training loss (for one batch) at step 231: 1.3205\n",
      "Seen so far: 14848 samples\n",
      "Training loss (for one batch) at step 232: 1.2873\n",
      "Seen so far: 14912 samples\n",
      "Training loss (for one batch) at step 233: 2.2928\n",
      "Seen so far: 14976 samples\n",
      "Training loss (for one batch) at step 234: 1.4549\n",
      "Seen so far: 15040 samples\n",
      "Training loss (for one batch) at step 235: 1.4959\n",
      "Seen so far: 15104 samples\n",
      "Training loss (for one batch) at step 236: 1.4009\n",
      "Seen so far: 15168 samples\n",
      "Training loss (for one batch) at step 237: 2.5712\n",
      "Seen so far: 15232 samples\n",
      "Training loss (for one batch) at step 238: 1.6343\n",
      "Seen so far: 15296 samples\n",
      "Training loss (for one batch) at step 239: 1.9338\n",
      "Seen so far: 15360 samples\n",
      "Training loss (for one batch) at step 240: 2.2595\n",
      "Seen so far: 15424 samples\n",
      "Training loss (for one batch) at step 241: 1.3804\n",
      "Seen so far: 15488 samples\n",
      "Training loss (for one batch) at step 242: 1.4367\n",
      "Seen so far: 15552 samples\n",
      "Training loss (for one batch) at step 243: 2.0981\n",
      "Seen so far: 15616 samples\n",
      "Training loss (for one batch) at step 244: 1.4335\n",
      "Seen so far: 15680 samples\n",
      "Training loss (for one batch) at step 245: 1.8563\n",
      "Seen so far: 15744 samples\n",
      "Training loss (for one batch) at step 246: 2.0711\n",
      "Seen so far: 15808 samples\n",
      "Training loss (for one batch) at step 247: 1.8278\n",
      "Seen so far: 15872 samples\n",
      "Training loss (for one batch) at step 248: 1.6805\n",
      "Seen so far: 15936 samples\n",
      "Training loss (for one batch) at step 249: 1.7259\n",
      "Seen so far: 16000 samples\n",
      "Training loss (for one batch) at step 250: 1.7906\n",
      "Seen so far: 16064 samples\n",
      "Training loss (for one batch) at step 251: 1.5987\n",
      "Seen so far: 16128 samples\n",
      "Training loss (for one batch) at step 252: 1.7256\n",
      "Seen so far: 16192 samples\n",
      "Training loss (for one batch) at step 253: 1.6388\n",
      "Seen so far: 16256 samples\n",
      "Training loss (for one batch) at step 254: 1.7566\n",
      "Seen so far: 16320 samples\n",
      "Training loss (for one batch) at step 255: 1.2988\n",
      "Seen so far: 16384 samples\n",
      "Training loss (for one batch) at step 256: 1.6398\n",
      "Seen so far: 16448 samples\n",
      "Training loss (for one batch) at step 257: 1.6217\n",
      "Seen so far: 16512 samples\n",
      "Training loss (for one batch) at step 258: 1.5958\n",
      "Seen so far: 16576 samples\n",
      "Training loss (for one batch) at step 259: 1.9124\n",
      "Seen so far: 16640 samples\n",
      "Training loss (for one batch) at step 260: 1.7425\n",
      "Seen so far: 16704 samples\n",
      "Training loss (for one batch) at step 261: 2.0610\n",
      "Seen so far: 16768 samples\n",
      "Training loss (for one batch) at step 262: 1.8870\n",
      "Seen so far: 16832 samples\n",
      "Training loss (for one batch) at step 263: 2.2277\n",
      "Seen so far: 16896 samples\n",
      "Training loss (for one batch) at step 264: 1.7613\n",
      "Seen so far: 16960 samples\n",
      "Training loss (for one batch) at step 265: 1.1494\n",
      "Seen so far: 17024 samples\n",
      "Training loss (for one batch) at step 266: 1.3213\n",
      "Seen so far: 17088 samples\n",
      "Training loss (for one batch) at step 267: 1.3547\n",
      "Seen so far: 17152 samples\n",
      "Training loss (for one batch) at step 268: 1.8754\n",
      "Seen so far: 17216 samples\n",
      "Training loss (for one batch) at step 269: 1.5174\n",
      "Seen so far: 17280 samples\n",
      "Training loss (for one batch) at step 270: 1.2834\n",
      "Seen so far: 17344 samples\n",
      "Training loss (for one batch) at step 271: 2.2448\n",
      "Seen so far: 17408 samples\n",
      "Training loss (for one batch) at step 272: 2.0938\n",
      "Seen so far: 17472 samples\n",
      "Training loss (for one batch) at step 273: 2.0506\n",
      "Seen so far: 17536 samples\n",
      "Training loss (for one batch) at step 274: 2.0636\n",
      "Seen so far: 17600 samples\n",
      "Training loss (for one batch) at step 275: 1.4126\n",
      "Seen so far: 17664 samples\n",
      "Training loss (for one batch) at step 276: 1.7623\n",
      "Seen so far: 17728 samples\n",
      "Training loss (for one batch) at step 277: 1.8966\n",
      "Seen so far: 17792 samples\n",
      "Training loss (for one batch) at step 278: 1.6915\n",
      "Seen so far: 17856 samples\n",
      "Training loss (for one batch) at step 279: 1.5851\n",
      "Seen so far: 17920 samples\n",
      "Training loss (for one batch) at step 280: 1.5046\n",
      "Seen so far: 17984 samples\n",
      "Training loss (for one batch) at step 281: 2.2301\n",
      "Seen so far: 18048 samples\n",
      "Training loss (for one batch) at step 282: 2.4991\n",
      "Seen so far: 18112 samples\n",
      "Training loss (for one batch) at step 283: 1.8446\n",
      "Seen so far: 18176 samples\n",
      "Training loss (for one batch) at step 284: 3.4785\n",
      "Seen so far: 18240 samples\n",
      "Training loss (for one batch) at step 285: 1.7511\n",
      "Seen so far: 18304 samples\n",
      "Training loss (for one batch) at step 286: 1.7737\n",
      "Seen so far: 18368 samples\n",
      "Training loss (for one batch) at step 287: 1.3824\n",
      "Seen so far: 18432 samples\n",
      "Training loss (for one batch) at step 288: 1.7659\n",
      "Seen so far: 18496 samples\n",
      "Training loss (for one batch) at step 289: 1.6806\n",
      "Seen so far: 18560 samples\n",
      "Training loss (for one batch) at step 290: 1.5332\n",
      "Seen so far: 18624 samples\n",
      "Training loss (for one batch) at step 291: 1.4214\n",
      "Seen so far: 18688 samples\n",
      "Training loss (for one batch) at step 292: 1.3370\n",
      "Seen so far: 18752 samples\n",
      "Training loss (for one batch) at step 293: 1.5413\n",
      "Seen so far: 18816 samples\n",
      "Training loss (for one batch) at step 294: 1.8646\n",
      "Seen so far: 18880 samples\n",
      "Training loss (for one batch) at step 295: 1.7483\n",
      "Seen so far: 18944 samples\n",
      "Training loss (for one batch) at step 296: 2.0828\n",
      "Seen so far: 19008 samples\n",
      "Training loss (for one batch) at step 297: 1.9576\n",
      "Seen so far: 19072 samples\n",
      "Training loss (for one batch) at step 298: 1.7357\n",
      "Seen so far: 19136 samples\n",
      "Training loss (for one batch) at step 299: 2.0315\n",
      "Seen so far: 19200 samples\n",
      "Training loss (for one batch) at step 300: 1.7516\n",
      "Seen so far: 19264 samples\n",
      "Training loss (for one batch) at step 301: 1.4591\n",
      "Seen so far: 19328 samples\n",
      "Training loss (for one batch) at step 302: 1.4140\n",
      "Seen so far: 19392 samples\n",
      "Training loss (for one batch) at step 303: 1.4323\n",
      "Seen so far: 19456 samples\n",
      "Training loss (for one batch) at step 304: 2.2656\n",
      "Seen so far: 19520 samples\n",
      "Training loss (for one batch) at step 305: 1.7227\n",
      "Seen so far: 19584 samples\n",
      "Training loss (for one batch) at step 306: 1.4595\n",
      "Seen so far: 19648 samples\n",
      "Training loss (for one batch) at step 307: 1.2903\n",
      "Seen so far: 19712 samples\n",
      "Training loss (for one batch) at step 308: 1.4728\n",
      "Seen so far: 19776 samples\n",
      "Training loss (for one batch) at step 309: 1.7072\n",
      "Seen so far: 19840 samples\n",
      "Training loss (for one batch) at step 310: 1.5571\n",
      "Seen so far: 19904 samples\n",
      "Training loss (for one batch) at step 311: 1.5061\n",
      "Seen so far: 19968 samples\n",
      "Training loss (for one batch) at step 312: 1.4512\n",
      "Seen so far: 20032 samples\n",
      "Training loss (for one batch) at step 313: 1.8851\n",
      "Seen so far: 20096 samples\n",
      "Training loss (for one batch) at step 314: 1.5089\n",
      "Seen so far: 20160 samples\n",
      "Training loss (for one batch) at step 315: 1.1863\n",
      "Seen so far: 20224 samples\n",
      "Training loss (for one batch) at step 316: 1.6424\n",
      "Seen so far: 20288 samples\n",
      "Training loss (for one batch) at step 317: 1.8221\n",
      "Seen so far: 20352 samples\n",
      "Training loss (for one batch) at step 318: 1.9380\n",
      "Seen so far: 20416 samples\n",
      "Training loss (for one batch) at step 319: 1.7139\n",
      "Seen so far: 20480 samples\n",
      "Training loss (for one batch) at step 320: 2.4220\n",
      "Seen so far: 20544 samples\n",
      "Training loss (for one batch) at step 321: 1.4392\n",
      "Seen so far: 20608 samples\n",
      "Training loss (for one batch) at step 322: 1.7677\n",
      "Seen so far: 20672 samples\n",
      "Training loss (for one batch) at step 323: 2.1447\n",
      "Seen so far: 20736 samples\n",
      "Training loss (for one batch) at step 324: 1.2795\n",
      "Seen so far: 20800 samples\n",
      "Training loss (for one batch) at step 325: 1.8929\n",
      "Seen so far: 20864 samples\n",
      "Training loss (for one batch) at step 326: 1.9066\n",
      "Seen so far: 20928 samples\n",
      "Training loss (for one batch) at step 327: 1.4778\n",
      "Seen so far: 20992 samples\n",
      "Training loss (for one batch) at step 328: 1.3995\n",
      "Seen so far: 21056 samples\n",
      "Training loss (for one batch) at step 329: 2.3278\n",
      "Seen so far: 21120 samples\n",
      "Training loss (for one batch) at step 330: 1.8028\n",
      "Seen so far: 21184 samples\n",
      "Training loss (for one batch) at step 331: 1.3367\n",
      "Seen so far: 21248 samples\n",
      "Training loss (for one batch) at step 332: 1.8002\n",
      "Seen so far: 21312 samples\n",
      "Training loss (for one batch) at step 333: 1.5168\n",
      "Seen so far: 21376 samples\n",
      "Training loss (for one batch) at step 334: 2.2032\n",
      "Seen so far: 21440 samples\n",
      "Training loss (for one batch) at step 335: 1.9170\n",
      "Seen so far: 21504 samples\n",
      "Training loss (for one batch) at step 336: 1.6618\n",
      "Seen so far: 21568 samples\n",
      "Training loss (for one batch) at step 337: 1.3703\n",
      "Seen so far: 21632 samples\n",
      "Training loss (for one batch) at step 338: 1.3311\n",
      "Seen so far: 21696 samples\n",
      "Training loss (for one batch) at step 339: 1.7835\n",
      "Seen so far: 21760 samples\n",
      "Training loss (for one batch) at step 340: 1.2261\n",
      "Seen so far: 21824 samples\n",
      "Training loss (for one batch) at step 341: 1.2160\n",
      "Seen so far: 21888 samples\n",
      "Training loss (for one batch) at step 342: 1.5789\n",
      "Seen so far: 21952 samples\n",
      "Training loss (for one batch) at step 343: 1.6448\n",
      "Seen so far: 22016 samples\n",
      "Training loss (for one batch) at step 344: 1.2843\n",
      "Seen so far: 22080 samples\n",
      "Training loss (for one batch) at step 345: 1.3957\n",
      "Seen so far: 22144 samples\n",
      "Training loss (for one batch) at step 346: 1.7369\n",
      "Seen so far: 22208 samples\n",
      "Training loss (for one batch) at step 347: 1.2752\n",
      "Seen so far: 22272 samples\n",
      "Training loss (for one batch) at step 348: 1.3239\n",
      "Seen so far: 22336 samples\n",
      "Training loss (for one batch) at step 349: 1.1944\n",
      "Seen so far: 22400 samples\n",
      "Training loss (for one batch) at step 350: 1.6514\n",
      "Seen so far: 22464 samples\n",
      "Training loss (for one batch) at step 351: 1.3734\n",
      "Seen so far: 22528 samples\n",
      "Training loss (for one batch) at step 352: 1.9924\n",
      "Seen so far: 22592 samples\n",
      "Training loss (for one batch) at step 353: 1.8149\n",
      "Seen so far: 22656 samples\n",
      "Training loss (for one batch) at step 354: 1.7537\n",
      "Seen so far: 22720 samples\n",
      "Training loss (for one batch) at step 355: 1.5690\n",
      "Seen so far: 22784 samples\n",
      "Training loss (for one batch) at step 356: 1.5815\n",
      "Seen so far: 22848 samples\n",
      "Training loss (for one batch) at step 357: 1.2254\n",
      "Seen so far: 22912 samples\n",
      "Training loss (for one batch) at step 358: 1.8941\n",
      "Seen so far: 22976 samples\n",
      "Training loss (for one batch) at step 359: 1.4612\n",
      "Seen so far: 23040 samples\n",
      "Training loss (for one batch) at step 360: 1.8873\n",
      "Seen so far: 23104 samples\n",
      "Training loss (for one batch) at step 361: 1.9002\n",
      "Seen so far: 23168 samples\n",
      "Training loss (for one batch) at step 362: 1.3134\n",
      "Seen so far: 23232 samples\n",
      "Training loss (for one batch) at step 363: 1.3148\n",
      "Seen so far: 23296 samples\n",
      "Training loss (for one batch) at step 364: 1.1800\n",
      "Seen so far: 23360 samples\n",
      "Training loss (for one batch) at step 365: 1.7403\n",
      "Seen so far: 23424 samples\n",
      "Training loss (for one batch) at step 366: 1.2870\n",
      "Seen so far: 23488 samples\n",
      "Training loss (for one batch) at step 367: 2.2282\n",
      "Seen so far: 23552 samples\n",
      "Training loss (for one batch) at step 368: 1.4580\n",
      "Seen so far: 23616 samples\n",
      "Training loss (for one batch) at step 369: 1.4415\n",
      "Seen so far: 23680 samples\n",
      "Training loss (for one batch) at step 370: 1.7620\n",
      "Seen so far: 23744 samples\n",
      "Training loss (for one batch) at step 371: 1.5707\n",
      "Seen so far: 23808 samples\n",
      "Training loss (for one batch) at step 372: 1.5782\n",
      "Seen so far: 23872 samples\n",
      "Training loss (for one batch) at step 373: 1.4994\n",
      "Seen so far: 23936 samples\n",
      "Training loss (for one batch) at step 374: 1.3417\n",
      "Seen so far: 24000 samples\n",
      "Training loss (for one batch) at step 375: 1.2953\n",
      "Seen so far: 24064 samples\n",
      "Training loss (for one batch) at step 376: 1.3810\n",
      "Seen so far: 24128 samples\n",
      "Training loss (for one batch) at step 377: 1.4778\n",
      "Seen so far: 24192 samples\n",
      "Training loss (for one batch) at step 378: 1.3560\n",
      "Seen so far: 24256 samples\n",
      "Training loss (for one batch) at step 379: 1.3795\n",
      "Seen so far: 24320 samples\n",
      "Training loss (for one batch) at step 380: 1.4958\n",
      "Seen so far: 24384 samples\n",
      "Training loss (for one batch) at step 381: 1.3659\n",
      "Seen so far: 24448 samples\n",
      "Training loss (for one batch) at step 382: 1.4787\n",
      "Seen so far: 24512 samples\n",
      "Training loss (for one batch) at step 383: 1.6037\n",
      "Seen so far: 24576 samples\n",
      "Training loss (for one batch) at step 384: 1.4423\n",
      "Seen so far: 24640 samples\n",
      "Training loss (for one batch) at step 385: 1.5602\n",
      "Seen so far: 24704 samples\n",
      "Training loss (for one batch) at step 386: 1.9872\n",
      "Seen so far: 24768 samples\n",
      "Training loss (for one batch) at step 387: 1.1779\n",
      "Seen so far: 24832 samples\n",
      "Training loss (for one batch) at step 388: 1.3709\n",
      "Seen so far: 24896 samples\n",
      "Training loss (for one batch) at step 389: 1.5733\n",
      "Seen so far: 24960 samples\n",
      "Training loss (for one batch) at step 390: 1.1850\n",
      "Seen so far: 25024 samples\n",
      "Training loss (for one batch) at step 391: 1.2667\n",
      "Seen so far: 25088 samples\n",
      "Training loss (for one batch) at step 392: 1.9812\n",
      "Seen so far: 25152 samples\n",
      "Training loss (for one batch) at step 393: 1.5886\n",
      "Seen so far: 25216 samples\n",
      "Training loss (for one batch) at step 394: 1.2705\n",
      "Seen so far: 25280 samples\n",
      "Training loss (for one batch) at step 395: 1.5617\n",
      "Seen so far: 25344 samples\n",
      "Training loss (for one batch) at step 396: 1.5753\n",
      "Seen so far: 25408 samples\n",
      "Training loss (for one batch) at step 397: 1.3988\n",
      "Seen so far: 25472 samples\n",
      "Training loss (for one batch) at step 398: 1.2666\n",
      "Seen so far: 25536 samples\n",
      "Training loss (for one batch) at step 399: 1.3680\n",
      "Seen so far: 25600 samples\n",
      "Training loss (for one batch) at step 401: 1.6313\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 402: 2.0268\n",
      "Seen so far: 25792 samples\n",
      "Training loss (for one batch) at step 403: 1.8821\n",
      "Seen so far: 25856 samples\n",
      "Training loss (for one batch) at step 404: 1.4287\n",
      "Seen so far: 25920 samples\n",
      "Training loss (for one batch) at step 405: 1.5707\n",
      "Seen so far: 25984 samples\n",
      "Training loss (for one batch) at step 406: 1.1249\n",
      "Seen so far: 26048 samples\n",
      "Training loss (for one batch) at step 407: 1.2528\n",
      "Seen so far: 26112 samples\n",
      "Training loss (for one batch) at step 408: 1.3328\n",
      "Seen so far: 26176 samples\n",
      "Training loss (for one batch) at step 409: 1.6729\n",
      "Seen so far: 26240 samples\n",
      "Training loss (for one batch) at step 410: 1.6087\n",
      "Seen so far: 26304 samples\n",
      "Training loss (for one batch) at step 411: 1.3440\n",
      "Seen so far: 26368 samples\n",
      "Training loss (for one batch) at step 412: 1.5788\n",
      "Seen so far: 26432 samples\n",
      "Training loss (for one batch) at step 413: 1.3143\n",
      "Seen so far: 26496 samples\n",
      "Training loss (for one batch) at step 414: 1.4849\n",
      "Seen so far: 26560 samples\n",
      "Training loss (for one batch) at step 415: 1.4480\n",
      "Seen so far: 26624 samples\n",
      "Training loss (for one batch) at step 416: 1.7638\n",
      "Seen so far: 26688 samples\n",
      "Training loss (for one batch) at step 417: 1.9680\n",
      "Seen so far: 26752 samples\n",
      "Training loss (for one batch) at step 418: 1.2130\n",
      "Seen so far: 26816 samples\n",
      "Training loss (for one batch) at step 419: 1.2676\n",
      "Seen so far: 26880 samples\n",
      "Training loss (for one batch) at step 420: 1.6709\n",
      "Seen so far: 26944 samples\n",
      "Training loss (for one batch) at step 421: 1.7949\n",
      "Seen so far: 27008 samples\n",
      "Training loss (for one batch) at step 422: 1.3202\n",
      "Seen so far: 27072 samples\n",
      "Training loss (for one batch) at step 423: 1.1966\n",
      "Seen so far: 27136 samples\n",
      "Training loss (for one batch) at step 424: 1.3387\n",
      "Seen so far: 27200 samples\n",
      "Training loss (for one batch) at step 425: 1.7694\n",
      "Seen so far: 27264 samples\n",
      "Training loss (for one batch) at step 426: 1.6209\n",
      "Seen so far: 27328 samples\n",
      "Training loss (for one batch) at step 427: 1.1293\n",
      "Seen so far: 27392 samples\n",
      "Training loss (for one batch) at step 428: 1.7451\n",
      "Seen so far: 27456 samples\n",
      "Training loss (for one batch) at step 429: 1.3809\n",
      "Seen so far: 27520 samples\n",
      "Training loss (for one batch) at step 430: 1.4843\n",
      "Seen so far: 27584 samples\n",
      "Training loss (for one batch) at step 431: 1.2593\n",
      "Seen so far: 27648 samples\n",
      "Training loss (for one batch) at step 432: 1.3733\n",
      "Seen so far: 27712 samples\n",
      "Training loss (for one batch) at step 433: 1.6450\n",
      "Seen so far: 27776 samples\n",
      "Training loss (for one batch) at step 434: 1.3437\n",
      "Seen so far: 27840 samples\n",
      "Training loss (for one batch) at step 435: 1.1541\n",
      "Seen so far: 27904 samples\n",
      "Training loss (for one batch) at step 436: 1.3801\n",
      "Seen so far: 27968 samples\n",
      "Training loss (for one batch) at step 437: 1.3816\n",
      "Seen so far: 28032 samples\n",
      "Training loss (for one batch) at step 438: 1.5666\n",
      "Seen so far: 28096 samples\n",
      "Training loss (for one batch) at step 439: 1.3225\n",
      "Seen so far: 28160 samples\n",
      "Training loss (for one batch) at step 440: 1.8929\n",
      "Seen so far: 28224 samples\n",
      "Training loss (for one batch) at step 441: 1.7513\n",
      "Seen so far: 28288 samples\n",
      "Training loss (for one batch) at step 442: 1.2813\n",
      "Seen so far: 28352 samples\n",
      "Training loss (for one batch) at step 443: 1.9181\n",
      "Seen so far: 28416 samples\n",
      "Training loss (for one batch) at step 444: 1.9242\n",
      "Seen so far: 28480 samples\n",
      "Training loss (for one batch) at step 445: 1.6209\n",
      "Seen so far: 28544 samples\n",
      "Training loss (for one batch) at step 446: 2.1156\n",
      "Seen so far: 28608 samples\n",
      "Training loss (for one batch) at step 447: 1.7701\n",
      "Seen so far: 28672 samples\n",
      "Training loss (for one batch) at step 448: 2.3732\n",
      "Seen so far: 28736 samples\n",
      "Training loss (for one batch) at step 449: 1.4651\n",
      "Seen so far: 28800 samples\n",
      "Training loss (for one batch) at step 450: 1.5855\n",
      "Seen so far: 28864 samples\n",
      "Training loss (for one batch) at step 451: 2.3114\n",
      "Seen so far: 28928 samples\n",
      "Training loss (for one batch) at step 452: 1.4242\n",
      "Seen so far: 28992 samples\n",
      "Training loss (for one batch) at step 453: 1.2938\n",
      "Seen so far: 29056 samples\n",
      "Training loss (for one batch) at step 454: 1.3438\n",
      "Seen so far: 29120 samples\n",
      "Training loss (for one batch) at step 455: 1.1530\n",
      "Seen so far: 29184 samples\n",
      "Training loss (for one batch) at step 456: 2.0273\n",
      "Seen so far: 29248 samples\n",
      "Training loss (for one batch) at step 457: 1.3831\n",
      "Seen so far: 29312 samples\n",
      "Training loss (for one batch) at step 458: 1.1850\n",
      "Seen so far: 29376 samples\n",
      "Training loss (for one batch) at step 459: 1.5596\n",
      "Seen so far: 29440 samples\n",
      "Training loss (for one batch) at step 460: 1.5459\n",
      "Seen so far: 29504 samples\n",
      "Training loss (for one batch) at step 461: 1.7027\n",
      "Seen so far: 29568 samples\n",
      "Training loss (for one batch) at step 462: 1.2919\n",
      "Seen so far: 29632 samples\n",
      "Training loss (for one batch) at step 463: 1.3853\n",
      "Seen so far: 29696 samples\n",
      "Training loss (for one batch) at step 464: 1.2634\n",
      "Seen so far: 29760 samples\n",
      "Training loss (for one batch) at step 465: 1.2806\n",
      "Seen so far: 29824 samples\n",
      "Training loss (for one batch) at step 466: 1.2873\n",
      "Seen so far: 29888 samples\n",
      "Training loss (for one batch) at step 467: 1.1142\n",
      "Seen so far: 29952 samples\n",
      "Training loss (for one batch) at step 468: 1.2289\n",
      "Seen so far: 30016 samples\n",
      "Training loss (for one batch) at step 469: 1.1290\n",
      "Seen so far: 30080 samples\n",
      "Training loss (for one batch) at step 470: 2.0849\n",
      "Seen so far: 30144 samples\n",
      "Training loss (for one batch) at step 471: 1.4226\n",
      "Seen so far: 30208 samples\n",
      "Training loss (for one batch) at step 472: 1.3154\n",
      "Seen so far: 30272 samples\n",
      "Training loss (for one batch) at step 473: 1.1587\n",
      "Seen so far: 30336 samples\n",
      "Training loss (for one batch) at step 474: 1.4912\n",
      "Seen so far: 30400 samples\n",
      "Training loss (for one batch) at step 475: 1.2650\n",
      "Seen so far: 30464 samples\n",
      "Training loss (for one batch) at step 476: 0.9708\n",
      "Seen so far: 30528 samples\n",
      "Training loss (for one batch) at step 477: 1.7643\n",
      "Seen so far: 30592 samples\n",
      "Training loss (for one batch) at step 478: 1.6954\n",
      "Seen so far: 30656 samples\n",
      "Training loss (for one batch) at step 479: 0.9233\n",
      "Seen so far: 30720 samples\n",
      "Training loss (for one batch) at step 480: 1.2122\n",
      "Seen so far: 30784 samples\n",
      "Training loss (for one batch) at step 481: 1.2293\n",
      "Seen so far: 30848 samples\n",
      "Training loss (for one batch) at step 482: 1.1898\n",
      "Seen so far: 30912 samples\n",
      "Training loss (for one batch) at step 483: 1.3731\n",
      "Seen so far: 30976 samples\n",
      "Training loss (for one batch) at step 484: 1.1754\n",
      "Seen so far: 31040 samples\n",
      "Training loss (for one batch) at step 485: 1.0931\n",
      "Seen so far: 31104 samples\n",
      "Training loss (for one batch) at step 486: 1.3189\n",
      "Seen so far: 31168 samples\n",
      "Training loss (for one batch) at step 487: 1.4215\n",
      "Seen so far: 31232 samples\n",
      "Training loss (for one batch) at step 488: 1.3174\n",
      "Seen so far: 31296 samples\n",
      "Training loss (for one batch) at step 489: 2.1997\n",
      "Seen so far: 31360 samples\n",
      "Training loss (for one batch) at step 490: 1.4140\n",
      "Seen so far: 31424 samples\n",
      "Training loss (for one batch) at step 491: 1.3859\n",
      "Seen so far: 31488 samples\n",
      "Training loss (for one batch) at step 492: 1.5396\n",
      "Seen so far: 31552 samples\n",
      "Training loss (for one batch) at step 493: 0.9995\n",
      "Seen so far: 31616 samples\n",
      "Training loss (for one batch) at step 494: 1.8552\n",
      "Seen so far: 31680 samples\n",
      "Training loss (for one batch) at step 495: 1.8896\n",
      "Seen so far: 31744 samples\n",
      "Training loss (for one batch) at step 496: 1.6143\n",
      "Seen so far: 31808 samples\n",
      "Training loss (for one batch) at step 497: 0.9820\n",
      "Seen so far: 31872 samples\n",
      "Training loss (for one batch) at step 498: 1.6293\n",
      "Seen so far: 31936 samples\n",
      "Training loss (for one batch) at step 499: 1.3018\n",
      "Seen so far: 32000 samples\n",
      "Training loss (for one batch) at step 500: 1.6507\n",
      "Seen so far: 32064 samples\n",
      "Training loss (for one batch) at step 501: 1.1738\n",
      "Seen so far: 32128 samples\n",
      "Training loss (for one batch) at step 502: 1.2946\n",
      "Seen so far: 32192 samples\n",
      "Training loss (for one batch) at step 503: 1.4489\n",
      "Seen so far: 32256 samples\n",
      "Training loss (for one batch) at step 504: 1.2983\n",
      "Seen so far: 32320 samples\n",
      "Training loss (for one batch) at step 505: 1.1996\n",
      "Seen so far: 32384 samples\n",
      "Training loss (for one batch) at step 506: 1.5073\n",
      "Seen so far: 32448 samples\n",
      "Training loss (for one batch) at step 507: 1.5846\n",
      "Seen so far: 32512 samples\n",
      "Training loss (for one batch) at step 508: 1.1827\n",
      "Seen so far: 32576 samples\n",
      "Training loss (for one batch) at step 509: 0.9792\n",
      "Seen so far: 32640 samples\n",
      "Training loss (for one batch) at step 510: 1.3765\n",
      "Seen so far: 32704 samples\n",
      "Training loss (for one batch) at step 511: 1.3966\n",
      "Seen so far: 32768 samples\n",
      "Training loss (for one batch) at step 512: 1.0347\n",
      "Seen so far: 32832 samples\n",
      "Training loss (for one batch) at step 513: 1.8327\n",
      "Seen so far: 32896 samples\n",
      "Training loss (for one batch) at step 514: 1.1204\n",
      "Seen so far: 32960 samples\n",
      "Training loss (for one batch) at step 515: 1.6289\n",
      "Seen so far: 33024 samples\n",
      "Training loss (for one batch) at step 516: 2.2990\n",
      "Seen so far: 33088 samples\n",
      "Training loss (for one batch) at step 517: 1.6626\n",
      "Seen so far: 33152 samples\n",
      "Training loss (for one batch) at step 518: 1.3092\n",
      "Seen so far: 33216 samples\n",
      "Training loss (for one batch) at step 519: 1.1516\n",
      "Seen so far: 33280 samples\n",
      "Training loss (for one batch) at step 520: 1.4808\n",
      "Seen so far: 33344 samples\n",
      "Training loss (for one batch) at step 521: 1.3157\n",
      "Seen so far: 33408 samples\n",
      "Training loss (for one batch) at step 522: 1.3986\n",
      "Seen so far: 33472 samples\n",
      "Training loss (for one batch) at step 523: 1.1674\n",
      "Seen so far: 33536 samples\n",
      "Training loss (for one batch) at step 524: 1.2456\n",
      "Seen so far: 33600 samples\n",
      "Training loss (for one batch) at step 525: 1.4424\n",
      "Seen so far: 33664 samples\n",
      "Training loss (for one batch) at step 526: 1.6235\n",
      "Seen so far: 33728 samples\n",
      "Training loss (for one batch) at step 527: 1.4953\n",
      "Seen so far: 33792 samples\n",
      "Training loss (for one batch) at step 528: 1.3541\n",
      "Seen so far: 33856 samples\n",
      "Training loss (for one batch) at step 529: 1.5739\n",
      "Seen so far: 33920 samples\n",
      "Training loss (for one batch) at step 530: 1.3289\n",
      "Seen so far: 33984 samples\n",
      "Training loss (for one batch) at step 531: 1.7051\n",
      "Seen so far: 34048 samples\n",
      "Training loss (for one batch) at step 532: 1.4854\n",
      "Seen so far: 34112 samples\n",
      "Training loss (for one batch) at step 533: 1.2879\n",
      "Seen so far: 34176 samples\n",
      "Training loss (for one batch) at step 534: 1.6882\n",
      "Seen so far: 34240 samples\n",
      "Training loss (for one batch) at step 535: 1.2774\n",
      "Seen so far: 34304 samples\n",
      "Training loss (for one batch) at step 536: 1.4541\n",
      "Seen so far: 34368 samples\n",
      "Training loss (for one batch) at step 537: 0.9797\n",
      "Seen so far: 34432 samples\n",
      "Training loss (for one batch) at step 538: 1.8684\n",
      "Seen so far: 34496 samples\n",
      "Training loss (for one batch) at step 539: 1.1609\n",
      "Seen so far: 34560 samples\n",
      "Training loss (for one batch) at step 540: 1.3873\n",
      "Seen so far: 34624 samples\n",
      "Training loss (for one batch) at step 541: 1.3049\n",
      "Seen so far: 34688 samples\n",
      "Training loss (for one batch) at step 542: 1.3358\n",
      "Seen so far: 34752 samples\n",
      "Training loss (for one batch) at step 543: 1.2144\n",
      "Seen so far: 34816 samples\n",
      "Training loss (for one batch) at step 544: 1.3654\n",
      "Seen so far: 34880 samples\n",
      "Training loss (for one batch) at step 545: 0.9029\n",
      "Seen so far: 34944 samples\n",
      "Training loss (for one batch) at step 546: 1.5845\n",
      "Seen so far: 35008 samples\n",
      "Training loss (for one batch) at step 547: 1.6118\n",
      "Seen so far: 35072 samples\n",
      "Training loss (for one batch) at step 548: 1.2796\n",
      "Seen so far: 35136 samples\n",
      "Training loss (for one batch) at step 549: 1.5117\n",
      "Seen so far: 35200 samples\n",
      "Training loss (for one batch) at step 550: 1.3173\n",
      "Seen so far: 35264 samples\n",
      "Training loss (for one batch) at step 551: 1.1228\n",
      "Seen so far: 35328 samples\n",
      "Training loss (for one batch) at step 552: 1.7420\n",
      "Seen so far: 35392 samples\n",
      "Training loss (for one batch) at step 553: 1.0210\n",
      "Seen so far: 35456 samples\n",
      "Training loss (for one batch) at step 554: 1.7693\n",
      "Seen so far: 35520 samples\n",
      "Training loss (for one batch) at step 555: 0.9902\n",
      "Seen so far: 35584 samples\n",
      "Training loss (for one batch) at step 556: 1.1198\n",
      "Seen so far: 35648 samples\n",
      "Training loss (for one batch) at step 557: 1.0860\n",
      "Seen so far: 35712 samples\n",
      "Training loss (for one batch) at step 558: 1.0471\n",
      "Seen so far: 35776 samples\n",
      "Training loss (for one batch) at step 559: 1.6060\n",
      "Seen so far: 35840 samples\n",
      "Training loss (for one batch) at step 560: 1.5373\n",
      "Seen so far: 35904 samples\n",
      "Training loss (for one batch) at step 561: 1.4415\n",
      "Seen so far: 35968 samples\n",
      "Training loss (for one batch) at step 562: 1.6626\n",
      "Seen so far: 36032 samples\n",
      "Training loss (for one batch) at step 563: 1.6689\n",
      "Seen so far: 36096 samples\n",
      "Training loss (for one batch) at step 564: 1.1910\n",
      "Seen so far: 36160 samples\n",
      "Training loss (for one batch) at step 565: 1.1578\n",
      "Seen so far: 36224 samples\n",
      "Training loss (for one batch) at step 566: 1.3201\n",
      "Seen so far: 36288 samples\n",
      "Training loss (for one batch) at step 567: 1.2443\n",
      "Seen so far: 36352 samples\n",
      "Training loss (for one batch) at step 568: 1.3828\n",
      "Seen so far: 36416 samples\n",
      "Training loss (for one batch) at step 569: 1.0763\n",
      "Seen so far: 36480 samples\n",
      "Training loss (for one batch) at step 570: 1.8381\n",
      "Seen so far: 36544 samples\n",
      "Training loss (for one batch) at step 571: 1.0960\n",
      "Seen so far: 36608 samples\n",
      "Training loss (for one batch) at step 572: 1.2794\n",
      "Seen so far: 36672 samples\n",
      "Training loss (for one batch) at step 573: 0.9264\n",
      "Seen so far: 36736 samples\n",
      "Training loss (for one batch) at step 574: 1.4324\n",
      "Seen so far: 36800 samples\n",
      "Training loss (for one batch) at step 575: 0.9769\n",
      "Seen so far: 36864 samples\n",
      "Training loss (for one batch) at step 576: 0.7603\n",
      "Seen so far: 36928 samples\n",
      "Training loss (for one batch) at step 577: 1.3473\n",
      "Seen so far: 36992 samples\n",
      "Training loss (for one batch) at step 578: 1.5394\n",
      "Seen so far: 37056 samples\n",
      "Training loss (for one batch) at step 579: 1.2365\n",
      "Seen so far: 37120 samples\n",
      "Training loss (for one batch) at step 580: 1.5002\n",
      "Seen so far: 37184 samples\n",
      "Training loss (for one batch) at step 581: 1.0874\n",
      "Seen so far: 37248 samples\n",
      "Training loss (for one batch) at step 582: 1.2159\n",
      "Seen so far: 37312 samples\n",
      "Training loss (for one batch) at step 583: 1.0296\n",
      "Seen so far: 37376 samples\n",
      "Training loss (for one batch) at step 584: 1.6418\n",
      "Seen so far: 37440 samples\n",
      "Training loss (for one batch) at step 585: 1.1117\n",
      "Seen so far: 37504 samples\n",
      "Training loss (for one batch) at step 586: 1.6886\n",
      "Seen so far: 37568 samples\n",
      "Training loss (for one batch) at step 587: 1.1932\n",
      "Seen so far: 37632 samples\n",
      "Training loss (for one batch) at step 588: 1.1470\n",
      "Seen so far: 37696 samples\n",
      "Training loss (for one batch) at step 589: 1.1949\n",
      "Seen so far: 37760 samples\n",
      "Training loss (for one batch) at step 590: 1.2356\n",
      "Seen so far: 37824 samples\n",
      "Training loss (for one batch) at step 591: 1.2385\n",
      "Seen so far: 37888 samples\n",
      "Training loss (for one batch) at step 592: 1.1537\n",
      "Seen so far: 37952 samples\n",
      "Training loss (for one batch) at step 593: 0.9788\n",
      "Seen so far: 38016 samples\n",
      "Training loss (for one batch) at step 594: 1.0227\n",
      "Seen so far: 38080 samples\n",
      "Training loss (for one batch) at step 595: 2.4571\n",
      "Seen so far: 38144 samples\n",
      "Training loss (for one batch) at step 596: 1.1826\n",
      "Seen so far: 38208 samples\n",
      "Training loss (for one batch) at step 597: 1.7449\n",
      "Seen so far: 38272 samples\n",
      "Training loss (for one batch) at step 598: 1.1806\n",
      "Seen so far: 38336 samples\n",
      "Training loss (for one batch) at step 599: 1.2469\n",
      "Seen so far: 38400 samples\n",
      "Training loss (for one batch) at step 601: 1.3065\n",
      "Seen so far: 38528 samples\n",
      "Training loss (for one batch) at step 602: 1.6448\n",
      "Seen so far: 38592 samples\n",
      "Training loss (for one batch) at step 603: 1.5954\n",
      "Seen so far: 38656 samples\n",
      "Training loss (for one batch) at step 604: 0.8371\n",
      "Seen so far: 38720 samples\n",
      "Training loss (for one batch) at step 605: 1.2337\n",
      "Seen so far: 38784 samples\n",
      "Training loss (for one batch) at step 606: 1.6376\n",
      "Seen so far: 38848 samples\n",
      "Training loss (for one batch) at step 607: 1.3449\n",
      "Seen so far: 38912 samples\n",
      "Training loss (for one batch) at step 608: 1.2210\n",
      "Seen so far: 38976 samples\n",
      "Training loss (for one batch) at step 609: 1.0291\n",
      "Seen so far: 39040 samples\n",
      "Training loss (for one batch) at step 610: 1.2555\n",
      "Seen so far: 39104 samples\n",
      "Training loss (for one batch) at step 611: 1.3092\n",
      "Seen so far: 39168 samples\n",
      "Training loss (for one batch) at step 612: 1.3777\n",
      "Seen so far: 39232 samples\n",
      "Training loss (for one batch) at step 613: 0.8990\n",
      "Seen so far: 39296 samples\n",
      "Training loss (for one batch) at step 614: 1.2864\n",
      "Seen so far: 39360 samples\n",
      "Training loss (for one batch) at step 615: 1.0331\n",
      "Seen so far: 39424 samples\n",
      "Training loss (for one batch) at step 616: 1.9227\n",
      "Seen so far: 39488 samples\n",
      "Training loss (for one batch) at step 617: 0.9991\n",
      "Seen so far: 39552 samples\n",
      "Training loss (for one batch) at step 618: 1.6628\n",
      "Seen so far: 39616 samples\n",
      "Training loss (for one batch) at step 619: 0.9018\n",
      "Seen so far: 39680 samples\n",
      "Training loss (for one batch) at step 620: 1.4054\n",
      "Seen so far: 39744 samples\n",
      "Training loss (for one batch) at step 621: 1.0808\n",
      "Seen so far: 39808 samples\n",
      "Training loss (for one batch) at step 622: 1.2408\n",
      "Seen so far: 39872 samples\n",
      "Training loss (for one batch) at step 623: 1.2376\n",
      "Seen so far: 39936 samples\n",
      "Training loss (for one batch) at step 624: 1.0831\n",
      "Seen so far: 40000 samples\n",
      "Training loss (for one batch) at step 625: 1.3730\n",
      "Seen so far: 40064 samples\n",
      "Training loss (for one batch) at step 626: 1.2438\n",
      "Seen so far: 40128 samples\n",
      "Training loss (for one batch) at step 627: 1.1188\n",
      "Seen so far: 40192 samples\n",
      "Training loss (for one batch) at step 628: 1.3235\n",
      "Seen so far: 40256 samples\n",
      "Training loss (for one batch) at step 629: 1.4795\n",
      "Seen so far: 40320 samples\n",
      "Training loss (for one batch) at step 630: 1.2774\n",
      "Seen so far: 40384 samples\n",
      "Training loss (for one batch) at step 631: 1.3325\n",
      "Seen so far: 40448 samples\n",
      "Training loss (for one batch) at step 632: 1.1980\n",
      "Seen so far: 40512 samples\n",
      "Training loss (for one batch) at step 633: 1.4526\n",
      "Seen so far: 40576 samples\n",
      "Training loss (for one batch) at step 634: 1.1482\n",
      "Seen so far: 40640 samples\n",
      "Training loss (for one batch) at step 635: 0.9633\n",
      "Seen so far: 40704 samples\n",
      "Training loss (for one batch) at step 636: 0.9297\n",
      "Seen so far: 40768 samples\n",
      "Training loss (for one batch) at step 637: 1.1762\n",
      "Seen so far: 40832 samples\n",
      "Training loss (for one batch) at step 638: 1.2239\n",
      "Seen so far: 40896 samples\n",
      "Training loss (for one batch) at step 639: 1.4158\n",
      "Seen so far: 40960 samples\n",
      "Training loss (for one batch) at step 640: 1.3638\n",
      "Seen so far: 41024 samples\n",
      "Training loss (for one batch) at step 641: 0.9428\n",
      "Seen so far: 41088 samples\n",
      "Training loss (for one batch) at step 642: 0.9716\n",
      "Seen so far: 41152 samples\n",
      "Training loss (for one batch) at step 643: 1.6361\n",
      "Seen so far: 41216 samples\n",
      "Training loss (for one batch) at step 644: 1.3681\n",
      "Seen so far: 41280 samples\n",
      "Training loss (for one batch) at step 645: 1.1651\n",
      "Seen so far: 41344 samples\n",
      "Training loss (for one batch) at step 646: 0.9007\n",
      "Seen so far: 41408 samples\n",
      "Training loss (for one batch) at step 647: 1.0897\n",
      "Seen so far: 41472 samples\n",
      "Training loss (for one batch) at step 648: 1.6114\n",
      "Seen so far: 41536 samples\n",
      "Training loss (for one batch) at step 649: 0.9511\n",
      "Seen so far: 41600 samples\n",
      "Training loss (for one batch) at step 650: 1.0149\n",
      "Seen so far: 41664 samples\n",
      "Training loss (for one batch) at step 651: 1.0687\n",
      "Seen so far: 41728 samples\n",
      "Training loss (for one batch) at step 652: 1.7764\n",
      "Seen so far: 41792 samples\n",
      "Training loss (for one batch) at step 653: 1.0810\n",
      "Seen so far: 41856 samples\n",
      "Training loss (for one batch) at step 654: 1.2239\n",
      "Seen so far: 41920 samples\n",
      "Training loss (for one batch) at step 655: 1.4177\n",
      "Seen so far: 41984 samples\n",
      "Training loss (for one batch) at step 656: 1.5100\n",
      "Seen so far: 42048 samples\n",
      "Training loss (for one batch) at step 657: 1.0885\n",
      "Seen so far: 42112 samples\n",
      "Training loss (for one batch) at step 658: 1.0911\n",
      "Seen so far: 42176 samples\n",
      "Training loss (for one batch) at step 659: 1.2739\n",
      "Seen so far: 42240 samples\n",
      "Training loss (for one batch) at step 660: 1.4542\n",
      "Seen so far: 42304 samples\n",
      "Training loss (for one batch) at step 661: 1.1798\n",
      "Seen so far: 42368 samples\n",
      "Training loss (for one batch) at step 662: 1.2500\n",
      "Seen so far: 42432 samples\n",
      "Training loss (for one batch) at step 663: 1.4689\n",
      "Seen so far: 42496 samples\n",
      "Training loss (for one batch) at step 664: 1.2134\n",
      "Seen so far: 42560 samples\n",
      "Training loss (for one batch) at step 665: 3.0307\n",
      "Seen so far: 42624 samples\n",
      "Training loss (for one batch) at step 666: 1.1394\n",
      "Seen so far: 42688 samples\n",
      "Training loss (for one batch) at step 667: 1.1073\n",
      "Seen so far: 42752 samples\n",
      "Training loss (for one batch) at step 668: 1.1712\n",
      "Seen so far: 42816 samples\n",
      "Training loss (for one batch) at step 669: 1.5562\n",
      "Seen so far: 42880 samples\n",
      "Training loss (for one batch) at step 670: 1.1789\n",
      "Seen so far: 42944 samples\n",
      "Training loss (for one batch) at step 671: 1.6978\n",
      "Seen so far: 43008 samples\n",
      "Training loss (for one batch) at step 672: 1.2076\n",
      "Seen so far: 43072 samples\n",
      "Training loss (for one batch) at step 673: 1.1301\n",
      "Seen so far: 43136 samples\n",
      "Training loss (for one batch) at step 674: 1.2201\n",
      "Seen so far: 43200 samples\n",
      "Training loss (for one batch) at step 675: 1.2899\n",
      "Seen so far: 43264 samples\n",
      "Training loss (for one batch) at step 676: 1.9640\n",
      "Seen so far: 43328 samples\n",
      "Training loss (for one batch) at step 677: 1.1132\n",
      "Seen so far: 43392 samples\n",
      "Training loss (for one batch) at step 678: 1.1934\n",
      "Seen so far: 43456 samples\n",
      "Training loss (for one batch) at step 679: 1.0709\n",
      "Seen so far: 43520 samples\n",
      "Training loss (for one batch) at step 680: 1.3718\n",
      "Seen so far: 43584 samples\n",
      "Training loss (for one batch) at step 681: 0.9373\n",
      "Seen so far: 43648 samples\n",
      "Training loss (for one batch) at step 682: 1.0228\n",
      "Seen so far: 43712 samples\n",
      "Training loss (for one batch) at step 683: 1.1374\n",
      "Seen so far: 43776 samples\n",
      "Training loss (for one batch) at step 684: 1.6068\n",
      "Seen so far: 43840 samples\n",
      "Training loss (for one batch) at step 685: 1.1484\n",
      "Seen so far: 43904 samples\n",
      "Training loss (for one batch) at step 686: 1.1256\n",
      "Seen so far: 43968 samples\n",
      "Training loss (for one batch) at step 687: 0.8811\n",
      "Seen so far: 44032 samples\n",
      "Training loss (for one batch) at step 688: 1.0922\n",
      "Seen so far: 44096 samples\n",
      "Training loss (for one batch) at step 689: 2.3055\n",
      "Seen so far: 44160 samples\n",
      "Training loss (for one batch) at step 690: 1.8857\n",
      "Seen so far: 44224 samples\n",
      "Training loss (for one batch) at step 691: 1.4763\n",
      "Seen so far: 44288 samples\n",
      "Training loss (for one batch) at step 692: 1.0283\n",
      "Seen so far: 44352 samples\n",
      "Training loss (for one batch) at step 693: 1.2629\n",
      "Seen so far: 44416 samples\n",
      "Training loss (for one batch) at step 694: 1.2911\n",
      "Seen so far: 44480 samples\n",
      "Training loss (for one batch) at step 695: 1.0091\n",
      "Seen so far: 44544 samples\n",
      "Training loss (for one batch) at step 696: 0.9862\n",
      "Seen so far: 44608 samples\n",
      "Training loss (for one batch) at step 697: 1.4512\n",
      "Seen so far: 44672 samples\n",
      "Training loss (for one batch) at step 698: 1.6750\n",
      "Seen so far: 44736 samples\n",
      "Training loss (for one batch) at step 699: 1.7083\n",
      "Seen so far: 44800 samples\n",
      "Training loss (for one batch) at step 700: 0.8692\n",
      "Seen so far: 44864 samples\n",
      "Training loss (for one batch) at step 701: 1.0670\n",
      "Seen so far: 44928 samples\n",
      "Training loss (for one batch) at step 702: 1.4933\n",
      "Seen so far: 44992 samples\n",
      "Training loss (for one batch) at step 703: 1.1455\n",
      "Seen so far: 45056 samples\n",
      "Training loss (for one batch) at step 704: 1.2951\n",
      "Seen so far: 45120 samples\n",
      "Training loss (for one batch) at step 705: 1.3829\n",
      "Seen so far: 45184 samples\n",
      "Training loss (for one batch) at step 706: 1.2358\n",
      "Seen so far: 45248 samples\n",
      "Training loss (for one batch) at step 707: 1.3366\n",
      "Seen so far: 45312 samples\n",
      "Training loss (for one batch) at step 708: 0.7327\n",
      "Seen so far: 45376 samples\n",
      "Training loss (for one batch) at step 709: 1.0489\n",
      "Seen so far: 45440 samples\n",
      "Training loss (for one batch) at step 710: 1.2614\n",
      "Seen so far: 45504 samples\n",
      "Training loss (for one batch) at step 711: 1.2510\n",
      "Seen so far: 45568 samples\n",
      "Training loss (for one batch) at step 712: 1.4216\n",
      "Seen so far: 45632 samples\n",
      "Training loss (for one batch) at step 713: 1.2143\n",
      "Seen so far: 45696 samples\n",
      "Training loss (for one batch) at step 714: 1.2918\n",
      "Seen so far: 45760 samples\n",
      "Training loss (for one batch) at step 715: 1.2074\n",
      "Seen so far: 45824 samples\n",
      "Training loss (for one batch) at step 716: 0.9984\n",
      "Seen so far: 45888 samples\n",
      "Training loss (for one batch) at step 717: 0.9686\n",
      "Seen so far: 45952 samples\n",
      "Training loss (for one batch) at step 718: 0.7775\n",
      "Seen so far: 46016 samples\n",
      "Training loss (for one batch) at step 719: 1.1895\n",
      "Seen so far: 46080 samples\n",
      "Training loss (for one batch) at step 720: 0.9498\n",
      "Seen so far: 46144 samples\n",
      "Training loss (for one batch) at step 721: 1.2803\n",
      "Seen so far: 46208 samples\n",
      "Training loss (for one batch) at step 722: 1.0954\n",
      "Seen so far: 46272 samples\n",
      "Training loss (for one batch) at step 723: 1.0042\n",
      "Seen so far: 46336 samples\n",
      "Training loss (for one batch) at step 724: 1.1840\n",
      "Seen so far: 46400 samples\n",
      "Training loss (for one batch) at step 725: 0.9891\n",
      "Seen so far: 46464 samples\n",
      "Training loss (for one batch) at step 726: 1.2774\n",
      "Seen so far: 46528 samples\n",
      "Training loss (for one batch) at step 727: 0.9444\n",
      "Seen so far: 46592 samples\n",
      "Training loss (for one batch) at step 728: 1.0134\n",
      "Seen so far: 46656 samples\n",
      "Training loss (for one batch) at step 729: 1.0960\n",
      "Seen so far: 46720 samples\n",
      "Training loss (for one batch) at step 730: 0.9301\n",
      "Seen so far: 46784 samples\n",
      "Training loss (for one batch) at step 731: 0.9900\n",
      "Seen so far: 46848 samples\n",
      "Training loss (for one batch) at step 732: 1.8941\n",
      "Seen so far: 46912 samples\n",
      "Training loss (for one batch) at step 733: 1.1455\n",
      "Seen so far: 46976 samples\n",
      "Training loss (for one batch) at step 734: 1.1936\n",
      "Seen so far: 47040 samples\n",
      "Training loss (for one batch) at step 735: 1.8728\n",
      "Seen so far: 47104 samples\n",
      "Training loss (for one batch) at step 736: 1.3677\n",
      "Seen so far: 47168 samples\n",
      "Training loss (for one batch) at step 737: 1.2269\n",
      "Seen so far: 47232 samples\n",
      "Training loss (for one batch) at step 738: 1.1421\n",
      "Seen so far: 47296 samples\n",
      "Training loss (for one batch) at step 739: 1.7316\n",
      "Seen so far: 47360 samples\n",
      "Training loss (for one batch) at step 740: 1.1682\n",
      "Seen so far: 47424 samples\n",
      "Training loss (for one batch) at step 741: 1.0316\n",
      "Seen so far: 47488 samples\n",
      "Training loss (for one batch) at step 742: 1.3585\n",
      "Seen so far: 47552 samples\n",
      "Training loss (for one batch) at step 743: 0.9726\n",
      "Seen so far: 47616 samples\n",
      "Training loss (for one batch) at step 744: 1.1643\n",
      "Seen so far: 47680 samples\n",
      "Training loss (for one batch) at step 745: 0.8729\n",
      "Seen so far: 47744 samples\n",
      "Training loss (for one batch) at step 746: 1.0503\n",
      "Seen so far: 47808 samples\n",
      "Training loss (for one batch) at step 747: 1.2613\n",
      "Seen so far: 47872 samples\n",
      "Training loss (for one batch) at step 748: 1.5375\n",
      "Seen so far: 47936 samples\n",
      "Training loss (for one batch) at step 749: 0.9215\n",
      "Seen so far: 48000 samples\n",
      "Training loss (for one batch) at step 750: 1.2126\n",
      "Seen so far: 48064 samples\n",
      "Training loss (for one batch) at step 751: 0.9724\n",
      "Seen so far: 48128 samples\n",
      "Training loss (for one batch) at step 752: 0.8274\n",
      "Seen so far: 48192 samples\n",
      "Training loss (for one batch) at step 753: 1.2407\n",
      "Seen so far: 48256 samples\n",
      "Training loss (for one batch) at step 754: 0.6803\n",
      "Seen so far: 48320 samples\n",
      "Training loss (for one batch) at step 755: 1.3231\n",
      "Seen so far: 48384 samples\n",
      "Training loss (for one batch) at step 756: 1.4080\n",
      "Seen so far: 48448 samples\n",
      "Training loss (for one batch) at step 757: 1.2611\n",
      "Seen so far: 48512 samples\n",
      "Training loss (for one batch) at step 758: 1.1764\n",
      "Seen so far: 48576 samples\n",
      "Training loss (for one batch) at step 759: 1.1338\n",
      "Seen so far: 48640 samples\n",
      "Training loss (for one batch) at step 760: 1.1847\n",
      "Seen so far: 48704 samples\n",
      "Training loss (for one batch) at step 761: 1.5167\n",
      "Seen so far: 48768 samples\n",
      "Training loss (for one batch) at step 762: 0.8574\n",
      "Seen so far: 48832 samples\n",
      "Training loss (for one batch) at step 763: 1.1746\n",
      "Seen so far: 48896 samples\n",
      "Training loss (for one batch) at step 764: 1.2405\n",
      "Seen so far: 48960 samples\n",
      "Training loss (for one batch) at step 765: 0.9680\n",
      "Seen so far: 49024 samples\n",
      "Training loss (for one batch) at step 766: 0.8061\n",
      "Seen so far: 49088 samples\n",
      "Training loss (for one batch) at step 767: 1.0526\n",
      "Seen so far: 49152 samples\n",
      "Training loss (for one batch) at step 768: 1.4178\n",
      "Seen so far: 49216 samples\n",
      "Training loss (for one batch) at step 769: 1.5693\n",
      "Seen so far: 49280 samples\n",
      "Training loss (for one batch) at step 770: 1.0208\n",
      "Seen so far: 49344 samples\n",
      "Training loss (for one batch) at step 771: 0.8840\n",
      "Seen so far: 49408 samples\n",
      "Training loss (for one batch) at step 772: 1.1409\n",
      "Seen so far: 49472 samples\n",
      "Training loss (for one batch) at step 773: 0.9570\n",
      "Seen so far: 49536 samples\n",
      "Training loss (for one batch) at step 774: 1.2265\n",
      "Seen so far: 49600 samples\n",
      "Training loss (for one batch) at step 775: 0.9542\n",
      "Seen so far: 49664 samples\n",
      "Training loss (for one batch) at step 776: 1.5145\n",
      "Seen so far: 49728 samples\n",
      "Training loss (for one batch) at step 777: 1.3486\n",
      "Seen so far: 49792 samples\n",
      "Training loss (for one batch) at step 778: 1.1806\n",
      "Seen so far: 49856 samples\n",
      "Training loss (for one batch) at step 779: 1.4134\n",
      "Seen so far: 49920 samples\n",
      "Training loss (for one batch) at step 780: 1.1892\n",
      "Seen so far: 49984 samples\n",
      "Training loss (for one batch) at step 781: 2.1570\n",
      "Seen so far: 50048 samples\n",
      "Start epoch 1\n",
      "Training loss (for one batch) at step 1: 1.8260\n",
      "Seen so far: 128 samples\n",
      "Training loss (for one batch) at step 2: 1.2301\n",
      "Seen so far: 192 samples\n",
      "Training loss (for one batch) at step 3: 1.1423\n",
      "Seen so far: 256 samples\n",
      "Training loss (for one batch) at step 4: 1.3423\n",
      "Seen so far: 320 samples\n",
      "Training loss (for one batch) at step 5: 1.1201\n",
      "Seen so far: 384 samples\n",
      "Training loss (for one batch) at step 6: 0.8966\n",
      "Seen so far: 448 samples\n",
      "Training loss (for one batch) at step 7: 1.2526\n",
      "Seen so far: 512 samples\n",
      "Training loss (for one batch) at step 8: 1.1364\n",
      "Seen so far: 576 samples\n",
      "Training loss (for one batch) at step 9: 1.4490\n",
      "Seen so far: 640 samples\n",
      "Training loss (for one batch) at step 10: 1.2206\n",
      "Seen so far: 704 samples\n",
      "Training loss (for one batch) at step 11: 0.9207\n",
      "Seen so far: 768 samples\n",
      "Training loss (for one batch) at step 12: 1.4755\n",
      "Seen so far: 832 samples\n",
      "Training loss (for one batch) at step 13: 1.5479\n",
      "Seen so far: 896 samples\n",
      "Training loss (for one batch) at step 14: 1.7679\n",
      "Seen so far: 960 samples\n",
      "Training loss (for one batch) at step 15: 0.9393\n",
      "Seen so far: 1024 samples\n",
      "Training loss (for one batch) at step 16: 1.2018\n",
      "Seen so far: 1088 samples\n",
      "Training loss (for one batch) at step 17: 1.0630\n",
      "Seen so far: 1152 samples\n",
      "Training loss (for one batch) at step 18: 1.9914\n",
      "Seen so far: 1216 samples\n",
      "Training loss (for one batch) at step 19: 0.9216\n",
      "Seen so far: 1280 samples\n",
      "Training loss (for one batch) at step 20: 1.0787\n",
      "Seen so far: 1344 samples\n",
      "Training loss (for one batch) at step 21: 1.7508\n",
      "Seen so far: 1408 samples\n",
      "Training loss (for one batch) at step 22: 1.3530\n",
      "Seen so far: 1472 samples\n",
      "Training loss (for one batch) at step 23: 1.1360\n",
      "Seen so far: 1536 samples\n",
      "Training loss (for one batch) at step 24: 1.2159\n",
      "Seen so far: 1600 samples\n",
      "Training loss (for one batch) at step 25: 1.5933\n",
      "Seen so far: 1664 samples\n",
      "Training loss (for one batch) at step 26: 0.7781\n",
      "Seen so far: 1728 samples\n",
      "Training loss (for one batch) at step 27: 1.2109\n",
      "Seen so far: 1792 samples\n",
      "Training loss (for one batch) at step 28: 1.5893\n",
      "Seen so far: 1856 samples\n",
      "Training loss (for one batch) at step 29: 1.0199\n",
      "Seen so far: 1920 samples\n",
      "Training loss (for one batch) at step 30: 1.0457\n",
      "Seen so far: 1984 samples\n",
      "Training loss (for one batch) at step 31: 0.9422\n",
      "Seen so far: 2048 samples\n",
      "Training loss (for one batch) at step 32: 1.2819\n",
      "Seen so far: 2112 samples\n",
      "Training loss (for one batch) at step 33: 0.9158\n",
      "Seen so far: 2176 samples\n",
      "Training loss (for one batch) at step 34: 1.3687\n",
      "Seen so far: 2240 samples\n",
      "Training loss (for one batch) at step 35: 1.3958\n",
      "Seen so far: 2304 samples\n",
      "Training loss (for one batch) at step 36: 1.3569\n",
      "Seen so far: 2368 samples\n",
      "Training loss (for one batch) at step 37: 0.9797\n",
      "Seen so far: 2432 samples\n",
      "Training loss (for one batch) at step 38: 1.2911\n",
      "Seen so far: 2496 samples\n",
      "Training loss (for one batch) at step 39: 1.1765\n",
      "Seen so far: 2560 samples\n",
      "Training loss (for one batch) at step 40: 1.4195\n",
      "Seen so far: 2624 samples\n",
      "Training loss (for one batch) at step 41: 1.2680\n",
      "Seen so far: 2688 samples\n",
      "Training loss (for one batch) at step 42: 1.2789\n",
      "Seen so far: 2752 samples\n",
      "Training loss (for one batch) at step 43: 1.2155\n",
      "Seen so far: 2816 samples\n",
      "Training loss (for one batch) at step 44: 1.5018\n",
      "Seen so far: 2880 samples\n",
      "Training loss (for one batch) at step 45: 1.1596\n",
      "Seen so far: 2944 samples\n",
      "Training loss (for one batch) at step 46: 1.0921\n",
      "Seen so far: 3008 samples\n",
      "Training loss (for one batch) at step 47: 1.9578\n",
      "Seen so far: 3072 samples\n",
      "Training loss (for one batch) at step 48: 1.6672\n",
      "Seen so far: 3136 samples\n",
      "Training loss (for one batch) at step 49: 1.1789\n",
      "Seen so far: 3200 samples\n",
      "Training loss (for one batch) at step 50: 1.2126\n",
      "Seen so far: 3264 samples\n",
      "Training loss (for one batch) at step 51: 0.8747\n",
      "Seen so far: 3328 samples\n",
      "Training loss (for one batch) at step 52: 1.0635\n",
      "Seen so far: 3392 samples\n",
      "Training loss (for one batch) at step 53: 1.2795\n",
      "Seen so far: 3456 samples\n",
      "Training loss (for one batch) at step 54: 1.0145\n",
      "Seen so far: 3520 samples\n",
      "Training loss (for one batch) at step 55: 1.1171\n",
      "Seen so far: 3584 samples\n",
      "Training loss (for one batch) at step 56: 1.2806\n",
      "Seen so far: 3648 samples\n",
      "Training loss (for one batch) at step 57: 1.2309\n",
      "Seen so far: 3712 samples\n",
      "Training loss (for one batch) at step 58: 1.6285\n",
      "Seen so far: 3776 samples\n",
      "Training loss (for one batch) at step 59: 1.6460\n",
      "Seen so far: 3840 samples\n",
      "Training loss (for one batch) at step 60: 1.3630\n",
      "Seen so far: 3904 samples\n",
      "Training loss (for one batch) at step 61: 1.2756\n",
      "Seen so far: 3968 samples\n",
      "Training loss (for one batch) at step 62: 1.1388\n",
      "Seen so far: 4032 samples\n",
      "Training loss (for one batch) at step 63: 1.1495\n",
      "Seen so far: 4096 samples\n",
      "Training loss (for one batch) at step 64: 1.5348\n",
      "Seen so far: 4160 samples\n",
      "Training loss (for one batch) at step 65: 1.2250\n",
      "Seen so far: 4224 samples\n",
      "Training loss (for one batch) at step 66: 1.2736\n",
      "Seen so far: 4288 samples\n",
      "Training loss (for one batch) at step 67: 2.0192\n",
      "Seen so far: 4352 samples\n",
      "Training loss (for one batch) at step 68: 1.1611\n",
      "Seen so far: 4416 samples\n",
      "Training loss (for one batch) at step 69: 1.1576\n",
      "Seen so far: 4480 samples\n",
      "Training loss (for one batch) at step 70: 1.7502\n",
      "Seen so far: 4544 samples\n",
      "Training loss (for one batch) at step 71: 1.0406\n",
      "Seen so far: 4608 samples\n",
      "Training loss (for one batch) at step 72: 1.6566\n",
      "Seen so far: 4672 samples\n",
      "Training loss (for one batch) at step 73: 1.2046\n",
      "Seen so far: 4736 samples\n",
      "Training loss (for one batch) at step 74: 1.2627\n",
      "Seen so far: 4800 samples\n",
      "Training loss (for one batch) at step 75: 1.3566\n",
      "Seen so far: 4864 samples\n",
      "Training loss (for one batch) at step 76: 1.2055\n",
      "Seen so far: 4928 samples\n",
      "Training loss (for one batch) at step 77: 1.3313\n",
      "Seen so far: 4992 samples\n",
      "Training loss (for one batch) at step 78: 0.7390\n",
      "Seen so far: 5056 samples\n",
      "Training loss (for one batch) at step 79: 1.5014\n",
      "Seen so far: 5120 samples\n",
      "Training loss (for one batch) at step 80: 1.0911\n",
      "Seen so far: 5184 samples\n",
      "Training loss (for one batch) at step 81: 1.2143\n",
      "Seen so far: 5248 samples\n",
      "Training loss (for one batch) at step 82: 1.2483\n",
      "Seen so far: 5312 samples\n",
      "Training loss (for one batch) at step 83: 1.0957\n",
      "Seen so far: 5376 samples\n",
      "Training loss (for one batch) at step 84: 1.7209\n",
      "Seen so far: 5440 samples\n",
      "Training loss (for one batch) at step 85: 1.0115\n",
      "Seen so far: 5504 samples\n",
      "Training loss (for one batch) at step 86: 0.8757\n",
      "Seen so far: 5568 samples\n",
      "Training loss (for one batch) at step 87: 1.1857\n",
      "Seen so far: 5632 samples\n",
      "Training loss (for one batch) at step 88: 0.9974\n",
      "Seen so far: 5696 samples\n",
      "Training loss (for one batch) at step 89: 0.9836\n",
      "Seen so far: 5760 samples\n",
      "Training loss (for one batch) at step 90: 0.7737\n",
      "Seen so far: 5824 samples\n",
      "Training loss (for one batch) at step 91: 1.0004\n",
      "Seen so far: 5888 samples\n",
      "Training loss (for one batch) at step 92: 1.3606\n",
      "Seen so far: 5952 samples\n",
      "Training loss (for one batch) at step 93: 1.3930\n",
      "Seen so far: 6016 samples\n",
      "Training loss (for one batch) at step 94: 1.1589\n",
      "Seen so far: 6080 samples\n",
      "Training loss (for one batch) at step 95: 1.1513\n",
      "Seen so far: 6144 samples\n",
      "Training loss (for one batch) at step 96: 1.5304\n",
      "Seen so far: 6208 samples\n",
      "Training loss (for one batch) at step 97: 1.5175\n",
      "Seen so far: 6272 samples\n",
      "Training loss (for one batch) at step 98: 0.9589\n",
      "Seen so far: 6336 samples\n",
      "Training loss (for one batch) at step 99: 0.8617\n",
      "Seen so far: 6400 samples\n",
      "Training loss (for one batch) at step 100: 1.1398\n",
      "Seen so far: 6464 samples\n",
      "Training loss (for one batch) at step 101: 1.0980\n",
      "Seen so far: 6528 samples\n",
      "Training loss (for one batch) at step 102: 1.3031\n",
      "Seen so far: 6592 samples\n",
      "Training loss (for one batch) at step 103: 1.3536\n",
      "Seen so far: 6656 samples\n",
      "Training loss (for one batch) at step 104: 0.9710\n",
      "Seen so far: 6720 samples\n",
      "Training loss (for one batch) at step 105: 1.1556\n",
      "Seen so far: 6784 samples\n",
      "Training loss (for one batch) at step 106: 0.9594\n",
      "Seen so far: 6848 samples\n",
      "Training loss (for one batch) at step 107: 1.1770\n",
      "Seen so far: 6912 samples\n",
      "Training loss (for one batch) at step 108: 0.9814\n",
      "Seen so far: 6976 samples\n",
      "Training loss (for one batch) at step 109: 0.8955\n",
      "Seen so far: 7040 samples\n",
      "Training loss (for one batch) at step 110: 1.0467\n",
      "Seen so far: 7104 samples\n",
      "Training loss (for one batch) at step 111: 0.9545\n",
      "Seen so far: 7168 samples\n",
      "Training loss (for one batch) at step 112: 1.1844\n",
      "Seen so far: 7232 samples\n",
      "Training loss (for one batch) at step 113: 1.2256\n",
      "Seen so far: 7296 samples\n",
      "Training loss (for one batch) at step 114: 1.0395\n",
      "Seen so far: 7360 samples\n",
      "Training loss (for one batch) at step 115: 0.8372\n",
      "Seen so far: 7424 samples\n",
      "Training loss (for one batch) at step 116: 1.1266\n",
      "Seen so far: 7488 samples\n",
      "Training loss (for one batch) at step 117: 0.8861\n",
      "Seen so far: 7552 samples\n",
      "Training loss (for one batch) at step 118: 0.8971\n",
      "Seen so far: 7616 samples\n",
      "Training loss (for one batch) at step 119: 1.0678\n",
      "Seen so far: 7680 samples\n",
      "Training loss (for one batch) at step 120: 1.1277\n",
      "Seen so far: 7744 samples\n",
      "Training loss (for one batch) at step 121: 0.8121\n",
      "Seen so far: 7808 samples\n",
      "Training loss (for one batch) at step 122: 1.0492\n",
      "Seen so far: 7872 samples\n",
      "Training loss (for one batch) at step 123: 0.9059\n",
      "Seen so far: 7936 samples\n",
      "Training loss (for one batch) at step 124: 1.0499\n",
      "Seen so far: 8000 samples\n",
      "Training loss (for one batch) at step 125: 1.2346\n",
      "Seen so far: 8064 samples\n",
      "Training loss (for one batch) at step 126: 1.1248\n",
      "Seen so far: 8128 samples\n",
      "Training loss (for one batch) at step 127: 0.9700\n",
      "Seen so far: 8192 samples\n",
      "Training loss (for one batch) at step 128: 1.1398\n",
      "Seen so far: 8256 samples\n",
      "Training loss (for one batch) at step 129: 1.2055\n",
      "Seen so far: 8320 samples\n",
      "Training loss (for one batch) at step 130: 1.3994\n",
      "Seen so far: 8384 samples\n",
      "Training loss (for one batch) at step 131: 0.9000\n",
      "Seen so far: 8448 samples\n",
      "Training loss (for one batch) at step 132: 0.8678\n",
      "Seen so far: 8512 samples\n",
      "Training loss (for one batch) at step 133: 2.0301\n",
      "Seen so far: 8576 samples\n",
      "Training loss (for one batch) at step 134: 1.0497\n",
      "Seen so far: 8640 samples\n",
      "Training loss (for one batch) at step 135: 1.0278\n",
      "Seen so far: 8704 samples\n",
      "Training loss (for one batch) at step 136: 0.8595\n",
      "Seen so far: 8768 samples\n",
      "Training loss (for one batch) at step 137: 0.9236\n",
      "Seen so far: 8832 samples\n",
      "Training loss (for one batch) at step 138: 1.2758\n",
      "Seen so far: 8896 samples\n",
      "Training loss (for one batch) at step 139: 1.4968\n",
      "Seen so far: 8960 samples\n",
      "Training loss (for one batch) at step 140: 0.9640\n",
      "Seen so far: 9024 samples\n",
      "Training loss (for one batch) at step 141: 0.7018\n",
      "Seen so far: 9088 samples\n",
      "Training loss (for one batch) at step 142: 1.1527\n",
      "Seen so far: 9152 samples\n",
      "Training loss (for one batch) at step 143: 1.0492\n",
      "Seen so far: 9216 samples\n",
      "Training loss (for one batch) at step 144: 1.1428\n",
      "Seen so far: 9280 samples\n",
      "Training loss (for one batch) at step 145: 0.9580\n",
      "Seen so far: 9344 samples\n",
      "Training loss (for one batch) at step 146: 1.2423\n",
      "Seen so far: 9408 samples\n",
      "Training loss (for one batch) at step 147: 1.8114\n",
      "Seen so far: 9472 samples\n",
      "Training loss (for one batch) at step 148: 1.1683\n",
      "Seen so far: 9536 samples\n",
      "Training loss (for one batch) at step 149: 1.0611\n",
      "Seen so far: 9600 samples\n",
      "Training loss (for one batch) at step 150: 0.7140\n",
      "Seen so far: 9664 samples\n",
      "Training loss (for one batch) at step 151: 1.1365\n",
      "Seen so far: 9728 samples\n",
      "Training loss (for one batch) at step 152: 1.1132\n",
      "Seen so far: 9792 samples\n",
      "Training loss (for one batch) at step 153: 0.9014\n",
      "Seen so far: 9856 samples\n",
      "Training loss (for one batch) at step 154: 0.9730\n",
      "Seen so far: 9920 samples\n",
      "Training loss (for one batch) at step 155: 1.2739\n",
      "Seen so far: 9984 samples\n",
      "Training loss (for one batch) at step 156: 0.8732\n",
      "Seen so far: 10048 samples\n",
      "Training loss (for one batch) at step 157: 1.4592\n",
      "Seen so far: 10112 samples\n",
      "Training loss (for one batch) at step 158: 1.0909\n",
      "Seen so far: 10176 samples\n",
      "Training loss (for one batch) at step 159: 1.0413\n",
      "Seen so far: 10240 samples\n",
      "Training loss (for one batch) at step 160: 1.5691\n",
      "Seen so far: 10304 samples\n",
      "Training loss (for one batch) at step 161: 0.9659\n",
      "Seen so far: 10368 samples\n",
      "Training loss (for one batch) at step 162: 1.0424\n",
      "Seen so far: 10432 samples\n",
      "Training loss (for one batch) at step 163: 0.8898\n",
      "Seen so far: 10496 samples\n",
      "Training loss (for one batch) at step 164: 1.0673\n",
      "Seen so far: 10560 samples\n",
      "Training loss (for one batch) at step 165: 0.8244\n",
      "Seen so far: 10624 samples\n",
      "Training loss (for one batch) at step 166: 0.8121\n",
      "Seen so far: 10688 samples\n",
      "Training loss (for one batch) at step 167: 1.0426\n",
      "Seen so far: 10752 samples\n",
      "Training loss (for one batch) at step 168: 0.7615\n",
      "Seen so far: 10816 samples\n",
      "Training loss (for one batch) at step 169: 1.1461\n",
      "Seen so far: 10880 samples\n",
      "Training loss (for one batch) at step 170: 0.8950\n",
      "Seen so far: 10944 samples\n",
      "Training loss (for one batch) at step 171: 1.4703\n",
      "Seen so far: 11008 samples\n",
      "Training loss (for one batch) at step 172: 0.8578\n",
      "Seen so far: 11072 samples\n",
      "Training loss (for one batch) at step 173: 1.5589\n",
      "Seen so far: 11136 samples\n",
      "Training loss (for one batch) at step 174: 1.3079\n",
      "Seen so far: 11200 samples\n",
      "Training loss (for one batch) at step 175: 0.8192\n",
      "Seen so far: 11264 samples\n",
      "Training loss (for one batch) at step 176: 1.1093\n",
      "Seen so far: 11328 samples\n",
      "Training loss (for one batch) at step 177: 1.0559\n",
      "Seen so far: 11392 samples\n",
      "Training loss (for one batch) at step 178: 0.9084\n",
      "Seen so far: 11456 samples\n",
      "Training loss (for one batch) at step 179: 0.8773\n",
      "Seen so far: 11520 samples\n",
      "Training loss (for one batch) at step 180: 1.1131\n",
      "Seen so far: 11584 samples\n",
      "Training loss (for one batch) at step 181: 0.9830\n",
      "Seen so far: 11648 samples\n",
      "Training loss (for one batch) at step 182: 1.6029\n",
      "Seen so far: 11712 samples\n",
      "Training loss (for one batch) at step 183: 1.0669\n",
      "Seen so far: 11776 samples\n",
      "Training loss (for one batch) at step 184: 0.8883\n",
      "Seen so far: 11840 samples\n",
      "Training loss (for one batch) at step 185: 1.1591\n",
      "Seen so far: 11904 samples\n",
      "Training loss (for one batch) at step 186: 0.9225\n",
      "Seen so far: 11968 samples\n",
      "Training loss (for one batch) at step 187: 0.7998\n",
      "Seen so far: 12032 samples\n",
      "Training loss (for one batch) at step 188: 0.7657\n",
      "Seen so far: 12096 samples\n",
      "Training loss (for one batch) at step 189: 1.1679\n",
      "Seen so far: 12160 samples\n",
      "Training loss (for one batch) at step 190: 1.6516\n",
      "Seen so far: 12224 samples\n",
      "Training loss (for one batch) at step 191: 0.9837\n",
      "Seen so far: 12288 samples\n",
      "Training loss (for one batch) at step 192: 1.0910\n",
      "Seen so far: 12352 samples\n",
      "Training loss (for one batch) at step 193: 0.9203\n",
      "Seen so far: 12416 samples\n",
      "Training loss (for one batch) at step 194: 1.1254\n",
      "Seen so far: 12480 samples\n",
      "Training loss (for one batch) at step 195: 1.3595\n",
      "Seen so far: 12544 samples\n",
      "Training loss (for one batch) at step 196: 1.6575\n",
      "Seen so far: 12608 samples\n",
      "Training loss (for one batch) at step 197: 1.0121\n",
      "Seen so far: 12672 samples\n",
      "Training loss (for one batch) at step 198: 0.8615\n",
      "Seen so far: 12736 samples\n",
      "Training loss (for one batch) at step 199: 2.1296\n",
      "Seen so far: 12800 samples\n",
      "Training loss (for one batch) at step 201: 1.0506\n",
      "Seen so far: 12928 samples\n",
      "Training loss (for one batch) at step 202: 1.1410\n",
      "Seen so far: 12992 samples\n",
      "Training loss (for one batch) at step 203: 0.9699\n",
      "Seen so far: 13056 samples\n",
      "Training loss (for one batch) at step 204: 1.3498\n",
      "Seen so far: 13120 samples\n",
      "Training loss (for one batch) at step 205: 0.9725\n",
      "Seen so far: 13184 samples\n",
      "Training loss (for one batch) at step 206: 1.1622\n",
      "Seen so far: 13248 samples\n",
      "Training loss (for one batch) at step 207: 0.8767\n",
      "Seen so far: 13312 samples\n",
      "Training loss (for one batch) at step 208: 1.6082\n",
      "Seen so far: 13376 samples\n",
      "Training loss (for one batch) at step 209: 0.9420\n",
      "Seen so far: 13440 samples\n",
      "Training loss (for one batch) at step 210: 0.8870\n",
      "Seen so far: 13504 samples\n",
      "Training loss (for one batch) at step 211: 1.1282\n",
      "Seen so far: 13568 samples\n",
      "Training loss (for one batch) at step 212: 0.9105\n",
      "Seen so far: 13632 samples\n",
      "Training loss (for one batch) at step 213: 1.1624\n",
      "Seen so far: 13696 samples\n",
      "Training loss (for one batch) at step 214: 0.8460\n",
      "Seen so far: 13760 samples\n",
      "Training loss (for one batch) at step 215: 0.9251\n",
      "Seen so far: 13824 samples\n",
      "Training loss (for one batch) at step 216: 1.6167\n",
      "Seen so far: 13888 samples\n",
      "Training loss (for one batch) at step 217: 1.1525\n",
      "Seen so far: 13952 samples\n",
      "Training loss (for one batch) at step 218: 1.0388\n",
      "Seen so far: 14016 samples\n",
      "Training loss (for one batch) at step 219: 1.3507\n",
      "Seen so far: 14080 samples\n",
      "Training loss (for one batch) at step 220: 0.8046\n",
      "Seen so far: 14144 samples\n",
      "Training loss (for one batch) at step 221: 0.8114\n",
      "Seen so far: 14208 samples\n",
      "Training loss (for one batch) at step 222: 0.8071\n",
      "Seen so far: 14272 samples\n",
      "Training loss (for one batch) at step 223: 0.8118\n",
      "Seen so far: 14336 samples\n",
      "Training loss (for one batch) at step 224: 0.7673\n",
      "Seen so far: 14400 samples\n",
      "Training loss (for one batch) at step 225: 1.1306\n",
      "Seen so far: 14464 samples\n",
      "Training loss (for one batch) at step 226: 1.1782\n",
      "Seen so far: 14528 samples\n",
      "Training loss (for one batch) at step 227: 1.1947\n",
      "Seen so far: 14592 samples\n",
      "Training loss (for one batch) at step 228: 1.4015\n",
      "Seen so far: 14656 samples\n",
      "Training loss (for one batch) at step 229: 2.0306\n",
      "Seen so far: 14720 samples\n",
      "Training loss (for one batch) at step 230: 1.0168\n",
      "Seen so far: 14784 samples\n",
      "Training loss (for one batch) at step 231: 1.3306\n",
      "Seen so far: 14848 samples\n",
      "Training loss (for one batch) at step 232: 0.8018\n",
      "Seen so far: 14912 samples\n",
      "Training loss (for one batch) at step 233: 1.1244\n",
      "Seen so far: 14976 samples\n",
      "Training loss (for one batch) at step 234: 0.8026\n",
      "Seen so far: 15040 samples\n",
      "Training loss (for one batch) at step 235: 1.4689\n",
      "Seen so far: 15104 samples\n",
      "Training loss (for one batch) at step 236: 1.6078\n",
      "Seen so far: 15168 samples\n",
      "Training loss (for one batch) at step 237: 1.3428\n",
      "Seen so far: 15232 samples\n",
      "Training loss (for one batch) at step 238: 0.9474\n",
      "Seen so far: 15296 samples\n",
      "Training loss (for one batch) at step 239: 2.1110\n",
      "Seen so far: 15360 samples\n",
      "Training loss (for one batch) at step 240: 0.9421\n",
      "Seen so far: 15424 samples\n",
      "Training loss (for one batch) at step 241: 1.3520\n",
      "Seen so far: 15488 samples\n",
      "Training loss (for one batch) at step 242: 0.9710\n",
      "Seen so far: 15552 samples\n",
      "Training loss (for one batch) at step 243: 1.0641\n",
      "Seen so far: 15616 samples\n",
      "Training loss (for one batch) at step 244: 0.7349\n",
      "Seen so far: 15680 samples\n",
      "Training loss (for one batch) at step 245: 1.3809\n",
      "Seen so far: 15744 samples\n",
      "Training loss (for one batch) at step 246: 0.8818\n",
      "Seen so far: 15808 samples\n",
      "Training loss (for one batch) at step 247: 0.9198\n",
      "Seen so far: 15872 samples\n",
      "Training loss (for one batch) at step 248: 1.1490\n",
      "Seen so far: 15936 samples\n",
      "Training loss (for one batch) at step 249: 0.8423\n",
      "Seen so far: 16000 samples\n",
      "Training loss (for one batch) at step 250: 1.4967\n",
      "Seen so far: 16064 samples\n",
      "Training loss (for one batch) at step 251: 0.8043\n",
      "Seen so far: 16128 samples\n",
      "Training loss (for one batch) at step 252: 0.9721\n",
      "Seen so far: 16192 samples\n",
      "Training loss (for one batch) at step 253: 1.0000\n",
      "Seen so far: 16256 samples\n",
      "Training loss (for one batch) at step 254: 1.0415\n",
      "Seen so far: 16320 samples\n",
      "Training loss (for one batch) at step 255: 0.8377\n",
      "Seen so far: 16384 samples\n",
      "Training loss (for one batch) at step 256: 0.8901\n",
      "Seen so far: 16448 samples\n",
      "Training loss (for one batch) at step 257: 0.7997\n",
      "Seen so far: 16512 samples\n",
      "Training loss (for one batch) at step 258: 1.4511\n",
      "Seen so far: 16576 samples\n",
      "Training loss (for one batch) at step 259: 0.9385\n",
      "Seen so far: 16640 samples\n",
      "Training loss (for one batch) at step 260: 1.1783\n",
      "Seen so far: 16704 samples\n",
      "Training loss (for one batch) at step 261: 1.0726\n",
      "Seen so far: 16768 samples\n",
      "Training loss (for one batch) at step 262: 0.8104\n",
      "Seen so far: 16832 samples\n",
      "Training loss (for one batch) at step 263: 1.5649\n",
      "Seen so far: 16896 samples\n",
      "Training loss (for one batch) at step 264: 1.2311\n",
      "Seen so far: 16960 samples\n",
      "Training loss (for one batch) at step 265: 0.8453\n",
      "Seen so far: 17024 samples\n",
      "Training loss (for one batch) at step 266: 0.8487\n",
      "Seen so far: 17088 samples\n",
      "Training loss (for one batch) at step 267: 0.9893\n",
      "Seen so far: 17152 samples\n",
      "Training loss (for one batch) at step 268: 1.2792\n",
      "Seen so far: 17216 samples\n",
      "Training loss (for one batch) at step 269: 1.0213\n",
      "Seen so far: 17280 samples\n",
      "Training loss (for one batch) at step 270: 1.0328\n",
      "Seen so far: 17344 samples\n",
      "Training loss (for one batch) at step 271: 1.0185\n",
      "Seen so far: 17408 samples\n",
      "Training loss (for one batch) at step 272: 1.2153\n",
      "Seen so far: 17472 samples\n",
      "Training loss (for one batch) at step 273: 1.1903\n",
      "Seen so far: 17536 samples\n",
      "Training loss (for one batch) at step 274: 1.0737\n",
      "Seen so far: 17600 samples\n",
      "Training loss (for one batch) at step 275: 1.1171\n",
      "Seen so far: 17664 samples\n",
      "Training loss (for one batch) at step 276: 1.5386\n",
      "Seen so far: 17728 samples\n",
      "Training loss (for one batch) at step 277: 1.2510\n",
      "Seen so far: 17792 samples\n",
      "Training loss (for one batch) at step 278: 1.2264\n",
      "Seen so far: 17856 samples\n",
      "Training loss (for one batch) at step 279: 0.9728\n",
      "Seen so far: 17920 samples\n",
      "Training loss (for one batch) at step 280: 1.4512\n",
      "Seen so far: 17984 samples\n",
      "Training loss (for one batch) at step 281: 0.6486\n",
      "Seen so far: 18048 samples\n",
      "Training loss (for one batch) at step 282: 0.7851\n",
      "Seen so far: 18112 samples\n",
      "Training loss (for one batch) at step 283: 0.8421\n",
      "Seen so far: 18176 samples\n",
      "Training loss (for one batch) at step 284: 1.8421\n",
      "Seen so far: 18240 samples\n",
      "Training loss (for one batch) at step 285: 1.2608\n",
      "Seen so far: 18304 samples\n",
      "Training loss (for one batch) at step 286: 1.3020\n",
      "Seen so far: 18368 samples\n",
      "Training loss (for one batch) at step 287: 0.9950\n",
      "Seen so far: 18432 samples\n",
      "Training loss (for one batch) at step 288: 1.4289\n",
      "Seen so far: 18496 samples\n",
      "Training loss (for one batch) at step 289: 1.0358\n",
      "Seen so far: 18560 samples\n",
      "Training loss (for one batch) at step 290: 1.0463\n",
      "Seen so far: 18624 samples\n",
      "Training loss (for one batch) at step 291: 0.8802\n",
      "Seen so far: 18688 samples\n",
      "Training loss (for one batch) at step 292: 0.9691\n",
      "Seen so far: 18752 samples\n",
      "Training loss (for one batch) at step 293: 1.6873\n",
      "Seen so far: 18816 samples\n",
      "Training loss (for one batch) at step 294: 1.0474\n",
      "Seen so far: 18880 samples\n",
      "Training loss (for one batch) at step 295: 2.0253\n",
      "Seen so far: 18944 samples\n",
      "Training loss (for one batch) at step 296: 1.1042\n",
      "Seen so far: 19008 samples\n",
      "Training loss (for one batch) at step 297: 0.9813\n",
      "Seen so far: 19072 samples\n",
      "Training loss (for one batch) at step 298: 1.1302\n",
      "Seen so far: 19136 samples\n",
      "Training loss (for one batch) at step 299: 1.2996\n",
      "Seen so far: 19200 samples\n",
      "Training loss (for one batch) at step 300: 1.3955\n",
      "Seen so far: 19264 samples\n",
      "Training loss (for one batch) at step 301: 1.0765\n",
      "Seen so far: 19328 samples\n",
      "Training loss (for one batch) at step 302: 1.9402\n",
      "Seen so far: 19392 samples\n",
      "Training loss (for one batch) at step 303: 0.7698\n",
      "Seen so far: 19456 samples\n",
      "Training loss (for one batch) at step 304: 1.2466\n",
      "Seen so far: 19520 samples\n",
      "Training loss (for one batch) at step 305: 0.8755\n",
      "Seen so far: 19584 samples\n",
      "Training loss (for one batch) at step 306: 1.0701\n",
      "Seen so far: 19648 samples\n",
      "Training loss (for one batch) at step 307: 0.9078\n",
      "Seen so far: 19712 samples\n",
      "Training loss (for one batch) at step 308: 1.2758\n",
      "Seen so far: 19776 samples\n",
      "Training loss (for one batch) at step 309: 1.7180\n",
      "Seen so far: 19840 samples\n",
      "Training loss (for one batch) at step 310: 1.1584\n",
      "Seen so far: 19904 samples\n",
      "Training loss (for one batch) at step 311: 0.7952\n",
      "Seen so far: 19968 samples\n",
      "Training loss (for one batch) at step 312: 1.0583\n",
      "Seen so far: 20032 samples\n",
      "Training loss (for one batch) at step 313: 0.9285\n",
      "Seen so far: 20096 samples\n",
      "Training loss (for one batch) at step 314: 1.2900\n",
      "Seen so far: 20160 samples\n",
      "Training loss (for one batch) at step 315: 0.8397\n",
      "Seen so far: 20224 samples\n",
      "Training loss (for one batch) at step 316: 1.3156\n",
      "Seen so far: 20288 samples\n",
      "Training loss (for one batch) at step 317: 1.1705\n",
      "Seen so far: 20352 samples\n",
      "Training loss (for one batch) at step 318: 1.2991\n",
      "Seen so far: 20416 samples\n",
      "Training loss (for one batch) at step 319: 0.9243\n",
      "Seen so far: 20480 samples\n",
      "Training loss (for one batch) at step 320: 0.9616\n",
      "Seen so far: 20544 samples\n",
      "Training loss (for one batch) at step 321: 1.0976\n",
      "Seen so far: 20608 samples\n",
      "Training loss (for one batch) at step 322: 1.0416\n",
      "Seen so far: 20672 samples\n",
      "Training loss (for one batch) at step 323: 1.0579\n",
      "Seen so far: 20736 samples\n",
      "Training loss (for one batch) at step 324: 1.2474\n",
      "Seen so far: 20800 samples\n",
      "Training loss (for one batch) at step 325: 1.3480\n",
      "Seen so far: 20864 samples\n",
      "Training loss (for one batch) at step 326: 0.9602\n",
      "Seen so far: 20928 samples\n",
      "Training loss (for one batch) at step 327: 1.2913\n",
      "Seen so far: 20992 samples\n",
      "Training loss (for one batch) at step 328: 1.4012\n",
      "Seen so far: 21056 samples\n",
      "Training loss (for one batch) at step 329: 1.1971\n",
      "Seen so far: 21120 samples\n",
      "Training loss (for one batch) at step 330: 0.8400\n",
      "Seen so far: 21184 samples\n",
      "Training loss (for one batch) at step 331: 1.3007\n",
      "Seen so far: 21248 samples\n",
      "Training loss (for one batch) at step 332: 1.1605\n",
      "Seen so far: 21312 samples\n",
      "Training loss (for one batch) at step 333: 1.0572\n",
      "Seen so far: 21376 samples\n",
      "Training loss (for one batch) at step 334: 0.9836\n",
      "Seen so far: 21440 samples\n",
      "Training loss (for one batch) at step 335: 1.5416\n",
      "Seen so far: 21504 samples\n",
      "Training loss (for one batch) at step 336: 1.0345\n",
      "Seen so far: 21568 samples\n",
      "Training loss (for one batch) at step 337: 1.3485\n",
      "Seen so far: 21632 samples\n",
      "Training loss (for one batch) at step 338: 1.2043\n",
      "Seen so far: 21696 samples\n",
      "Training loss (for one batch) at step 339: 1.1368\n",
      "Seen so far: 21760 samples\n",
      "Training loss (for one batch) at step 340: 0.9950\n",
      "Seen so far: 21824 samples\n",
      "Training loss (for one batch) at step 341: 0.7420\n",
      "Seen so far: 21888 samples\n",
      "Training loss (for one batch) at step 342: 0.7034\n",
      "Seen so far: 21952 samples\n",
      "Training loss (for one batch) at step 343: 1.0702\n",
      "Seen so far: 22016 samples\n",
      "Training loss (for one batch) at step 344: 1.5617\n",
      "Seen so far: 22080 samples\n",
      "Training loss (for one batch) at step 345: 1.1168\n",
      "Seen so far: 22144 samples\n",
      "Training loss (for one batch) at step 346: 0.8069\n",
      "Seen so far: 22208 samples\n",
      "Training loss (for one batch) at step 347: 0.9284\n",
      "Seen so far: 22272 samples\n",
      "Training loss (for one batch) at step 348: 1.0923\n",
      "Seen so far: 22336 samples\n",
      "Training loss (for one batch) at step 349: 0.9278\n",
      "Seen so far: 22400 samples\n",
      "Training loss (for one batch) at step 350: 0.8944\n",
      "Seen so far: 22464 samples\n",
      "Training loss (for one batch) at step 351: 1.0445\n",
      "Seen so far: 22528 samples\n",
      "Training loss (for one batch) at step 352: 1.1124\n",
      "Seen so far: 22592 samples\n",
      "Training loss (for one batch) at step 353: 0.8750\n",
      "Seen so far: 22656 samples\n",
      "Training loss (for one batch) at step 354: 1.9180\n",
      "Seen so far: 22720 samples\n",
      "Training loss (for one batch) at step 355: 0.8152\n",
      "Seen so far: 22784 samples\n",
      "Training loss (for one batch) at step 356: 1.1167\n",
      "Seen so far: 22848 samples\n",
      "Training loss (for one batch) at step 357: 0.9451\n",
      "Seen so far: 22912 samples\n",
      "Training loss (for one batch) at step 358: 1.0288\n",
      "Seen so far: 22976 samples\n",
      "Training loss (for one batch) at step 359: 0.7124\n",
      "Seen so far: 23040 samples\n",
      "Training loss (for one batch) at step 360: 0.7902\n",
      "Seen so far: 23104 samples\n",
      "Training loss (for one batch) at step 361: 0.9613\n",
      "Seen so far: 23168 samples\n",
      "Training loss (for one batch) at step 362: 0.6500\n",
      "Seen so far: 23232 samples\n",
      "Training loss (for one batch) at step 363: 1.6488\n",
      "Seen so far: 23296 samples\n",
      "Training loss (for one batch) at step 364: 1.2853\n",
      "Seen so far: 23360 samples\n",
      "Training loss (for one batch) at step 365: 0.6461\n",
      "Seen so far: 23424 samples\n",
      "Training loss (for one batch) at step 366: 0.9322\n",
      "Seen so far: 23488 samples\n",
      "Training loss (for one batch) at step 367: 0.7950\n",
      "Seen so far: 23552 samples\n",
      "Training loss (for one batch) at step 368: 1.3611\n",
      "Seen so far: 23616 samples\n",
      "Training loss (for one batch) at step 369: 1.0896\n",
      "Seen so far: 23680 samples\n",
      "Training loss (for one batch) at step 370: 0.6778\n",
      "Seen so far: 23744 samples\n",
      "Training loss (for one batch) at step 371: 0.8474\n",
      "Seen so far: 23808 samples\n",
      "Training loss (for one batch) at step 372: 1.5453\n",
      "Seen so far: 23872 samples\n",
      "Training loss (for one batch) at step 373: 0.8573\n",
      "Seen so far: 23936 samples\n",
      "Training loss (for one batch) at step 374: 0.9574\n",
      "Seen so far: 24000 samples\n",
      "Training loss (for one batch) at step 375: 0.9050\n",
      "Seen so far: 24064 samples\n",
      "Training loss (for one batch) at step 376: 1.7041\n",
      "Seen so far: 24128 samples\n",
      "Training loss (for one batch) at step 377: 1.2240\n",
      "Seen so far: 24192 samples\n",
      "Training loss (for one batch) at step 378: 0.9424\n",
      "Seen so far: 24256 samples\n",
      "Training loss (for one batch) at step 379: 0.7784\n",
      "Seen so far: 24320 samples\n",
      "Training loss (for one batch) at step 380: 0.8410\n",
      "Seen so far: 24384 samples\n",
      "Training loss (for one batch) at step 381: 0.7644\n",
      "Seen so far: 24448 samples\n",
      "Training loss (for one batch) at step 382: 0.9699\n",
      "Seen so far: 24512 samples\n",
      "Training loss (for one batch) at step 383: 0.9176\n",
      "Seen so far: 24576 samples\n",
      "Training loss (for one batch) at step 384: 1.1665\n",
      "Seen so far: 24640 samples\n",
      "Training loss (for one batch) at step 385: 0.8846\n",
      "Seen so far: 24704 samples\n",
      "Training loss (for one batch) at step 386: 0.9661\n",
      "Seen so far: 24768 samples\n",
      "Training loss (for one batch) at step 387: 0.8041\n",
      "Seen so far: 24832 samples\n",
      "Training loss (for one batch) at step 388: 0.9762\n",
      "Seen so far: 24896 samples\n",
      "Training loss (for one batch) at step 389: 0.9511\n",
      "Seen so far: 24960 samples\n",
      "Training loss (for one batch) at step 390: 0.9353\n",
      "Seen so far: 25024 samples\n",
      "Training loss (for one batch) at step 391: 1.2707\n",
      "Seen so far: 25088 samples\n",
      "Training loss (for one batch) at step 392: 0.8622\n",
      "Seen so far: 25152 samples\n",
      "Training loss (for one batch) at step 393: 1.1193\n",
      "Seen so far: 25216 samples\n",
      "Training loss (for one batch) at step 394: 1.0479\n",
      "Seen so far: 25280 samples\n",
      "Training loss (for one batch) at step 395: 0.6703\n",
      "Seen so far: 25344 samples\n",
      "Training loss (for one batch) at step 396: 1.6947\n",
      "Seen so far: 25408 samples\n",
      "Training loss (for one batch) at step 397: 0.8342\n",
      "Seen so far: 25472 samples\n",
      "Training loss (for one batch) at step 398: 0.8141\n",
      "Seen so far: 25536 samples\n",
      "Training loss (for one batch) at step 399: 0.8565\n",
      "Seen so far: 25600 samples\n",
      "Training loss (for one batch) at step 401: 2.2969\n",
      "Seen so far: 25728 samples\n",
      "Training loss (for one batch) at step 402: 0.8894\n",
      "Seen so far: 25792 samples\n",
      "Training loss (for one batch) at step 403: 0.8416\n",
      "Seen so far: 25856 samples\n",
      "Training loss (for one batch) at step 404: 1.3812\n",
      "Seen so far: 25920 samples\n",
      "Training loss (for one batch) at step 405: 0.9738\n",
      "Seen so far: 25984 samples\n",
      "Training loss (for one batch) at step 406: 1.3721\n",
      "Seen so far: 26048 samples\n",
      "Training loss (for one batch) at step 407: 1.0077\n",
      "Seen so far: 26112 samples\n",
      "Training loss (for one batch) at step 408: 1.0806\n",
      "Seen so far: 26176 samples\n",
      "Training loss (for one batch) at step 409: 1.0442\n",
      "Seen so far: 26240 samples\n",
      "Training loss (for one batch) at step 410: 0.9006\n",
      "Seen so far: 26304 samples\n",
      "Training loss (for one batch) at step 411: 0.9035\n",
      "Seen so far: 26368 samples\n",
      "Training loss (for one batch) at step 412: 0.9481\n",
      "Seen so far: 26432 samples\n",
      "Training loss (for one batch) at step 413: 1.1939\n",
      "Seen so far: 26496 samples\n",
      "Training loss (for one batch) at step 414: 1.0614\n",
      "Seen so far: 26560 samples\n",
      "Training loss (for one batch) at step 415: 1.1136\n",
      "Seen so far: 26624 samples\n",
      "Training loss (for one batch) at step 416: 0.9083\n",
      "Seen so far: 26688 samples\n",
      "Training loss (for one batch) at step 417: 1.4492\n",
      "Seen so far: 26752 samples\n",
      "Training loss (for one batch) at step 418: 0.8190\n",
      "Seen so far: 26816 samples\n",
      "Training loss (for one batch) at step 419: 1.0910\n",
      "Seen so far: 26880 samples\n",
      "Training loss (for one batch) at step 420: 0.8829\n",
      "Seen so far: 26944 samples\n",
      "Training loss (for one batch) at step 421: 1.5538\n",
      "Seen so far: 27008 samples\n",
      "Training loss (for one batch) at step 422: 1.2656\n",
      "Seen so far: 27072 samples\n",
      "Training loss (for one batch) at step 423: 0.8648\n",
      "Seen so far: 27136 samples\n",
      "Training loss (for one batch) at step 424: 0.7639\n",
      "Seen so far: 27200 samples\n",
      "Training loss (for one batch) at step 425: 0.8147\n",
      "Seen so far: 27264 samples\n",
      "Training loss (for one batch) at step 426: 1.0064\n",
      "Seen so far: 27328 samples\n",
      "Training loss (for one batch) at step 427: 0.9830\n",
      "Seen so far: 27392 samples\n",
      "Training loss (for one batch) at step 428: 1.1110\n",
      "Seen so far: 27456 samples\n",
      "Training loss (for one batch) at step 429: 0.9587\n",
      "Seen so far: 27520 samples\n",
      "Training loss (for one batch) at step 430: 0.8390\n",
      "Seen so far: 27584 samples\n",
      "Training loss (for one batch) at step 431: 1.3247\n",
      "Seen so far: 27648 samples\n",
      "Training loss (for one batch) at step 432: 0.7559\n",
      "Seen so far: 27712 samples\n",
      "Training loss (for one batch) at step 433: 0.8258\n",
      "Seen so far: 27776 samples\n",
      "Training loss (for one batch) at step 434: 1.3576\n",
      "Seen so far: 27840 samples\n",
      "Training loss (for one batch) at step 435: 1.3386\n",
      "Seen so far: 27904 samples\n",
      "Training loss (for one batch) at step 436: 0.7675\n",
      "Seen so far: 27968 samples\n",
      "Training loss (for one batch) at step 437: 0.7730\n",
      "Seen so far: 28032 samples\n",
      "Training loss (for one batch) at step 438: 1.0692\n",
      "Seen so far: 28096 samples\n",
      "Training loss (for one batch) at step 439: 0.7879\n",
      "Seen so far: 28160 samples\n",
      "Training loss (for one batch) at step 440: 1.0296\n",
      "Seen so far: 28224 samples\n",
      "Training loss (for one batch) at step 441: 0.8225\n",
      "Seen so far: 28288 samples\n",
      "Training loss (for one batch) at step 442: 0.7721\n",
      "Seen so far: 28352 samples\n",
      "Training loss (for one batch) at step 443: 1.0434\n",
      "Seen so far: 28416 samples\n",
      "Training loss (for one batch) at step 444: 1.3482\n",
      "Seen so far: 28480 samples\n",
      "Training loss (for one batch) at step 445: 1.4225\n",
      "Seen so far: 28544 samples\n",
      "Training loss (for one batch) at step 446: 1.8131\n",
      "Seen so far: 28608 samples\n",
      "Training loss (for one batch) at step 447: 0.8105\n",
      "Seen so far: 28672 samples\n",
      "Training loss (for one batch) at step 448: 1.0581\n",
      "Seen so far: 28736 samples\n",
      "Training loss (for one batch) at step 449: 1.1929\n",
      "Seen so far: 28800 samples\n",
      "Training loss (for one batch) at step 450: 1.2214\n",
      "Seen so far: 28864 samples\n",
      "Training loss (for one batch) at step 451: 1.2128\n",
      "Seen so far: 28928 samples\n",
      "Training loss (for one batch) at step 452: 1.5117\n",
      "Seen so far: 28992 samples\n",
      "Training loss (for one batch) at step 453: 1.0893\n",
      "Seen so far: 29056 samples\n",
      "Training loss (for one batch) at step 454: 1.1254\n",
      "Seen so far: 29120 samples\n",
      "Training loss (for one batch) at step 455: 1.3709\n",
      "Seen so far: 29184 samples\n",
      "Training loss (for one batch) at step 456: 0.9251\n",
      "Seen so far: 29248 samples\n",
      "Training loss (for one batch) at step 457: 1.3403\n",
      "Seen so far: 29312 samples\n",
      "Training loss (for one batch) at step 458: 1.1855\n",
      "Seen so far: 29376 samples\n",
      "Training loss (for one batch) at step 459: 0.7461\n",
      "Seen so far: 29440 samples\n",
      "Training loss (for one batch) at step 460: 0.8658\n",
      "Seen so far: 29504 samples\n",
      "Training loss (for one batch) at step 461: 0.9445\n",
      "Seen so far: 29568 samples\n",
      "Training loss (for one batch) at step 462: 0.8950\n",
      "Seen so far: 29632 samples\n",
      "Training loss (for one batch) at step 463: 0.8722\n",
      "Seen so far: 29696 samples\n",
      "Training loss (for one batch) at step 464: 1.0254\n",
      "Seen so far: 29760 samples\n",
      "Training loss (for one batch) at step 465: 0.6099\n",
      "Seen so far: 29824 samples\n",
      "Training loss (for one batch) at step 466: 1.3091\n",
      "Seen so far: 29888 samples\n",
      "Training loss (for one batch) at step 467: 0.9156\n",
      "Seen so far: 29952 samples\n",
      "Training loss (for one batch) at step 468: 1.5730\n",
      "Seen so far: 30016 samples\n",
      "Training loss (for one batch) at step 469: 1.0425\n",
      "Seen so far: 30080 samples\n",
      "Training loss (for one batch) at step 470: 0.8741\n",
      "Seen so far: 30144 samples\n",
      "Training loss (for one batch) at step 471: 1.1959\n",
      "Seen so far: 30208 samples\n",
      "Training loss (for one batch) at step 472: 1.2134\n",
      "Seen so far: 30272 samples\n",
      "Training loss (for one batch) at step 473: 1.1415\n",
      "Seen so far: 30336 samples\n",
      "Training loss (for one batch) at step 474: 1.1197\n",
      "Seen so far: 30400 samples\n",
      "Training loss (for one batch) at step 475: 1.0353\n",
      "Seen so far: 30464 samples\n",
      "Training loss (for one batch) at step 476: 1.0481\n",
      "Seen so far: 30528 samples\n",
      "Training loss (for one batch) at step 477: 0.9317\n",
      "Seen so far: 30592 samples\n",
      "Training loss (for one batch) at step 478: 1.0351\n",
      "Seen so far: 30656 samples\n",
      "Training loss (for one batch) at step 479: 0.9530\n",
      "Seen so far: 30720 samples\n",
      "Training loss (for one batch) at step 480: 0.9033\n",
      "Seen so far: 30784 samples\n",
      "Training loss (for one batch) at step 481: 0.9825\n",
      "Seen so far: 30848 samples\n",
      "Training loss (for one batch) at step 482: 0.7265\n",
      "Seen so far: 30912 samples\n",
      "Training loss (for one batch) at step 483: 1.2869\n",
      "Seen so far: 30976 samples\n",
      "Training loss (for one batch) at step 484: 0.7964\n",
      "Seen so far: 31040 samples\n",
      "Training loss (for one batch) at step 485: 0.9537\n",
      "Seen so far: 31104 samples\n",
      "Training loss (for one batch) at step 486: 0.8612\n",
      "Seen so far: 31168 samples\n",
      "Training loss (for one batch) at step 487: 0.8463\n",
      "Seen so far: 31232 samples\n",
      "Training loss (for one batch) at step 488: 0.6716\n",
      "Seen so far: 31296 samples\n",
      "Training loss (for one batch) at step 489: 0.9478\n",
      "Seen so far: 31360 samples\n",
      "Training loss (for one batch) at step 490: 0.7113\n",
      "Seen so far: 31424 samples\n",
      "Training loss (for one batch) at step 491: 0.8639\n",
      "Seen so far: 31488 samples\n",
      "Training loss (for one batch) at step 492: 1.7115\n",
      "Seen so far: 31552 samples\n",
      "Training loss (for one batch) at step 493: 0.9931\n",
      "Seen so far: 31616 samples\n",
      "Training loss (for one batch) at step 494: 1.1991\n",
      "Seen so far: 31680 samples\n",
      "Training loss (for one batch) at step 495: 0.9680\n",
      "Seen so far: 31744 samples\n",
      "Training loss (for one batch) at step 496: 1.1550\n",
      "Seen so far: 31808 samples\n",
      "Training loss (for one batch) at step 497: 1.0924\n",
      "Seen so far: 31872 samples\n",
      "Training loss (for one batch) at step 498: 1.0326\n",
      "Seen so far: 31936 samples\n",
      "Training loss (for one batch) at step 499: 0.9880\n",
      "Seen so far: 32000 samples\n",
      "Training loss (for one batch) at step 500: 0.9123\n",
      "Seen so far: 32064 samples\n",
      "Training loss (for one batch) at step 501: 0.9547\n",
      "Seen so far: 32128 samples\n",
      "Training loss (for one batch) at step 502: 0.9643\n",
      "Seen so far: 32192 samples\n",
      "Training loss (for one batch) at step 503: 1.4368\n",
      "Seen so far: 32256 samples\n",
      "Training loss (for one batch) at step 504: 1.1001\n",
      "Seen so far: 32320 samples\n",
      "Training loss (for one batch) at step 505: 1.0447\n",
      "Seen so far: 32384 samples\n",
      "Training loss (for one batch) at step 506: 0.9470\n",
      "Seen so far: 32448 samples\n",
      "Training loss (for one batch) at step 507: 0.8220\n",
      "Seen so far: 32512 samples\n",
      "Training loss (for one batch) at step 508: 0.9577\n",
      "Seen so far: 32576 samples\n",
      "Training loss (for one batch) at step 509: 0.6510\n",
      "Seen so far: 32640 samples\n",
      "Training loss (for one batch) at step 510: 1.0651\n",
      "Seen so far: 32704 samples\n",
      "Training loss (for one batch) at step 511: 1.1334\n",
      "Seen so far: 32768 samples\n",
      "Training loss (for one batch) at step 512: 0.8019\n",
      "Seen so far: 32832 samples\n",
      "Training loss (for one batch) at step 513: 0.7907\n",
      "Seen so far: 32896 samples\n",
      "Training loss (for one batch) at step 514: 0.7764\n",
      "Seen so far: 32960 samples\n",
      "Training loss (for one batch) at step 515: 0.9690\n",
      "Seen so far: 33024 samples\n",
      "Training loss (for one batch) at step 516: 0.9232\n",
      "Seen so far: 33088 samples\n",
      "Training loss (for one batch) at step 517: 0.8299\n",
      "Seen so far: 33152 samples\n",
      "Training loss (for one batch) at step 518: 0.9772\n",
      "Seen so far: 33216 samples\n",
      "Training loss (for one batch) at step 519: 1.7245\n",
      "Seen so far: 33280 samples\n",
      "Training loss (for one batch) at step 520: 0.7628\n",
      "Seen so far: 33344 samples\n",
      "Training loss (for one batch) at step 521: 1.0481\n",
      "Seen so far: 33408 samples\n",
      "Training loss (for one batch) at step 522: 1.3882\n",
      "Seen so far: 33472 samples\n",
      "Training loss (for one batch) at step 523: 1.0138\n",
      "Seen so far: 33536 samples\n",
      "Training loss (for one batch) at step 524: 0.9245\n",
      "Seen so far: 33600 samples\n",
      "Training loss (for one batch) at step 525: 1.9544\n",
      "Seen so far: 33664 samples\n",
      "Training loss (for one batch) at step 526: 1.0147\n",
      "Seen so far: 33728 samples\n",
      "Training loss (for one batch) at step 527: 0.7650\n",
      "Seen so far: 33792 samples\n",
      "Training loss (for one batch) at step 528: 0.8830\n",
      "Seen so far: 33856 samples\n",
      "Training loss (for one batch) at step 529: 0.9465\n",
      "Seen so far: 33920 samples\n",
      "Training loss (for one batch) at step 530: 0.9856\n",
      "Seen so far: 33984 samples\n",
      "Training loss (for one batch) at step 531: 0.7205\n",
      "Seen so far: 34048 samples\n",
      "Training loss (for one batch) at step 532: 0.9180\n",
      "Seen so far: 34112 samples\n",
      "Training loss (for one batch) at step 533: 0.9327\n",
      "Seen so far: 34176 samples\n",
      "Training loss (for one batch) at step 534: 1.2376\n",
      "Seen so far: 34240 samples\n",
      "Training loss (for one batch) at step 535: 1.4444\n",
      "Seen so far: 34304 samples\n",
      "Training loss (for one batch) at step 536: 0.8889\n",
      "Seen so far: 34368 samples\n",
      "Training loss (for one batch) at step 537: 0.8640\n",
      "Seen so far: 34432 samples\n",
      "Training loss (for one batch) at step 538: 0.9857\n",
      "Seen so far: 34496 samples\n",
      "Training loss (for one batch) at step 539: 0.9578\n",
      "Seen so far: 34560 samples\n",
      "Training loss (for one batch) at step 540: 0.8868\n",
      "Seen so far: 34624 samples\n",
      "Training loss (for one batch) at step 541: 1.2719\n",
      "Seen so far: 34688 samples\n",
      "Training loss (for one batch) at step 542: 0.9463\n",
      "Seen so far: 34752 samples\n",
      "Training loss (for one batch) at step 543: 0.5956\n",
      "Seen so far: 34816 samples\n",
      "Training loss (for one batch) at step 544: 0.7654\n",
      "Seen so far: 34880 samples\n",
      "Training loss (for one batch) at step 545: 0.9583\n",
      "Seen so far: 34944 samples\n",
      "Training loss (for one batch) at step 546: 1.1595\n",
      "Seen so far: 35008 samples\n",
      "Training loss (for one batch) at step 547: 0.8427\n",
      "Seen so far: 35072 samples\n",
      "Training loss (for one batch) at step 548: 0.8693\n",
      "Seen so far: 35136 samples\n",
      "Training loss (for one batch) at step 549: 1.0929\n",
      "Seen so far: 35200 samples\n",
      "Training loss (for one batch) at step 550: 1.2122\n",
      "Seen so far: 35264 samples\n",
      "Training loss (for one batch) at step 551: 0.6893\n",
      "Seen so far: 35328 samples\n",
      "Training loss (for one batch) at step 552: 0.9754\n",
      "Seen so far: 35392 samples\n",
      "Training loss (for one batch) at step 553: 1.0367\n",
      "Seen so far: 35456 samples\n",
      "Training loss (for one batch) at step 554: 1.0159\n",
      "Seen so far: 35520 samples\n",
      "Training loss (for one batch) at step 555: 0.9002\n",
      "Seen so far: 35584 samples\n",
      "Training loss (for one batch) at step 556: 0.7371\n",
      "Seen so far: 35648 samples\n",
      "Training loss (for one batch) at step 557: 1.2401\n",
      "Seen so far: 35712 samples\n",
      "Training loss (for one batch) at step 558: 0.7748\n",
      "Seen so far: 35776 samples\n",
      "Training loss (for one batch) at step 559: 1.1121\n",
      "Seen so far: 35840 samples\n",
      "Training loss (for one batch) at step 560: 0.9240\n",
      "Seen so far: 35904 samples\n",
      "Training loss (for one batch) at step 561: 1.2655\n",
      "Seen so far: 35968 samples\n",
      "Training loss (for one batch) at step 562: 1.1061\n",
      "Seen so far: 36032 samples\n",
      "Training loss (for one batch) at step 563: 0.7676\n",
      "Seen so far: 36096 samples\n",
      "Training loss (for one batch) at step 564: 0.8241\n",
      "Seen so far: 36160 samples\n",
      "Training loss (for one batch) at step 565: 1.3975\n",
      "Seen so far: 36224 samples\n",
      "Training loss (for one batch) at step 566: 0.8064\n",
      "Seen so far: 36288 samples\n",
      "Training loss (for one batch) at step 567: 0.7598\n",
      "Seen so far: 36352 samples\n",
      "Training loss (for one batch) at step 568: 0.8021\n",
      "Seen so far: 36416 samples\n",
      "Training loss (for one batch) at step 569: 1.0612\n",
      "Seen so far: 36480 samples\n",
      "Training loss (for one batch) at step 570: 1.1098\n",
      "Seen so far: 36544 samples\n",
      "Training loss (for one batch) at step 571: 1.3076\n",
      "Seen so far: 36608 samples\n",
      "Training loss (for one batch) at step 572: 0.8198\n",
      "Seen so far: 36672 samples\n",
      "Training loss (for one batch) at step 573: 0.8456\n",
      "Seen so far: 36736 samples\n",
      "Training loss (for one batch) at step 574: 1.2937\n",
      "Seen so far: 36800 samples\n",
      "Training loss (for one batch) at step 575: 0.9271\n",
      "Seen so far: 36864 samples\n",
      "Training loss (for one batch) at step 576: 0.8457\n",
      "Seen so far: 36928 samples\n",
      "Training loss (for one batch) at step 577: 0.6191\n",
      "Seen so far: 36992 samples\n",
      "Training loss (for one batch) at step 578: 0.7085\n",
      "Seen so far: 37056 samples\n",
      "Training loss (for one batch) at step 579: 0.9339\n",
      "Seen so far: 37120 samples\n",
      "Training loss (for one batch) at step 580: 1.0980\n",
      "Seen so far: 37184 samples\n",
      "Training loss (for one batch) at step 581: 0.9832\n",
      "Seen so far: 37248 samples\n",
      "Training loss (for one batch) at step 582: 0.8332\n",
      "Seen so far: 37312 samples\n",
      "Training loss (for one batch) at step 583: 1.1487\n",
      "Seen so far: 37376 samples\n",
      "Training loss (for one batch) at step 584: 0.9153\n",
      "Seen so far: 37440 samples\n",
      "Training loss (for one batch) at step 585: 0.8426\n",
      "Seen so far: 37504 samples\n",
      "Training loss (for one batch) at step 586: 0.7821\n",
      "Seen so far: 37568 samples\n",
      "Training loss (for one batch) at step 587: 1.0955\n",
      "Seen so far: 37632 samples\n",
      "Training loss (for one batch) at step 588: 1.0486\n",
      "Seen so far: 37696 samples\n",
      "Training loss (for one batch) at step 589: 0.8433\n",
      "Seen so far: 37760 samples\n",
      "Training loss (for one batch) at step 590: 0.8343\n",
      "Seen so far: 37824 samples\n",
      "Training loss (for one batch) at step 591: 1.4008\n",
      "Seen so far: 37888 samples\n",
      "Training loss (for one batch) at step 592: 0.6726\n",
      "Seen so far: 37952 samples\n",
      "Training loss (for one batch) at step 593: 1.5323\n",
      "Seen so far: 38016 samples\n",
      "Training loss (for one batch) at step 594: 0.9320\n",
      "Seen so far: 38080 samples\n",
      "Training loss (for one batch) at step 595: 0.8825\n",
      "Seen so far: 38144 samples\n",
      "Training loss (for one batch) at step 596: 0.6512\n",
      "Seen so far: 38208 samples\n",
      "Training loss (for one batch) at step 597: 1.0645\n",
      "Seen so far: 38272 samples\n",
      "Training loss (for one batch) at step 598: 1.1362\n",
      "Seen so far: 38336 samples\n",
      "Training loss (for one batch) at step 599: 0.7452\n",
      "Seen so far: 38400 samples\n",
      "Training loss (for one batch) at step 601: 0.9124\n",
      "Seen so far: 38528 samples\n",
      "Training loss (for one batch) at step 602: 0.8773\n",
      "Seen so far: 38592 samples\n",
      "Training loss (for one batch) at step 603: 0.8696\n",
      "Seen so far: 38656 samples\n",
      "Training loss (for one batch) at step 604: 1.1053\n",
      "Seen so far: 38720 samples\n",
      "Training loss (for one batch) at step 605: 0.8558\n",
      "Seen so far: 38784 samples\n",
      "Training loss (for one batch) at step 606: 0.8548\n",
      "Seen so far: 38848 samples\n",
      "Training loss (for one batch) at step 607: 0.7355\n",
      "Seen so far: 38912 samples\n",
      "Training loss (for one batch) at step 608: 1.5149\n",
      "Seen so far: 38976 samples\n",
      "Training loss (for one batch) at step 609: 0.7748\n",
      "Seen so far: 39040 samples\n",
      "Training loss (for one batch) at step 610: 0.9474\n",
      "Seen so far: 39104 samples\n",
      "Training loss (for one batch) at step 611: 0.9438\n",
      "Seen so far: 39168 samples\n",
      "Training loss (for one batch) at step 612: 0.9574\n",
      "Seen so far: 39232 samples\n",
      "Training loss (for one batch) at step 613: 1.1720\n",
      "Seen so far: 39296 samples\n",
      "Training loss (for one batch) at step 614: 0.8568\n",
      "Seen so far: 39360 samples\n",
      "Training loss (for one batch) at step 615: 0.9937\n",
      "Seen so far: 39424 samples\n",
      "Training loss (for one batch) at step 616: 0.9102\n",
      "Seen so far: 39488 samples\n",
      "Training loss (for one batch) at step 617: 0.9043\n",
      "Seen so far: 39552 samples\n",
      "Training loss (for one batch) at step 618: 0.9222\n",
      "Seen so far: 39616 samples\n",
      "Training loss (for one batch) at step 619: 1.0038\n",
      "Seen so far: 39680 samples\n",
      "Training loss (for one batch) at step 620: 1.0128\n",
      "Seen so far: 39744 samples\n",
      "Training loss (for one batch) at step 621: 0.9499\n",
      "Seen so far: 39808 samples\n",
      "Training loss (for one batch) at step 622: 0.5383\n",
      "Seen so far: 39872 samples\n",
      "Training loss (for one batch) at step 623: 1.0637\n",
      "Seen so far: 39936 samples\n",
      "Training loss (for one batch) at step 624: 1.8609\n",
      "Seen so far: 40000 samples\n",
      "Training loss (for one batch) at step 625: 0.6533\n",
      "Seen so far: 40064 samples\n",
      "Training loss (for one batch) at step 626: 0.9700\n",
      "Seen so far: 40128 samples\n",
      "Training loss (for one batch) at step 627: 0.6647\n",
      "Seen so far: 40192 samples\n",
      "Training loss (for one batch) at step 628: 0.9226\n",
      "Seen so far: 40256 samples\n",
      "Training loss (for one batch) at step 629: 0.6469\n",
      "Seen so far: 40320 samples\n",
      "Training loss (for one batch) at step 630: 0.9051\n",
      "Seen so far: 40384 samples\n",
      "Training loss (for one batch) at step 631: 0.8357\n",
      "Seen so far: 40448 samples\n",
      "Training loss (for one batch) at step 632: 0.9719\n",
      "Seen so far: 40512 samples\n",
      "Training loss (for one batch) at step 633: 0.9593\n",
      "Seen so far: 40576 samples\n",
      "Training loss (for one batch) at step 634: 0.9490\n",
      "Seen so far: 40640 samples\n",
      "Training loss (for one batch) at step 635: 1.1682\n",
      "Seen so far: 40704 samples\n",
      "Training loss (for one batch) at step 636: 0.8164\n",
      "Seen so far: 40768 samples\n",
      "Training loss (for one batch) at step 637: 0.7491\n",
      "Seen so far: 40832 samples\n",
      "Training loss (for one batch) at step 638: 1.5629\n",
      "Seen so far: 40896 samples\n",
      "Training loss (for one batch) at step 639: 1.1642\n",
      "Seen so far: 40960 samples\n",
      "Training loss (for one batch) at step 640: 0.8041\n",
      "Seen so far: 41024 samples\n",
      "Training loss (for one batch) at step 641: 0.9126\n",
      "Seen so far: 41088 samples\n",
      "Training loss (for one batch) at step 642: 0.8941\n",
      "Seen so far: 41152 samples\n",
      "Training loss (for one batch) at step 643: 1.2709\n",
      "Seen so far: 41216 samples\n",
      "Training loss (for one batch) at step 644: 0.7848\n",
      "Seen so far: 41280 samples\n",
      "Training loss (for one batch) at step 645: 0.7348\n",
      "Seen so far: 41344 samples\n",
      "Training loss (for one batch) at step 646: 0.6927\n",
      "Seen so far: 41408 samples\n",
      "Training loss (for one batch) at step 647: 1.1910\n",
      "Seen so far: 41472 samples\n",
      "Training loss (for one batch) at step 648: 1.0106\n",
      "Seen so far: 41536 samples\n",
      "Training loss (for one batch) at step 649: 1.0933\n",
      "Seen so far: 41600 samples\n",
      "Training loss (for one batch) at step 650: 1.0482\n",
      "Seen so far: 41664 samples\n",
      "Training loss (for one batch) at step 651: 0.8144\n",
      "Seen so far: 41728 samples\n",
      "Training loss (for one batch) at step 652: 1.4044\n",
      "Seen so far: 41792 samples\n",
      "Training loss (for one batch) at step 653: 1.2212\n",
      "Seen so far: 41856 samples\n",
      "Training loss (for one batch) at step 654: 0.7409\n",
      "Seen so far: 41920 samples\n",
      "Training loss (for one batch) at step 655: 1.0325\n",
      "Seen so far: 41984 samples\n",
      "Training loss (for one batch) at step 656: 0.9912\n",
      "Seen so far: 42048 samples\n",
      "Training loss (for one batch) at step 657: 0.9764\n",
      "Seen so far: 42112 samples\n",
      "Training loss (for one batch) at step 658: 1.0027\n",
      "Seen so far: 42176 samples\n",
      "Training loss (for one batch) at step 659: 1.0577\n",
      "Seen so far: 42240 samples\n",
      "Training loss (for one batch) at step 660: 1.1690\n",
      "Seen so far: 42304 samples\n",
      "Training loss (for one batch) at step 661: 1.2727\n",
      "Seen so far: 42368 samples\n",
      "Training loss (for one batch) at step 662: 0.8892\n",
      "Seen so far: 42432 samples\n",
      "Training loss (for one batch) at step 663: 0.7386\n",
      "Seen so far: 42496 samples\n",
      "Training loss (for one batch) at step 664: 0.6486\n",
      "Seen so far: 42560 samples\n",
      "Training loss (for one batch) at step 665: 1.7237\n",
      "Seen so far: 42624 samples\n",
      "Training loss (for one batch) at step 666: 0.6904\n",
      "Seen so far: 42688 samples\n",
      "Training loss (for one batch) at step 667: 0.9794\n",
      "Seen so far: 42752 samples\n",
      "Training loss (for one batch) at step 668: 0.6521\n",
      "Seen so far: 42816 samples\n",
      "Training loss (for one batch) at step 669: 0.5675\n",
      "Seen so far: 42880 samples\n",
      "Training loss (for one batch) at step 670: 2.3461\n",
      "Seen so far: 42944 samples\n",
      "Training loss (for one batch) at step 671: 0.9804\n",
      "Seen so far: 43008 samples\n",
      "Training loss (for one batch) at step 672: 0.8258\n",
      "Seen so far: 43072 samples\n",
      "Training loss (for one batch) at step 673: 0.8873\n",
      "Seen so far: 43136 samples\n",
      "Training loss (for one batch) at step 674: 1.0144\n",
      "Seen so far: 43200 samples\n",
      "Training loss (for one batch) at step 675: 0.8672\n",
      "Seen so far: 43264 samples\n",
      "Training loss (for one batch) at step 676: 1.1837\n",
      "Seen so far: 43328 samples\n",
      "Training loss (for one batch) at step 677: 0.9127\n",
      "Seen so far: 43392 samples\n",
      "Training loss (for one batch) at step 678: 0.6797\n",
      "Seen so far: 43456 samples\n",
      "Training loss (for one batch) at step 679: 1.2892\n",
      "Seen so far: 43520 samples\n",
      "Training loss (for one batch) at step 680: 1.0048\n",
      "Seen so far: 43584 samples\n",
      "Training loss (for one batch) at step 681: 0.8477\n",
      "Seen so far: 43648 samples\n",
      "Training loss (for one batch) at step 682: 1.0968\n",
      "Seen so far: 43712 samples\n",
      "Training loss (for one batch) at step 683: 0.8657\n",
      "Seen so far: 43776 samples\n",
      "Training loss (for one batch) at step 684: 0.6870\n",
      "Seen so far: 43840 samples\n",
      "Training loss (for one batch) at step 685: 0.9724\n",
      "Seen so far: 43904 samples\n",
      "Training loss (for one batch) at step 686: 0.6663\n",
      "Seen so far: 43968 samples\n",
      "Training loss (for one batch) at step 687: 0.6914\n",
      "Seen so far: 44032 samples\n",
      "Training loss (for one batch) at step 688: 1.0216\n",
      "Seen so far: 44096 samples\n",
      "Training loss (for one batch) at step 689: 0.9351\n",
      "Seen so far: 44160 samples\n",
      "Training loss (for one batch) at step 690: 0.8684\n",
      "Seen so far: 44224 samples\n",
      "Training loss (for one batch) at step 691: 0.7495\n",
      "Seen so far: 44288 samples\n",
      "Training loss (for one batch) at step 692: 0.8550\n",
      "Seen so far: 44352 samples\n",
      "Training loss (for one batch) at step 693: 0.8921\n",
      "Seen so far: 44416 samples\n",
      "Training loss (for one batch) at step 694: 1.1199\n",
      "Seen so far: 44480 samples\n",
      "Training loss (for one batch) at step 695: 0.9805\n",
      "Seen so far: 44544 samples\n",
      "Training loss (for one batch) at step 696: 0.7777\n",
      "Seen so far: 44608 samples\n",
      "Training loss (for one batch) at step 697: 0.8241\n",
      "Seen so far: 44672 samples\n",
      "Training loss (for one batch) at step 698: 0.6773\n",
      "Seen so far: 44736 samples\n",
      "Training loss (for one batch) at step 699: 0.7403\n",
      "Seen so far: 44800 samples\n",
      "Training loss (for one batch) at step 700: 0.8112\n",
      "Seen so far: 44864 samples\n",
      "Training loss (for one batch) at step 701: 0.7266\n",
      "Seen so far: 44928 samples\n",
      "Training loss (for one batch) at step 702: 0.7460\n",
      "Seen so far: 44992 samples\n",
      "Training loss (for one batch) at step 703: 0.7494\n",
      "Seen so far: 45056 samples\n",
      "Training loss (for one batch) at step 704: 0.9733\n",
      "Seen so far: 45120 samples\n",
      "Training loss (for one batch) at step 705: 0.8711\n",
      "Seen so far: 45184 samples\n",
      "Training loss (for one batch) at step 706: 0.8356\n",
      "Seen so far: 45248 samples\n",
      "Training loss (for one batch) at step 707: 1.0797\n",
      "Seen so far: 45312 samples\n",
      "Training loss (for one batch) at step 708: 0.9671\n",
      "Seen so far: 45376 samples\n",
      "Training loss (for one batch) at step 709: 0.7667\n",
      "Seen so far: 45440 samples\n",
      "Training loss (for one batch) at step 710: 0.9348\n",
      "Seen so far: 45504 samples\n",
      "Training loss (for one batch) at step 711: 0.7021\n",
      "Seen so far: 45568 samples\n",
      "Training loss (for one batch) at step 712: 0.7580\n",
      "Seen so far: 45632 samples\n",
      "Training loss (for one batch) at step 713: 0.6665\n",
      "Seen so far: 45696 samples\n",
      "Training loss (for one batch) at step 714: 1.1303\n",
      "Seen so far: 45760 samples\n",
      "Training loss (for one batch) at step 715: 0.7492\n",
      "Seen so far: 45824 samples\n",
      "Training loss (for one batch) at step 716: 0.8864\n",
      "Seen so far: 45888 samples\n",
      "Training loss (for one batch) at step 717: 0.9103\n",
      "Seen so far: 45952 samples\n",
      "Training loss (for one batch) at step 718: 0.7815\n",
      "Seen so far: 46016 samples\n",
      "Training loss (for one batch) at step 719: 1.1161\n",
      "Seen so far: 46080 samples\n",
      "Training loss (for one batch) at step 720: 0.8973\n",
      "Seen so far: 46144 samples\n",
      "Training loss (for one batch) at step 721: 0.8818\n",
      "Seen so far: 46208 samples\n",
      "Training loss (for one batch) at step 722: 0.9057\n",
      "Seen so far: 46272 samples\n",
      "Training loss (for one batch) at step 723: 0.7454\n",
      "Seen so far: 46336 samples\n",
      "Training loss (for one batch) at step 724: 0.6270\n",
      "Seen so far: 46400 samples\n",
      "Training loss (for one batch) at step 725: 0.8068\n",
      "Seen so far: 46464 samples\n",
      "Training loss (for one batch) at step 726: 0.9634\n",
      "Seen so far: 46528 samples\n",
      "Training loss (for one batch) at step 727: 1.2352\n",
      "Seen so far: 46592 samples\n",
      "Training loss (for one batch) at step 728: 0.6285\n",
      "Seen so far: 46656 samples\n",
      "Training loss (for one batch) at step 729: 1.0612\n",
      "Seen so far: 46720 samples\n",
      "Training loss (for one batch) at step 730: 0.9107\n",
      "Seen so far: 46784 samples\n",
      "Training loss (for one batch) at step 731: 1.1633\n",
      "Seen so far: 46848 samples\n",
      "Training loss (for one batch) at step 732: 0.7456\n",
      "Seen so far: 46912 samples\n",
      "Training loss (for one batch) at step 733: 0.8219\n",
      "Seen so far: 46976 samples\n",
      "Training loss (for one batch) at step 734: 0.9085\n",
      "Seen so far: 47040 samples\n",
      "Training loss (for one batch) at step 735: 0.7820\n",
      "Seen so far: 47104 samples\n",
      "Training loss (for one batch) at step 736: 0.9130\n",
      "Seen so far: 47168 samples\n",
      "Training loss (for one batch) at step 737: 0.5204\n",
      "Seen so far: 47232 samples\n",
      "Training loss (for one batch) at step 738: 0.9216\n",
      "Seen so far: 47296 samples\n",
      "Training loss (for one batch) at step 739: 0.9256\n",
      "Seen so far: 47360 samples\n",
      "Training loss (for one batch) at step 740: 0.9353\n",
      "Seen so far: 47424 samples\n",
      "Training loss (for one batch) at step 741: 0.5419\n",
      "Seen so far: 47488 samples\n",
      "Training loss (for one batch) at step 742: 1.4911\n",
      "Seen so far: 47552 samples\n",
      "Training loss (for one batch) at step 743: 0.8483\n",
      "Seen so far: 47616 samples\n",
      "Training loss (for one batch) at step 744: 0.8080\n",
      "Seen so far: 47680 samples\n",
      "Training loss (for one batch) at step 745: 0.8067\n",
      "Seen so far: 47744 samples\n",
      "Training loss (for one batch) at step 746: 0.6125\n",
      "Seen so far: 47808 samples\n",
      "Training loss (for one batch) at step 747: 0.6346\n",
      "Seen so far: 47872 samples\n",
      "Training loss (for one batch) at step 748: 1.0520\n",
      "Seen so far: 47936 samples\n",
      "Training loss (for one batch) at step 749: 0.4519\n",
      "Seen so far: 48000 samples\n",
      "Training loss (for one batch) at step 750: 0.8531\n",
      "Seen so far: 48064 samples\n",
      "Training loss (for one batch) at step 751: 0.7370\n",
      "Seen so far: 48128 samples\n",
      "Training loss (for one batch) at step 752: 0.9657\n",
      "Seen so far: 48192 samples\n",
      "Training loss (for one batch) at step 753: 0.7108\n",
      "Seen so far: 48256 samples\n",
      "Training loss (for one batch) at step 754: 0.5425\n",
      "Seen so far: 48320 samples\n",
      "Training loss (for one batch) at step 755: 0.8745\n",
      "Seen so far: 48384 samples\n",
      "Training loss (for one batch) at step 756: 0.6491\n",
      "Seen so far: 48448 samples\n",
      "Training loss (for one batch) at step 757: 0.6503\n",
      "Seen so far: 48512 samples\n",
      "Training loss (for one batch) at step 758: 0.8362\n",
      "Seen so far: 48576 samples\n",
      "Training loss (for one batch) at step 759: 0.8924\n",
      "Seen so far: 48640 samples\n",
      "Training loss (for one batch) at step 760: 0.7505\n",
      "Seen so far: 48704 samples\n",
      "Training loss (for one batch) at step 761: 0.9079\n",
      "Seen so far: 48768 samples\n",
      "Training loss (for one batch) at step 762: 1.0498\n",
      "Seen so far: 48832 samples\n",
      "Training loss (for one batch) at step 763: 0.5417\n",
      "Seen so far: 48896 samples\n",
      "Training loss (for one batch) at step 764: 0.6845\n",
      "Seen so far: 48960 samples\n",
      "Training loss (for one batch) at step 765: 1.2345\n",
      "Seen so far: 49024 samples\n",
      "Training loss (for one batch) at step 766: 0.9142\n",
      "Seen so far: 49088 samples\n",
      "Training loss (for one batch) at step 767: 0.5151\n",
      "Seen so far: 49152 samples\n",
      "Training loss (for one batch) at step 768: 1.3045\n",
      "Seen so far: 49216 samples\n",
      "Training loss (for one batch) at step 769: 1.5114\n",
      "Seen so far: 49280 samples\n",
      "Training loss (for one batch) at step 770: 1.0908\n",
      "Seen so far: 49344 samples\n",
      "Training loss (for one batch) at step 771: 0.7330\n",
      "Seen so far: 49408 samples\n",
      "Training loss (for one batch) at step 772: 0.5758\n",
      "Seen so far: 49472 samples\n",
      "Training loss (for one batch) at step 773: 0.6446\n",
      "Seen so far: 49536 samples\n",
      "Training loss (for one batch) at step 774: 0.8171\n",
      "Seen so far: 49600 samples\n",
      "Training loss (for one batch) at step 775: 0.7063\n",
      "Seen so far: 49664 samples\n",
      "Training loss (for one batch) at step 776: 0.8069\n",
      "Seen so far: 49728 samples\n",
      "Training loss (for one batch) at step 777: 0.4547\n",
      "Seen so far: 49792 samples\n",
      "Training loss (for one batch) at step 778: 0.6944\n",
      "Seen so far: 49856 samples\n",
      "Training loss (for one batch) at step 779: 0.5106\n",
      "Seen so far: 49920 samples\n",
      "Training loss (for one batch) at step 780: 1.6667\n",
      "Seen so far: 49984 samples\n",
      "Training loss (for one batch) at step 781: 0.5225\n",
      "Seen so far: 50048 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print('Start epoch %d' % epoch)\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            \n",
    "            loss_value = loss_f(y_batch_train, logits)\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        if step % 200:\n",
    "            print(\"Training loss (for one batch) at step %d: %.4f\" % (step, float(loss_value)))\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-level handling of metrics\n",
    "\n",
    "Let's add metrics monitoring to this basic loop.\n",
    "\n",
    "You can readily reuse the built-in metrics (or custom ones you wrote) in such training loops written from scratch. Here's the flow:\n",
    "\n",
    "Instantiate the metric at the start of the loop\n",
    "- Call metric.update_state() after each batch\n",
    "- Call metric.result() when you need to display the current value of the metric\n",
    "- Call metric.reset_states() when you need to clear the state of the metric (typically at the end of an epoch)\n",
    "\n",
    "Let's use this knowledge to compute SparseCategoricalAccuracy on validation data at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 129.7590\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.3273\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.9911\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7046\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.7297\n",
      "Validation acc: 0.8133\n",
      "Time taken: 5.11s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.7668\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.6095\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.2927\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.4104\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8377\n",
      "Validation acc: 0.8635\n",
      "Time taken: 4.45s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding-up your training step with tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.2368\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.5075\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.4994\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.4527\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8669\n",
      "Validation acc: 0.8790\n",
      "Time taken: 1.25s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.2381\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.4733\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.4445\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.5088\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8843\n",
      "Validation acc: 0.8901\n",
      "Time taken: 0.91s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-level handling of losses tracked by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "        # Add any extra losses created during the forward pass.\n",
    "        loss_value += sum(model.losses)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
