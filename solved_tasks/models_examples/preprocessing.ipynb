{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение к стандартному распределению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 0.        , 0.33333333]),\n",
       " array([0.81649658, 0.81649658, 1.24721913]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)\n",
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## В диапазон\n",
    "Пример [0 1]\n",
    "\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ограничить амплитуду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение к равномерному распределению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Алексей\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\preprocessing\\_data.py:2593: UserWarning: n_quantiles (1000) is greater than the total number of samples (112). n_quantiles is set to n_samples.\n",
      "  \"n_samples.\" % (self.n_quantiles, n_samples)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.1, 5.8, 6.5, 7.9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "quantile_transformer = QuantileTransformer(random_state=0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.23873874, 0.50900901, 0.74324324, 1.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение к гауссовому распределению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49024349,  0.17881995, -0.1563781 ],\n",
       "       [-0.05102892,  0.58863195, -0.57612414],\n",
       "       [ 0.69420009, -0.84857822,  0.10051454]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\n",
    "X_lognormal\n",
    "\n",
    "\n",
    "\n",
    "pt.fit_transform(X_lognormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize, Normalizer\n",
    "\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодировка категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc.fit(X)\n",
    "\n",
    "enc.transform([['female', 'from US', 'uses Safari']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставка значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = [['male'], ['female'], [np.nan], ['female']]\n",
    "\n",
    "enc = Pipeline(steps=[\n",
    "    (\"encoder\", OrdinalEncoder()),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "])\n",
    "enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кодировка по принадлежности к категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\n",
    "enc = OneHotEncoder()\n",
    "enc.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['female', 'male'], dtype=object),\n",
       " array(['from Europe', 'from US'], dtype=object),\n",
       " array(['uses Firefox', 'uses Safari'], dtype=object)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы оставить только одну колонку для бинарных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [['male', 'from US', 'uses Safari'],\n",
    "     ['female', 'from Europe', 'uses Firefox']]\n",
    "drop_enc = OneHotEncoder(drop='first').fit(X)\n",
    "drop_enc.categories_\n",
    "\n",
    "\n",
    "drop_enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [['male', 'US', 'Safari'],\n",
    "     ['female', 'Europe', 'Firefox'],\n",
    "     ['female', 'Asia', 'Chrome']]\n",
    "drop_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
    "drop_enc.categories_\n",
    "\n",
    "\n",
    "drop_enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дискретизация данных разделением на участки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [2., 2., 2., 1.],\n",
       "       [2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "X = [[-2, 1, -4,   -1],\n",
    "     [-1, 2, -3, -0.5],\n",
    "     [ 0, 3, -2,  0.5],\n",
    "     [ 1, 4, -1,    2]]\n",
    "est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "est.fit(X)\n",
    "\n",
    "Xt = est.transform(X)\n",
    "Xt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2., -1.,  0.,  1.]),\n",
       " array([[-1.5,  1.5, -3.5, -0.5],\n",
       "        [-0.5,  2.5, -2.5, -0.5],\n",
       "        [ 0.5,  3.5, -1.5,  0.5],\n",
       "        [ 0.5,  3.5, -1.5,  1.5]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.bin_edges_[0],  est.inverse_transform(Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бинаризация по пороговому значению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=1.1)\n",
    "\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5., 16., 20., 25.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование пользовательских функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.09861229, 1.38629436]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "# Since FunctionTransformer is no-op during fit, we can call transform directly\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "### Available preprocessing\n",
    "\n",
    "- Text preprocessing\n",
    "    - tf.keras.layers.TextVectorization: turns raw strings into an encoded representation that can be read by an Embedding layer or Dense layer.\n",
    "- Numerical features preprocessing\n",
    "    - tf.keras.layers.Normalization: performs feature-wise normalize of input features.\n",
    "    - tf.keras.layers.Discretization: turns continuous numerical features into integer categorical features.\n",
    "- Categorical features preprocessing\n",
    "    - tf.keras.layers.CategoryEncoding: turns integer categorical features into one-hot, multi-hot, or count dense representations.\n",
    "    - tf.keras.layers.Hashing: performs categorical feature hashing, also known as the \"hashing trick\".\n",
    "    - tf.keras.layers.StringLookup: turns string categorical values an encoded representation that can be read by an Embedding layer or Dense layer.\n",
    "    - tf.keras.layers.IntegerLookup: turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.\n",
    "\n",
    "- Image preprocessing, These layers are for standardizing the inputs of an image model.\n",
    "\n",
    "    - tf.keras.layers.Resizing: resizes a batch of images to a target size.\n",
    "    - tf.keras.layers.Rescaling: rescales and offsets the values of a batch of image (e.g. go from inputs in the [0, 255] range to inputs in the [0, 1] range.\n",
    "    - tf.keras.layers.CenterCrop: returns a center crop of a batch of images.\n",
    "- Image data augmentation, These layers apply random augmentation transforms to a batch of images. They are only active during training.\n",
    "\n",
    "    - tf.keras.layers.RandomCrop\n",
    "    - tf.keras.layers.RandomFlip\n",
    "    - tf.keras.layers.RandomTranslation\n",
    "    - tf.keras.layers.RandomRotation\n",
    "    - tf.keras.layers.RandomZoom\n",
    "    - tf.keras.layers.RandomHeight\n",
    "    - tf.keras.layers.RandomWidth\n",
    "    - tf.keras.layers.RandomContrast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The adapt() method\n",
    "\n",
    "Some preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n",
    "\n",
    "- TextVectorization: holds a mapping between string tokens and integer indices\n",
    "\n",
    "- StringLookup and IntegerLookup: hold a mapping between input values and integer indices.\n",
    "- Normalization: holds the mean and standard deviation of the features.\n",
    "- Discretization: holds information about value bucket boundaries.\n",
    "\n",
    "Crucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data.\n",
    "\n",
    "You set the state of a preprocessing layer by exposing it to training data, via the adapt() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features mean: -0.00\n",
      "Features std: 1.00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data = np.array([[0.1, 0.2, 0.3], [0.8, 0.9, 1.0], [1.5, 1.6, 1.7],])\n",
    "layer = layers.Normalization()\n",
    "layer.adapt(data)\n",
    "normalized_data = layer(data)\n",
    "\n",
    "print(\"Features mean: %.2f\" % (normalized_data.numpy().mean()))\n",
    "print(\"Features std: %.2f\" % (normalized_data.numpy().std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[37 12 25  5  9 20 21  0  0]\n",
      " [51 34 27 33 29 18  0  0  0]\n",
      " [49 52 30 31 19 46 10  0  0]\n",
      " [ 7  5 50 43 28  7 47 17  0]\n",
      " [24 35 39 40  3  6 32 16  0]\n",
      " [ 4  2 15 14 22 23  0  0  0]\n",
      " [36 48  6 38 42  3 45  0  0]\n",
      " [ 4  2 13 41 53  8 44 26 11]], shape=(8, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"ξεῖν᾽, ἦ τοι μὲν ὄνειροι ἀμήχανοι ἀκριτόμυθοι\",\n",
    "    \"γίγνοντ᾽, οὐδέ τι πάντα τελείεται ἀνθρώποισι.\",\n",
    "    \"δοιαὶ γάρ τε πύλαι ἀμενηνῶν εἰσὶν ὀνείρων:\",\n",
    "    \"αἱ μὲν γὰρ κεράεσσι τετεύχαται, αἱ δ᾽ ἐλέφαντι:\",\n",
    "    \"τῶν οἳ μέν κ᾽ ἔλθωσι διὰ πριστοῦ ἐλέφαντος,\",\n",
    "    \"οἵ ῥ᾽ ἐλεφαίρονται, ἔπε᾽ ἀκράαντα φέροντες:\",\n",
    "    \"οἱ δὲ διὰ ξεστῶν κεράων ἔλθωσι θύραζε,\",\n",
    "    \"οἵ ῥ᾽ ἔτυμα κραίνουσι, βροτῶν ὅτε κέν τις ἴδηται.\",\n",
    "]\n",
    "layer = layers.TextVectorization()\n",
    "layer.adapt(data)\n",
    "vectorized_text = layer(data)\n",
    "print(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 3 4]\n",
      " [4 0 2]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"a\", \"b\", \"c\", \"d\"]\n",
    "data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n",
    "layer = layers.StringLookup(vocabulary=vocab)\n",
    "vectorized_data = layer(data)\n",
    "print(vectorized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data before the model or inside the model\n",
    "\n",
    "Option 1: Make them part of the model, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "x = preprocessing_layer(inputs)\n",
    "outputs = rest_of_the_model(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: apply it to your tf.data.Dataset, so as to obtain a dataset that yields batches of preprocessed data, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x, y: (preprocessing_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 51s 0us/step\n",
      "170508288/170498071 [==============================] - 51s 0us/step\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\Алексей\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (node:internal/process/task_queues:96:5)"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load some data\n",
    "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
    "input_shape = x_train.shape[1:]\n",
    "classes = 10\n",
    "\n",
    "# Create a tf.data pipeline of augmented images (and their labels)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(16).map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "\n",
    "# Create a model and train it on the augmented image data\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = layers.Rescaling(1.0 / 255)(inputs)  # Rescale inputs\n",
    "outputs = keras.applications.ResNet50(  # Add the rest of the model\n",
    "    weights=None, input_shape=input_shape, classes=classes\n",
    ")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.fit(train_dataset, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aa72facdc8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "# Load some data\n",
    "(x_train, y_train), _ = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.reshape((len(x_train), -1))\n",
    "input_shape = x_train.shape[1:]\n",
    "classes = 10\n",
    "\n",
    "# Create a Normalization layer and set its internal state using the training data\n",
    "normalizer = layers.Normalization()\n",
    "normalizer.adapt(x_train)\n",
    "\n",
    "# Create a model that include the normalization layer\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = normalizer(inputs)\n",
    "outputs = layers.Dense(classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding string categorical features via one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some toy data\n",
    "data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"b\"], [\"c\"], [\"a\"]])\n",
    "\n",
    "# Use StringLookup to build an index of the feature values and encode output.\n",
    "lookup = layers.StringLookup(output_mode=\"one_hot\")\n",
    "lookup.adapt(data)\n",
    "\n",
    "# Convert new test data (which includes unknown feature values)\n",
    "test_data = tf.constant([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [\"\"]])\n",
    "encoded_data = lookup(test_data)\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding integer categorical features via one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]], shape=(6, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some toy data\n",
    "data = tf.constant([[10], [20], [20], [10], [30], [0]])\n",
    "\n",
    "# Use IntegerLookup to build an index of the feature values and encode output.\n",
    "lookup = layers.IntegerLookup(output_mode=\"one_hot\")\n",
    "lookup.adapt(data)\n",
    "\n",
    "# Convert new test data (which includes unknown feature values)\n",
    "test_data = tf.constant([[10], [10], [20], [50], [60], [0]])\n",
    "encoded_data = lookup(test_data)\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text as a sequence of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      " [[ 2 19 14  1  9  2  1]]\n",
      "\n",
      "Training model...\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4935\n",
      "\n",
      "Calling end-to-end model on test string...\n",
      "Model output: tf.Tensor([[0.04956795]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "adapt_data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(output_mode=\"int\")\n",
    "# Index the vocabulary via `adapt()`\n",
    "text_vectorizer.adapt(adapt_data)\n",
    "\n",
    "# Try out the layer\n",
    "print(\n",
    "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
    ")\n",
    "\n",
    "# Create a simple model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=text_vectorizer.vocabulary_size(), output_dim=16)(inputs)\n",
    "x = layers.GRU(8)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Create a labeled dataset (which includes unknown tokens)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
    ")\n",
    "\n",
    "# Preprocess the string inputs, turning them into int sequences\n",
    "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
    "# Train the model on the int sequences\n",
    "print(\"\\nTraining model...\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "model.fit(train_dataset)\n",
    "\n",
    "# For inference, you can export a model that accepts strings as input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "outputs = model(x)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the end-to-end model on test data (which includes unknown tokens)\n",
    "print(\"\\nCalling end-to-end model on test string...\")\n",
    "test_data = tf.constant([\"The one the other will absorb\"])\n",
    "test_output = end_to_end_model(test_data)\n",
    "print(\"Model output:\", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text as a dense matrix of ngrams with multi-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1567 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000002AA77568A68> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Encoded text:\n",
      " [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0.]]\n",
      "\n",
      "Training model...\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 0.0424\n",
      "\n",
      "Calling end-to-end model on test string...\n",
      "Model output: tf.Tensor([[0.4657468]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "adapt_data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"multi_hot\" output_mode\n",
    "# and ngrams=2 (index all bigrams)\n",
    "text_vectorizer = layers.TextVectorization(output_mode=\"multi_hot\", ngrams=2)\n",
    "# Index the bigrams via `adapt()`\n",
    "text_vectorizer.adapt(adapt_data)\n",
    "\n",
    "# Try out the layer\n",
    "print(\n",
    "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
    ")\n",
    "\n",
    "# Create a simple model\n",
    "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
    "outputs = layers.Dense(1)(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Create a labeled dataset (which includes unknown tokens)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
    ")\n",
    "\n",
    "# Preprocess the string inputs, turning them into int sequences\n",
    "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
    "# Train the model on the int sequences\n",
    "print(\"\\nTraining model...\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "model.fit(train_dataset)\n",
    "\n",
    "# For inference, you can export a model that accepts strings as input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "outputs = model(x)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the end-to-end model on test data (which includes unknown tokens)\n",
    "print(\"\\nCalling end-to-end model on test string...\")\n",
    "test_data = tf.constant([\"The one the other will absorb\"])\n",
    "test_output = end_to_end_model(test_data)\n",
    "print(\"Model output:\", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text as a dense matrix of ngrams with TF-IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 1568 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000002AA78709F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Encoded text:\n",
      " [[5.461647  1.6945957 0.        0.        0.        0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  0.        0.        1.0986123 1.0986123 1.0986123 0.        0.\n",
      "  0.        0.        0.        0.        0.        0.        0.\n",
      "  1.0986123 0.        0.        0.        0.        0.        0.\n",
      "  0.        1.0986123 1.0986123 0.        0.        0.       ]]\n",
      "\n",
      "Training model...\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 3.6755\n",
      "\n",
      "Calling end-to-end model on test string...\n",
      "Model output: tf.Tensor([[0.28605202]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Define some text data to adapt the layer\n",
    "adapt_data = tf.constant(\n",
    "    [\n",
    "        \"The Brain is wider than the Sky\",\n",
    "        \"For put them side by side\",\n",
    "        \"The one the other will contain\",\n",
    "        \"With ease and You beside\",\n",
    "    ]\n",
    ")\n",
    "# Instantiate TextVectorization with \"tf-idf\" output_mode\n",
    "# (multi-hot with TF-IDF weighting) and ngrams=2 (index all bigrams)\n",
    "text_vectorizer = layers.TextVectorization(output_mode=\"tf-idf\", ngrams=2)\n",
    "# Index the bigrams and learn the TF-IDF weights via `adapt()`\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    # A bug that prevents this from running on GPU for now.\n",
    "    text_vectorizer.adapt(adapt_data)\n",
    "\n",
    "# Try out the layer\n",
    "print(\n",
    "    \"Encoded text:\\n\", text_vectorizer([\"The Brain is deeper than the sea\"]).numpy(),\n",
    ")\n",
    "\n",
    "# Create a simple model\n",
    "inputs = keras.Input(shape=(text_vectorizer.vocabulary_size(),))\n",
    "outputs = layers.Dense(1)(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Create a labeled dataset (which includes unknown tokens)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ([\"The Brain is deeper than the sea\", \"for if they are held Blue to Blue\"], [1, 0])\n",
    ")\n",
    "\n",
    "# Preprocess the string inputs, turning them into int sequences\n",
    "train_dataset = train_dataset.batch(2).map(lambda x, y: (text_vectorizer(x), y))\n",
    "# Train the model on the int sequences\n",
    "print(\"\\nTraining model...\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "model.fit(train_dataset)\n",
    "\n",
    "# For inference, you can export a model that accepts strings as input\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "outputs = model(x)\n",
    "end_to_end_model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Call the end-to-end model on test data (which includes unknown tokens)\n",
    "print(\"\\nCalling end-to-end model on test string...\")\n",
    "test_data = tf.constant([\"The one the other will absorb\"])\n",
    "test_output = end_to_end_model(test_data)\n",
    "print(\"Model output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
