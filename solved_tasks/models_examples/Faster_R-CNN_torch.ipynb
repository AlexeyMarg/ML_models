{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster R-CNN using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data flow\n",
    "\n",
    "1. Features Extraction from the image.\n",
    "2. Creating anchor targets.\n",
    "3. Locations and objectness score prediction from the RPN network.\n",
    "4. Taking the top N locations and their objectness scores aka proposal layer\n",
    "5. Passing these top N locations through Fast R-CNN network and generating locations and cls predictions for each location is suggested in 4.\n",
    "6. generating proposal targets for each location suggested in 4.\n",
    "7. Using 2 and 3 to calculate rpn_cls_loss and rpn_reg_loss.\n",
    "8. using 5 and 6 to calculate roi_cls_loss and roi_reg_loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use VGG16 for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.zeros((1, 3, 800, 800)).float()\n",
    "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]]) # [y1, x1, y2, x2] format\n",
    "labels = torch.LongTensor([6, 8]) # 0 represents background\n",
    "sub_sample = 16 # feature map size is 800 // 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 network is used as a feature extraction module here, This acts as a backbone for both the RPN network and Fast_R-CNN network. We need to make a few changes to the VGG network inorder to make this work. Since the input of the network is 800, the output of the feature extraction module should have a feature map size of (800//16). So we need to check where the VGG16 module is achieving this feature map size and trim the network till der. This can be done in the following way.\n",
    "\n",
    "- Create a dummy image and set the volatile to be False\n",
    "- List all the layers of the vgg16\n",
    "- Pass the image through the layers and subset the list when the output_size of the image (feature map) is below the required level (800//16)\n",
    "- Convert this list into a Sequential module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dummy image and set the volatile to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_img = torch.zeros((1, 3, 800, 800)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. List all the layers of the VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(weights=True)\n",
    "fe = list(model.features)\n",
    "print(fe) # length is 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pass the image through the layers and check where you are getting this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "req_features = []\n",
    "k = dummy_img.clone()\n",
    "for i in fe:\n",
    "    k = i(k)\n",
    "    if k.size()[2] < 800//16:\n",
    "        break\n",
    "    req_features.append(i)\n",
    "    out_channels = k.size()[1]\n",
    "print(len(req_features)) #30\n",
    "print(out_channels) # 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Convert this list into a Sequential module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_rcnn_fe_extractor = nn.Sequential(*req_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "out_map = faster_rcnn_fe_extractor(image)\n",
    "print(out_map.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate Anchor at a feature map location\n",
    "2. Generate Anchor at all the feature map location.\n",
    "3. Assign the labels and location of objects (with respect to the anchor) to each and every anchor.\n",
    "4. Generate Anchor at a feature map location\n",
    "\n",
    "We will use anchor_scales of 8, 16, 32, ratio of 0.5, 1, 2 and sub sampling of 16 (Since we have pooled our image from 800 px to 50px). Now every pixel in the output feature map maps to corresponding 16 * 16 pixels in the image. This is shown in the below image.\n",
    "\n",
    "We need to generate anchor boxes on top of this 16 * 16 pixels first and similarly do along x-axis and y-axis to get all the anchor boxes. This is done in the step-2.\n",
    "\n",
    "At each pixel location on the feature map, We need to generate 9 anchor boxes (number of anchor_scales and number of ratios) and each anchor box will have ‘y1’, ‘x1’, ‘y2’, ‘x2’. So at each location anchor will have a shape of (9, 4). Lets begin with a an empty array filled with zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fill these values with corresponding y1, x1, y2, x2 at each anchor_scale and ratios. Our center for this base anchor will be at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 8.0\n"
     ]
    }
   ],
   "source": [
    "ctr_y = sub_sample / 2.\n",
    "ctr_x = sub_sample / 2.\n",
    "print(ctr_y, ctr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ratios)):\n",
    "  for j in range(len(anchor_scales)):\n",
    "    h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "    w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "    index = i * len(anchor_scales) + j\n",
    "    anchor_base[index, 0] = ctr_y - h / 2.\n",
    "    anchor_base[index, 1] = ctr_x - w / 2.\n",
    "    anchor_base[index, 2] = ctr_y + h / 2.\n",
    "    anchor_base[index, 3] = ctr_x + w / 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the anchor locations at the first feature map pixel, we have to now generate these anchors at all the locations of feature map. Also note that negitive values mean that the anchor boxes are outside image dimension. In the later section we will label them with -1 and remove them when calculating the loss the functions and generating proposals for anchor boxes. Also Since we got 9 anchors at each location and there 50 * 50 such locations inside an image, We will get 17500 (50 * 50 * 9) anchors in total. Lets generate other anchors now,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = np.zeros((len(ctr_x) * len(ctr_y), 2))\n",
    "index = 0\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[x] - 8\n",
    "        ctr[index, 0] = ctr_y[y] - 8\n",
    "        index +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together we have 2500 anchor centers. Now at each center we need to generate the anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchors = np.zeros((fe_size * fe_size * 9, 4))\n",
    "index = 0\n",
    "for c in ctr:\n",
    "  ctr_y, ctr_x = c\n",
    "  for i in range(len(ratios)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "      h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "      w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "      anchors[index, 0] = ctr_y - h / 2.\n",
    "      anchors[index, 1] = ctr_x - w / 2.\n",
    "      anchors[index, 2] = ctr_y + h / 2.\n",
    "      anchors[index, 3] = ctr_x + w / 2.\n",
    "      index += 1\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Assign the labels and location of objects (with respect to the anchor) to each and every anchor.\n",
    "\n",
    "We assign a positive label to two kind of anchors a) The anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth-box or b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box. We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. d) Anchors that are neither positive nor negitive do not contribute to the training objective.\n",
    "\n",
    "- Find the indexes of valid anchor boxes and create an array with these indexes. create an label array with shape index array filled with -1.\n",
    "- check weather one of the above conditition a, b, c is statisfying or not and fill the label accordingly. Incase of positive anchor box (label is 1), Note which ground truth object has resulted in this.\n",
    "- calculate the locations (loc) of ground truth associated with the anchor box wrt to the anchor box.\n",
    "- Reorganize all anchor boxes by filling with -1 for all unvalid anchor boxes and values we have calculated for all valid anchor boxes.\n",
    "- Outputs should be labels with (N, 1) array and locs with (N, 4) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "# Find the index of all valid anchor boxes#\n",
    "index_inside = np.where(\n",
    "        (anchors[:, 0] >= 0) &\n",
    "        (anchors[:, 1] >= 0) &\n",
    "        (anchors[:, 2] <= 800) &\n",
    "        (anchors[:, 3] <= 800)\n",
    "    )[0]\n",
    "print(index_inside.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "# create an empty label array with inside_index shape and fill with -1\n",
    "label = np.empty((len(index_inside), ), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 4)\n"
     ]
    }
   ],
   "source": [
    "# reate an array with valid anchor boxes\n",
    "valid_anchor_boxes = anchors[index_inside]\n",
    "print(valid_anchor_boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each valid anchor box calculate the iou with each ground truth object. Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 20.,  30., 400., 500.],\n",
      "        [300., 400., 500., 600.]])\n",
      "(8940, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(valid_anchor_boxes), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "print(bbox)\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "                    iter_area = (inter_y2 - inter_y1) * \\\n",
    "        (inter_x2 - inter_x1)\n",
    "                    iou = iter_area / \\\n",
    "        (anchor_area+ box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the highest iou for each gt_box and its corresponding anchor box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 5620]\n",
      "[0.68130493 0.61035156]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the highest iou for each anchor box and its corresponding ground truth box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the anchor_boxes which have this max_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 2508 5620 5628 5636 5644 5866 5874 5882 5890 6112 6120 6128 6136\n",
      " 6358 6366 6374 6382]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put thresholds to some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold  = 0.7\n",
    "neg_iou_threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[max_ious < neg_iou_threshold] = 0\n",
    "label[gt_argmax_ious] = 1\n",
    "label[max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to balance positive and negative anchors for each batch. Derive two veriables as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = 0.5\n",
    "n_sample = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total positive samples\n",
    "n_pos = pos_ratio * n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive samples\n",
    "pos_index = np.where(label == 1)[0]\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samples\n",
    "n_neg = n_sample * np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning locations to anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets assign the locations to each anchor box with the ground truth object which has maximum iou.\n",
    "\n",
    "x, y , w, h are the groud truth box center co-ordinates, width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 20.,  30., 400., 500.],\n",
      "        [ 20.,  30., 400., 500.],\n",
      "        [ 20.,  30., 400., 500.],\n",
      "        ...,\n",
      "        [ 20.,  30., 400., 500.],\n",
      "        [ 20.,  30., 400., 500.],\n",
      "        [ 20.,  30., 400., 500.]])\n"
     ]
    }
   ],
   "source": [
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
    "width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width\n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5855728   2.30914558  0.7415674   1.64727602]\n",
      " [ 0.49718446  2.30914558  0.7415674   1.64727602]\n",
      " [ 0.40879611  2.30914558  0.7415674   1.64727602]\n",
      " ...\n",
      " [-2.50801936 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.59640771 -5.29225232  0.7415674   1.64727602]\n",
      " [-2.68479606 -5.29225232  0.7415674   1.64727602]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexe\\AppData\\Local\\Temp\\ipykernel_12700\\4052958258.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  dy = (base_ctr_y - ctr_y) / height\n",
      "C:\\Users\\alexe\\AppData\\Local\\Temp\\ipykernel_12700\\4052958258.py:5: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  dx = (base_ctr_x - ctr_x) / width\n",
      "C:\\Users\\alexe\\AppData\\Local\\Temp\\ipykernel_12700\\4052958258.py:6: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  dh = np.log(base_height / height)\n",
      "C:\\Users\\alexe\\AppData\\Local\\Temp\\ipykernel_12700\\4052958258.py:7: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  dw = np.log(base_width / width)\n"
     ]
    }
   ],
   "source": [
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final labels\n",
    "anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final locations\n",
    "anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
