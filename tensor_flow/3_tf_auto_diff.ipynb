{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides the tf.GradientTape API for automatic differentiation; that is, computing the gradient of a \n",
    "computation with respect to some inputs, usually tf.Variables. TensorFlow \"records\" relevant operations executed inside \n",
    "the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" \n",
    "computation using reverse mode differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of some \n",
    "target (often a loss) relative to some source (often the model's variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example uses scalars, but tf.GradientTape works as easily on any tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 1.068181  , -0.53591543],\n",
      "       [ 0.0620077 , -1.177022  ],\n",
      "       [ 0.35467574, -0.90062493]], dtype=float32)>\n",
      "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal( (3, 2) ), name='w')\n",
    "print(w)\n",
    "b = tf.Variable(tf.zeros( (2) ), dtype=tf.float32, name='b')\n",
    "print(b)\n",
    "x = [[1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.math.reduce_min(y ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "print(w.shape)\n",
    "print(dl_dw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4.5124474 -0.       ], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#or\n",
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "print(grad['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient with respect to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant( [ [1., 2., 3.] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    #Forward pass\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling what the tape watches\n",
    "The default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n",
    "\n",
    "The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "For example, the following fails to calculate a gradient because the tf.Tensor is not \"watched\" by default, and the tf.Variable is not trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = (x0 ** 2) + (x1 ** 2) + (x2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "for g in grad:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:0\n"
     ]
    }
   ],
   "source": [
    "for g in [var.name for var in tape.watched_variables()]:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To record gradients with respect to a tf.Tensor, you need to call GradientTape.watch(x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, to disable the default behavior of watching all tf.Variables, set watch_accessed_variables=False\n",
    "\n",
    "when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GradientTape.watch was not called on x0, no gradient is computed with respect to it:\n",
    "\n",
    "dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate results\n",
    "\n",
    "You can also request gradients of the output with respect to intermediate values computed inside the tf.GradientTape context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0, name='x')\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "    z = y ** 2\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as the GradientTape.gradient method is called.\n",
    "To compute multiple gradients over the same computation, create a gradient tape with persistent=True.\n",
    " This allows multiple calls to the gradient method as resources are released when the tape object is garbage collected. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "    z = y ** 2\n",
    "\n",
    "print(tape.gradient(z, x).numpy())\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
